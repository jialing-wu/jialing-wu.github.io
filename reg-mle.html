---
layout: methods-course
title: "MLE & Generalized Linear Models"
breadcrumb: "Statistics"
bilingual: true
prev:
  url: reg-ols.html
  title: "Regression Analysis"
next:
  url: reg-causal.html
  title: "Causal Inference"
---

<style>
.problem-index{margin:0 0 8px;padding:16px 20px;border:1px solid var(--parchment);border-radius:4px;background:var(--warm)}
.problem-index-title{font-family:var(--sans);font-size:10px;font-weight:700;letter-spacing:.12em;text-transform:uppercase;color:var(--gold);margin-bottom:12px}
.problem-index a{display:block;font-size:14.5px;line-height:2;color:var(--ink-faded);text-decoration:none;transition:color .2s}
.problem-index a:hover{color:var(--red)}
.problem-index a .pi-arrow{font-family:var(--sans);font-size:11px;color:var(--gold);margin-left:6px}
.paradigm-table{border-collapse:collapse;width:100%;font-size:13.5px;line-height:1.6}
.paradigm-table th{background:var(--gold);color:var(--white);padding:10px;text-align:left;font-weight:700}
.paradigm-table td{border:1px solid var(--parchment);padding:8px 10px}
.paradigm-table tr:nth-child(even){background:var(--white)}
.paradigm-table tr:nth-child(odd){background:var(--light)}
.paradigm-table code{background:var(--light);padding:2px 4px;border-radius:2px;font-size:12px}
</style>

<!-- HEADER -->
<div class="method-header">
  <h1>Maximum Likelihood Estimation &amp; Generalized Linear Models</h1>
  <div class="method-meta">Statistics &middot; Intermediate 04</div>
</div>

<!-- INTRO CARDS -->
<div class="intro-cards">
  <div class="intro-card">
    <div class="card-label" data-lang="en">What Is This?</div>
    <div class="card-label" data-lang="zh">这一页讲什么？</div>
    <div data-lang="en"><p>OLS assumes your outcome is continuous and unbounded. But what if you're modeling whether a bill passes (0/1), whether someone is employed (0/1), or how many protests occur in a country? Generalized Linear Models extend regression to handle these outcomes.</p></div>
    <div data-lang="zh"><p>OLS 假设结果是连续无界的。但如果你在建模法案是否通过（0/1）、是否就业（0/1）或国家每年有多少抗议呢？广义线性模型把回归扩展到处理这类结果。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Prerequisites</div>
    <div class="card-label" data-lang="zh">前置知识</div>
    <div data-lang="en"><p>Regression Analysis (this page). Some exposure to probability distributions helpful.</p></div>
    <div data-lang="zh"><p>回归分析（上一页）。了解概率分布会有帮助。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Software &amp; Tools</div>
    <div class="card-label" data-lang="zh">软件工具</div>
    <div data-lang="en"><p>R (glm, MASS, pscl) or Stata (logit, probit, poisson).</p></div>
    <div data-lang="zh"><p>R（glm、MASS、pscl）或 Stata（logit、probit、poisson）。</p></div>
  </div>
</div>

<!-- PROBLEM INDEX -->
<div class="problem-index">
  <div class="problem-index-title" data-lang="en">What problem are you facing?</div>
  <div class="problem-index-title" data-lang="zh">你遇到了什么问题？</div>
  <a href="#mle-s1" data-lang="en">My outcome is binary (0 or 1) — what model do I use? <span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#mle-s1" data-lang="zh">结果是二元的（0 或 1）——用什么模型？<span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#mle-s2" data-lang="en">My outcome is ordered categories — how do I model that? <span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#mle-s2" data-lang="zh">结果是有序类别——怎么建模？<span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#mle-s3" data-lang="en">My outcome is a count — what handles that? <span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#mle-s3" data-lang="zh">结果是计数数据——用什么方法？<span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#mle-s4" data-lang="en">How does maximum likelihood estimation actually work? <span class="pi-arrow">&rarr; &sect;4</span></a>
  <a href="#mle-s4" data-lang="zh">最大似然估计究竟是怎么工作的？<span class="pi-arrow">&rarr; &sect;4</span></a>
</div>

<hr class="section-divider">

<!-- SECTION 1 -->
<div class="section" id="mle-s1">
  <h2 data-lang="en">My Outcome Is Binary (0 or 1) — What Model Do I Use?</h2>
  <h2 data-lang="zh">我的结果变量是二元的（0 或 1）——用什么模型？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You're modeling whether a legislator votes yes (1) or no (0) on a bill. You use OLS and get predicted probabilities like −0.15 and 1.47. That's nonsense — probabilities must fall between 0 and 1. Worse, OLS assumes the error term has constant variance, but for binary outcomes the variance of errors is lowest near 0 or 1 and highest near 0.5. You need a model that respects the bounded nature of probabilities.</p>
    <p data-lang="zh"><strong>问题：</strong>你在建模立法者是否投票赞成（1）或反对（0）某法案。运行 OLS 得到的预测概率像 −0.15 和 1.47——这毫无意义，概率必须在 0 和 1 之间。更糟的是，OLS 假设误差方差恒定，但对二元结果，误差方差在 0 或 1 附近最低，在 0.5 附近最高。你需要一个尊重概率有界性的模型。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Why OLS Fails for Binary Outcomes</h3>
    <h3 data-lang="zh">为什么 OLS 对二元结果失败</h3>
    <p class="method-desc" data-lang="en">Ordinary least squares predicts a linear function of your predictors. That linear function can be negative or greater than 1. For a binary outcome, you need the inverse: given predictors, what is the probability Y=1? This requires a function that is always bounded between 0 and 1, and that's nonlinear.</p>
    <p class="method-desc" data-lang="zh">普通最小二乘法预测的是预测变量的线性函数。这个线性函数可以是负数或大于 1。对于二元结果，你想要的是反过来的问题：给定预测变量，Y=1 的概率是多少？这需要一个始终在 0 和 1 之间、且是非线性的函数。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Imagine predicting whether a coin lands heads based on how hard you flip it. A linear model might predict "flipping twice as hard gives twice the probability of heads" — but that's wrong. You can't have probability > 1. Instead, each doubling of force might increase probability from 0.4 to 0.65 to 0.85, following an S-shaped curve. That's what logit and probit do.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>想象根据抛硬币的力度预测是否落正面。线性模型可能说「力度加倍，正面概率也加倍」——但那错了。你不能有概率 > 1。实际上，力度翻倍时，概率可能从 0.4 跳到 0.65 再到 0.85，形成 S 形曲线。这就是 logit 和 probit 的作用。</div>
    <div class="method-when" data-lang="en"><strong>When to use:</strong> Whenever your outcome is binary (0/1 coding is standard). Examples: voting yes/no, employed/unemployed, law passed/not passed, approved/rejected. <strong>When NOT to use:</strong> Do not use OLS for binary outcomes, even though software will run it. Do not use binary logit/probit if your outcome is continuous, ordered categories, or counts — use the appropriate GLM.</div>
    <div class="method-when" data-lang="zh"><strong>何时用：</strong>结果是二元的（0/1 编码是标准的）。例子：投票赞成/反对、就业/失业、法案通过/未通过、获批/拒绝。<strong>何时不用：</strong>不要为二元结果用 OLS，即使软件能跑。结果是连续的、有序的或计数的时候，也别用二元 logit/probit——用对应的 GLM。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Green and Gerber's GOTV field experiments. Binary outcome: did the voter turn out (1) or not (0)? Treatment: random assignment to receive a nonpartisan mailer, canvassing visit, or nothing. A logit model lets you ask: how does a canvassing visit shift the probability of turnout? If baseline turnout is 40% and canvassing raises it to 52%, that's a 12 percentage-point effect.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>Green 和 Gerber 的 GOTV 田间实验。二元结果：选民是否投票（1）或未投票（0）。处理：随机分配接收无党派信函、电话拜访或对照。logit 模型让你问：电话拜访如何改变投票概率？如果基准投票率 40%，拜访将其提高到 52%，那是 12 个百分点的效应。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Logit vs. Probit: Practically Equivalent, Logit More Interpretable</h3>
    <h3 data-lang="zh">Logit 与 Probit：基本等价，Logit 更易解释</h3>
    <p class="method-desc" data-lang="en">Both logit and probit use S-shaped curves to map linear predictors onto the probability scale [0, 1]. The difference is the specific curve: logit uses the logistic cumulative distribution function (CDF); probit uses the normal CDF. In practice, they give nearly identical results unless you have extreme predicted probabilities (very close to 0 or 1). Use logit because its coefficients have a natural interpretation: the log-odds scale.</p>
    <p class="method-desc" data-lang="zh">Logit 和 probit 都用 S 形曲线把线性预测变量映射到概率尺度 [0, 1]。区别在于曲线本身：logit 用逻辑累积分布函数（CDF），probit 用正态 CDF。实际上两者给出几乎相同的结果，除非你有极端预测概率（非常接近 0 或 1）。用 logit 因为系数有自然的解释：对数赔率。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Both logit and probit are like translators converting a linear forecast into a probability. One translator (logit) uses one conversion method, the other (probit) uses a slightly different method. For most practical purposes, they say the same thing — it's rare to see substantive differences.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>Logit 和 probit 都像翻译，把线性预测转成概率。一个翻译（logit）用一套方法，另一个（probit）用略微不同的方法。大多数情况下，他们说的一样——很少看到实质差异。</div>
    <div class="method-when" data-lang="en"><strong>When to use logit:</strong> Default choice. Easier to interpret coefficients (log-odds). When you care about marginal effects at specific probability levels. <strong>When to use probit:</strong> Rarely needed. Some prefer it when they assume underlying latent variable is normally distributed (e.g., latent propensity to vote). Results are usually similar enough that logit is preferred for simplicity.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 logit：</strong>默认选择。系数更易解释（对数赔率）。关心特定概率水平的边际效应时。<strong>何时用 probit：</strong>很少需要。有人喜欢它是因为假设潜在隐变量呈正态分布（比如潜在投票倾向）。但结果通常相似，logit 因为简洁更常用。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Predicting whether a country democratizes (1) or stays authoritarian (0) based on development, inequality, and regional diffusion. A logit coefficient of +0.45 for GDP per capita means: a one-unit increase in log(GDP) raises the log-odds of democratization by 0.45. At the sample mean probability of 0.35, that translates to roughly a 10 percentage-point increase in the probability of democratization.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>基于发展、不平等和区域扩散预测国家是否民主化（1）或保持专制（0）。GDP 人均的 logit 系数 +0.45 意味着：log(GDP) 增加一个单位，民主化的对数赔率提高 0.45。在平均概率 0.35 处，这大约转化为民主化概率增加 10 个百分点。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Computing and Interpreting Marginal Effects</h3>
    <h3 data-lang="zh">计算和解释边际效应</h3>
    <p class="method-desc" data-lang="en">The logit coefficient is not the effect on the probability of Y=1. The effect depends on the starting probability: a one-unit increase in X has a bigger effect on probability when the baseline is near 0.5 than when it's near 0 or 1. That's why you must compute marginal effects. The Average Marginal Effect (AME) is the average across all individuals in your sample: take the partial derivative of the predicted probability with respect to X, compute it for each person, then average. This tells you the typical effect of a one-unit change in X on the probability of Y=1.</p>
    <p class="method-desc" data-lang="zh">Logit 系数不是 Y=1 概率的效应。效应取决于起始概率：当基准概率接近 0.5 时，X 增加一个单位的影响比接近 0 或 1 时大。这就是为什么必须计算边际效应。平均边际效应（AME）是样本中所有人的平均：取预测概率对 X 的偏导，对每个人算，然后平均。这告诉你 X 增加一个单位对 Y=1 概率的典型效应。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> A logit coefficient is like the "steepness" of a curve at all points combined. But the actual impact of a change in X depends where you start on the curve: near the flat parts (0 or 1), the effect is small; near the steep part (0.5), the effect is large. Marginal effects let you ask: at typical probability levels in my data, how much does a one-unit change move the needle?</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>Logit 系数像曲线所有点的"陡峭程度"总和。但 X 变化的实际影响取决于你在曲线上从哪开始：平坦部分（0 或 1）效应小；陡峭部分（0.5）效应大。边际效应让你问：在我数据的典型概率水平，单位变化能动多少？</div>
    <div class="method-when" data-lang="en"><strong>When to use AME:</strong> When you want to communicate a one-unit effect in probability-point terms (e.g., "college education raises the probability of voting by 8 percentage points"). This is often clearer to audiences than log-odds. <strong>When NOT to use:</strong> Do not report only the logit coefficient without computing marginal effects; readers cannot interpret it directly in probability terms.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 AME：</strong>想用百分点来传达一个单位的效应（比如「大学教育将投票概率提高 8 个百分点」）。这对听众来说通常比对数赔率清楚。<strong>何时不用：</strong>别仅报告 logit 系数不算边际效应；读者无法直接以概率解释。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Studying voter turnout. A logit model shows income has a coefficient of +0.08. That's uninterpretable to most people. But computing the AME: a $10,000 increase in annual income is associated with a 3.2 percentage-point increase in the probability of voting. That is clear and actionable.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究选民投票率。logit 模型显示收入系数 +0.08。大多数人难以解释。但算 AME：年收入增加 $10,000 与投票概率增加 3.2 个百分点相关。这就清楚了。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Limits of Binary Logit</h3>
    <h3 data-lang="zh">二元 Logit 的局限</h3>
    <p class="method-desc" data-lang="en">Binary logit assumes you have perfectly coded 0/1 outcomes. It cannot handle measurement error in the outcome, partially observed outcomes, or ordinal outcomes where categories are ordered but intervals between them are not equal. It also assumes no unobserved confounding — logit is no more causal than OLS. A high predictive probability does not prove causation. For causal inference, you need experimental design or quasi-experimental methods (covered later in this course).</p>
    <p class="method-desc" data-lang="zh">二元 logit 假设你有完美编码的 0/1 结果。它处理不了结果中的测量误差、部分观察的数据或序数结果（类别有序但间隔不等）。也假设没有未观察的混淆——logit 不比 OLS 更因果。高预测概率不能证明因果。要因果推断，需要实验设计或准实验方法（本课程后面讲）。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Logit is a powerful tool for modeling the relationship between predictors and a binary outcome, but it doesn't magically prove causation. It's like building a detailed map of where crime happens and which neighborhoods have more police patrols — the map might be very accurate, but you can't tell from the map alone whether police cause lower crime or vice versa.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>Logit 是建模预测变量与二元结果关系的强大工具，但不能神奇地证明因果。就像绘制详细地图显示犯罪在哪、哪些社区警察多——地图可能很准，但仅从地图你无法判断警察是否导致犯罪率低或反之。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 2 -->
<div class="section" id="mle-s2">
  <h2 data-lang="en">My Outcome Is Ordered Categories — How Do I Model That?</h2>
  <h2 data-lang="zh">我的结果变量是有序类别——怎么建模？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You ask survey respondents: "How much do you trust the government?" Responses are: 1=Not at all, 2=A little, 3=Somewhat, 4=Very much. You know these are ordered (4 > 3 > 2 > 1), but the jump from 1 to 2 is not the same as from 3 to 4. Using OLS treats the distances as equal. Using binary logit treats 3 and 4 as the same category and ignores information. You need ordered logit, which respects the ordering while allowing unequal intervals between categories.</p>
    <p data-lang="zh"><strong>问题：</strong>你问调查受访者：「你在多大程度上信任政府？」回答是：1=根本不相信，2=有点相信，3=比较相信，4=非常相信。你知道这些有序（4 > 3 > 2 > 1），但 1 到 2 的跳跃与 3 到 4 的不一样。OLS 把距离视为相等。二元 logit 把 3 和 4 当同一类别，浪费信息。你需要有序 logit，既尊重顺序又允许不等间隔。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Why OLS Fails for Ordered Outcomes</h3>
    <h3 data-lang="zh">为什么 OLS 对有序结果失败</h3>
    <p class="method-desc" data-lang="en">OLS assumes equal spacing: the difference between 1 and 2 equals the difference between 2 and 3. But that's not true for attitudes. The difference between "not at all trust" and "a little trust" might be much larger than between "somewhat" and "very much." Ordered logit builds in the assumption that categories are ordered without assuming equal spacing. It estimates thresholds (cutoff points) that separate categories, allowing those thresholds to be unequally spaced.</p>
    <p class="method-desc" data-lang="zh">OLS 假设等距：1 和 2 之间的差与 2 和 3 之间的差相等。但对态度来说不成立。「根本不相信」和「有点相信」的差可能比「比较」和「非常」的差大得多。有序 logit 假设类别有序，但不假设等距。它估计分离类别的阈值（截断点），让这些阈值可以不等距。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Letter grades A, B, C, D, F are ordered — an A is better than a B — but the difference in actual learning between A and B is not the same as between B and C. Similarly, the "distance" between two grade cutoffs (say, 90 vs. 80 vs. 70) reflects pedagogical judgment, not equal intervals. Ordered logit respects this: it finds where the cutoff between each grade lies, allowing those cutoffs to be unequally spaced.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>字母等级 A、B、C、D、F 是有序的——A 比 B 好——但 A 和 B 间的实际学习差与 B 和 C 间的差不同。同样，两个等级分界（比如 90 对 80 对 70）间的"距离"反映教学判断，不是等距。有序 logit 尊重这点：找到每级之间的分界，允许分界不等距。</div>
    <div class="method-when" data-lang="en"><strong>When to use:</strong> Likert scales (strongly disagree to strongly agree), satisfaction ratings (very dissatisfied to very satisfied), education levels (primary, secondary, tertiary), income brackets. Any survey item with ordinal response categories. <strong>When NOT to use:</strong> Do not use ordered logit if categories are truly nominal (Democrat, Republican, Independent) with no natural ordering. Do not use if your outcome is truly continuous (e.g., actual income in dollars) — use OLS instead.</div>
    <div class="method-when" data-lang="zh"><strong>何时用：</strong>李克特量表（强烈反对到强烈同意）、满意度评级（非常不满意到非常满意）、教育水平（小学、中学、大学）、收入阶段。任何有序回答的调查题。<strong>何时不用：</strong>类别是名义的（民主党、共和党、独立）没有自然顺序，别用有序 logit。结果是连续的（比如实际收入美元）——用 OLS。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Pew surveys on immigration attitudes. Question: "Do you think immigration is a very big problem, big problem, small problem, or not a problem?" Respondents span all four categories. An ordered logit model reveals how education, age, and media exposure shift respondents toward more positive views. The model estimates: not a problem cutoff at 2.1, small problem at 0.8, big problem at −0.4. This means "big problem" respondents are those with predicted latent attitudes between 0.8 and −0.4.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>皮尤的移民态度调查。问题：「你认为移民是大问题、中等问题、小问题还是不是问题？」受访者遍布四个类别。有序 logit 模型显示教育、年龄和媒体接触如何让受访者更正面。模型估计：不是问题截断 2.1、小问题 0.8、大问题 −0.4。这意味着「大问题」受访者是那些潜在态度在 0.8 和 −0.4 之间的。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Proportional Odds Assumption</h3>
    <h3 data-lang="zh">比例赔率假设</h3>
    <p class="method-desc" data-lang="en">Ordered logit assumes proportional odds: the effect of a predictor on the log-odds of being in a higher category is the same across all category boundaries. For instance, education's effect on jumping from "not at all" to "a little" trust is the same as its effect on jumping from "somewhat" to "very much." This is a strong assumption. If it's violated, different predictors have different effects at different thresholds. You can test this with a Brant test or visualize by running separate binary logits for each threshold.</p>
    <p class="method-desc" data-lang="zh">有序 logit 假设比例赔率：预测变量对更高类别的对数赔率的影响在所有类别边界相同。比如教育对从「根本不」到「有点」信任的影响，与对从「比较」到「非常」的影响相同。这是强假设。如果违反了，不同预测变量在不同阈值有不同效应。可以用 Brant 检验测试，或为每个阈值单独跑二元 logit 来看。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Proportional odds says: the boost to your career from having a college degree is the same whether you start from "unemployed," "part-time worker," or "manager." It's a simplifying assumption — sometimes the degree helps you get hired but doesn't help you move from middle management to director. When it breaks, you need partial proportional odds models (allowing different effects at different thresholds).</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>比例赔率说：拥有大学学位的职业提升，无论你从「失业」、「兼职」还是「经理」开始都一样。这是简化假设——有时学位帮你找工作但不能帮你从中层管到董事。违反时，需要部分比例赔率模型（允许不同阈值不同效应）。</div>
    <div class="method-when" data-lang="en"><strong>When proportional odds holds:</strong> Most Likert scales, when the effect of a predictor is consistent across the rating scale. <strong>When it fails:</strong> Highly polarized opinion items where the effect of education on moving from "strongly disagree" to "disagree" differs from moving from "strongly agree" to "agree." Use Brant test (p > 0.05 supports proportional odds). If violated, use partial proportional odds models.</div>
    <div class="method-when" data-lang="zh"><strong>当比例赔率成立：</strong>大多数李克特量表，预测变量效应在评级中一致时。<strong>当它失败：</strong>高度两极化的观点，教育对从「强烈反对」到「反对」的效应与从「强烈同意」到「同意」的不同。用 Brant 检验（p > 0.05 支持比例赔率）。如果违反，用部分比例赔率模型。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Interpreting Predicted Probabilities</h3>
    <h3 data-lang="zh">解释预测概率</h3>
    <p class="method-desc" data-lang="en">Unlike binary logit, ordered logit gives you a set of probabilities: P(Y=1), P(Y=2), P(Y=3), P(Y=4) for a four-category outcome. These probabilities sum to 1. The model tells you: given this person's characteristics (education, age, etc.), what is the distribution of their likely response? You often present these as predicted probability plots, showing how the probability of each category changes as a key predictor varies.</p>
    <p class="method-desc" data-lang="zh">不同于二元 logit，有序 logit 给你一组概率：P(Y=1)、P(Y=2)、P(Y=3)、P(Y=4) 对四类别结果。这些概率加起来 1。模型告诉你：鉴于这个人的特征（教育、年龄等），他们可能回答的分布是什么？通常呈现为预测概率图，显示关键预测变量变化时，每个类别的概率如何变。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> It's like asking a weather service: "What's the probability of rain, clouds, or sunshine tomorrow?" They don't give you one number; they give you a distribution. With ordered logit, you do the same: as education increases, the probability of "very much trust" government increases, while "not at all trust" decreases. The total always sums to 100%.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>就像问天气服务：「明天下雨、多云或晴天的概率？」他们不给你一个数；给你分布。有序 logit 也是：随教育增加，「非常信任」政府的概率增加，「根本不相信」减少。总数始终 100%。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 3 -->
<div class="section" id="mle-s3">
  <h2 data-lang="en">My Outcome Is Count Data — What Handles That?</h2>
  <h2 data-lang="zh">我的结果变量是计数数据——用什么方法？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You're studying the number of violent conflicts per country-year. Your data: 0, 0, 1, 0, 3, 0, 0, 2, 0, 5. You run OLS and the model predicts −0.2 conflicts for some countries. That's impossible — you can't have negative conflicts. Worse, count data is often zero-inflated (many zeros) and highly right-skewed (a few countries with many conflicts). OLS assumes normally distributed errors and constant variance. Count data violates both. You need Poisson or negative binomial regression.</p>
    <p data-lang="zh"><strong>问题：</strong>你在研究每个国家年份的暴力冲突数。数据：0, 0, 1, 0, 3, 0, 0, 2, 0, 5。运行 OLS，模型对某些国家预测 −0.2 个冲突——不可能，不能有负数冲突。更糟的是计数数据常是零膨胀的（许多零）且高度右偏（少数国家冲突多）。OLS 假设误差正态、方差恒定。计数数据都违反了。需要泊松或负二项回归。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Why Counts Violate OLS Assumptions</h3>
    <h3 data-lang="zh">为什么计数违反 OLS 假设</h3>
    <p class="method-desc" data-lang="en">Count data (number of events, number of bills passed, number of protests) is non-negative, integer-valued, and often has many zeros. OLS doesn't respect these constraints. It predicts real numbers, including negatives and decimals. Additionally, count data is often heteroskedastic in a particular way: when the mean count is low, the variance is low; when the mean is high, the variance is high. The variance increases with the mean, violating OLS's constant variance assumption.</p>
    <p class="method-desc" data-lang="zh">计数数据（事件数、通过的法案、抗议）非负、整数，通常有很多零。OLS 不尊重这些约束。它预测实数，包括负数和小数。而且计数数据常常以特定方式是异方差的：平均计数低时方差低；平均值高时方差高。方差随平均值增加，违反 OLS 的恒定方差假设。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Imagine counting car accidents on a highway each day. Most days have zero or one accident. Occasionally you have five. OLS might predict "on average, −0.1 accidents" — absurd. Poisson says: "given traffic volume and weather, the expected number of accidents is 1.2, and the variance in accidents is also about 1.2." That's much more realistic.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>想象每天计算高速公路事故。大多数日子零或一个。偶尔五个。OLS 可能预测「平均 −0.1 个事故」——荒谬。泊松说：「鉴于交通和天气，预期 1.2 个事故，方差也约 1.2。」这现实多了。</div>
    <div class="method-when" data-lang="en"><strong>When to use count models:</strong> Number of legislative bills passed, number of protest events, number of war casualties, number of arrests, number of patents filed. <strong>When NOT to use:</strong> Do not use Poisson or negative binomial for continuous outcomes. Do not use for ordered categories (use ordered logit). Do not use if counts are so large (say, over 1000) that normality is a good approximation — OLS is fine then.</div>
    <div class="method-when" data-lang="zh"><strong>何时用计数模型：</strong>通过的法案数、抗议事件数、战争伤亡数、逮捕数、申请的专利数。<strong>何时不用：</strong>不要为连续结果用泊松或负二项。不要为有序类别用（用有序 logit）。计数很大（比如超过 1000），正态性是好近似，不用——那时 OLS 没问题。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Studying protest frequency in democracies. Outcome: number of significant protest events per country-year. Predictors: unemployment rate, repression level, previous year's protests. Most country-years see zero protests (baseline low); unemployment spikes increase the rate to 1–3. A negative binomial model accommodates this zero-inflation and finds that a one-percentage-point increase in unemployment raises expected protests by 0.8 events.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究民主国家的抗议频率。结果：每个国家年份的重大抗议事件数。预测变量：失业率、镇压水平、前年抗议。大多数国家年份没有抗议（基线很低）；失业率飙升将比率升到 1-3。负二项模型容纳零膨胀，发现失业率增加一个百分点会使预期抗议增加 0.8 个事件。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Poisson vs. Negative Binomial: When Mean ≠ Variance</h3>
    <h3 data-lang="zh">泊松与负二项：当平均值 ≠ 方差时</h3>
    <p class="method-desc" data-lang="en">Poisson regression assumes the mean equals the variance. This is restrictive. In real data, count outcomes often exhibit overdispersion: the variance exceeds the mean. Example: conflicts per country average 2, but variance is 12. Negative binomial regression relaxes this assumption, adding a dispersion parameter that allows variance to exceed the mean. Use negative binomial as the default unless you've confirmed that mean equals variance (rarely true in practice). You can test for overdispersion with a likelihood ratio test or visual inspection of residuals.</p>
    <p class="method-desc" data-lang="zh">泊松回归假设平均值等于方差。这很严格。在真实数据中，计数结果常表现出过度离散：方差超过平均值。例子：每个国家冲突平均 2，但方差 12。负二项回归放松这假设，添加离散参数，让方差超过平均值。除非已确认平均值等于方差（实践中很少成立），否则用负二项作默认。可以用似然比检验或残差图检验过度离散。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Poisson is like assuming a well-behaved assembly line where variation is predictable. Negative binomial acknowledges that some processes are messier: sometimes you overproduce, sometimes underproduce, and the variation isn't as neat as Poisson predicts. It's a more flexible model for messy real-world data.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>泊松就像假设运行良好的装配线，变化可预测。负二项承认某些过程更混乱：有时超生产，有时欠生产，变化不像泊松那样整洁。对现实世界的混乱数据，它是更灵活的模型。</div>
    <div class="method-when" data-lang="en"><strong>When Poisson suffices:</strong> Rare events with low means and modest dispersion. <strong>When to use negative binomial:</strong> Overdispersed count data (most real applications). Conduct a Poisson model first, then test for overdispersion using a dispersion test; if p < 0.05, use negative binomial instead.</div>
    <div class="method-when" data-lang="zh"><strong>泊松足够时：</strong>罕见事件，低平均值且适度离散。<strong>何时用负二项：</strong>过度离散的计数数据（大多数实际应用）。先跑泊松模型，然后用离散检验；p < 0.05 就改用负二项。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Zero-Inflated Models</h3>
    <h3 data-lang="zh">零膨胀模型</h3>
    <p class="method-desc" data-lang="en">Many count outcomes have more zeros than Poisson or negative binomial predicts. This is zero-inflation: two processes generate the data. First, a binary process decides whether an event can occur (e.g., "did this country experience conflict initiation risk?"). Second, if yes, a count process determines how many. Zero-inflated negative binomial (ZINB) models this explicitly: it estimates one set of predictors for the binary decision and another for the count magnitude. Use ZINB when you have theoretical or empirical reason to believe zero has a different mechanism than non-zero counts.</p>
    <p class="method-desc" data-lang="zh">许多计数结果的零比泊松或负二项预测的多。这是零膨胀：两个过程生成数据。首先，二元过程决定事件是否可能发生（比如「这个国家是否面临冲突风险？」）。其次，如果是，计数过程决定有多少个。零膨胀负二项（ZINB）模型明确建模这点：为二元决定估计一组预测变量，为计数大小估计另一组。当有理论或实证理由相信零有不同机制与非零计数时，用 ZINB。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> ZINB is like asking: "Does this country even experience conflict?" (binary) and, if yes, "How many conflicts?" (count). Some countries are structurally peaceful and always report zero (first process). Others can have conflict, and the number depends on economic and political factors (second process). ZINB separates these mechanisms.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>ZINB 就像问：「这个国家是否经历冲突？」（二元）和，如果是，「有多少个？」（计数）。有些国家在结构上和平，总报告零（第一过程）。其他国家可能有冲突，数量取决于经济和政治因素（第二过程）。ZINB 把这些机制分开。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 4 -->
<div class="section" id="mle-s4">
  <h2 data-lang="en">How Does Maximum Likelihood Estimation Actually Work?</h2>
  <h2 data-lang="zh">最大似然估计究竟是怎么工作的？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You've learned about logit, probit, Poisson, and negative binomial. They all use Maximum Likelihood Estimation (MLE). But how does it work? OLS has a simple closed-form solution: multiply X'X inverted by X'y. MLE has no simple formula. Instead, an algorithm searches for parameter values that maximize the likelihood — the probability of observing your data given those parameters. Understanding this is key to diagnosing model failures and comparing models.</p>
    <p data-lang="zh"><strong>问题：</strong>你已经了解了 logit、probit、泊松和负二项。都使用最大似然估计（MLE）。但怎么工作的？OLS 有简单的闭式解：(X'X)^-1 X'y。MLE 没有简单公式。相反，算法搜索最大化似然的参数值——给定参数观察你的数据的概率。理解这是诊断模型失败和比较模型的关键。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Likelihood Function: Making Your Data Probable</h3>
    <h3 data-lang="zh">似然函数：使你的数据概率最大</h3>
    <p class="method-desc" data-lang="en">The likelihood is the probability of observing your data given a set of parameter values. For logit, suppose person i voted (Y=1) with probability p_i = logit(beta_0 + beta_1 * X_i). The likelihood contribution is p_i for person i if they voted, or (1 − p_i) if they didn't. The total likelihood is the product of all individual contributions. Higher likelihood means the parameters you chose make your observed data more probable. MLE finds the parameter values that maximize this product.</p>
    <p class="method-desc" data-lang="zh">似然是给定参数值观察你的数据的概率。对于 logit，假设人 i 以概率 p_i = logit(β₀ + β₁X_i) 投票（Y=1）。投票了，贡献是 p_i；没投票则是 (1 − p_i)。总似然是所有个人贡献的乘积。较高的似然意味着参数使你观察的数据更可能。MLE 找到最大化这个乘积的参数值。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Imagine tuning a radio. Each dial position (parameter value) corresponds to a particular strength of reception for a station (likelihood). You're searching for the dial position that gives the clearest, strongest signal. That's the position (parameters) where your observed static and clear-voice pattern is most probable. You can't hear all dial positions at once; you adjust the dial gradually until the signal is clearest. That's what MLE's optimization algorithm does.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>想象调谐收音机。每个拨号位置（参数值）对应某电台的特定接收强度（似然）。你搜索最清晰、最强信号的拨号位置。那是你观察的静电和清晰语音模式最可能的位置（参数）。不能一次听所有位置；你逐渐调，直到信号最清。这就是 MLE 的优化算法做的。</div>
    <div class="method-when" data-lang="en"><strong>When this intuition matters:</strong> When interpreting convergence warnings (the algorithm couldn't find parameters that made the data much more probable). When comparing nested models (the larger maximum likelihood is better). When understanding why some datasets yield unstable estimates (the likelihood surface is flat, making it hard to find a unique maximum).</div>
    <div class="method-when" data-lang="zh"><strong>这个直觉何时重要：</strong>解释收敛警告（算法找不到使数据更可能的参数）。比较嵌套模型（更大的最大似然更好）。理解为什么某些数据集产生不稳定估计（似然曲面平坦，难找唯一最大值）。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> You model voting with a logit. You observe: 200 people voted, 100 didn't. Parameter guess 1: all coefficients = 0, giving probability 0.5 for everyone. Likelihood = 0.5^300. Parameter guess 2: coefficients chosen to give 80% probability to voters and 20% to non-voters. Likelihood = 0.8^200 × 0.2^100, which is much higher. MLE finds parameter guess 2 (or better).</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>用 logit 建模投票。观察：200 人投票，100 人未投。参数猜测 1：所有系数 = 0，每人 0.5 概率。似然 = 0.5^300。参数猜测 2：系数给投票者 80% 概率，非投票者 20%。似然 = 0.8^200 × 0.2^100，要高得多。MLE 找到猜测 2（或更好的）。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Log-Likelihood and Gradient-Based Optimization</h3>
    <h3 data-lang="zh">对数似然和基于梯度的优化</h3>
    <p class="method-desc" data-lang="en">Multiplying many probabilities yields numbers very close to zero (computational underflow). Instead, we work with log-likelihood: the sum of log-probabilities. Maximizing the log-likelihood is mathematically equivalent to maximizing the likelihood, but numerically stable. Most software uses iterative algorithms (Newton-Raphson, BFGS) that compute the gradient (slope) of the log-likelihood function. The gradient points "uphill" toward higher likelihood. The algorithm takes steps in that direction, updating parameters, recomputing the gradient, and stepping again until the gradient is near zero (a maximum). This is gradient descent or ascent.</p>
    <p class="method-desc" data-lang="zh">许多概率相乘产生接近零的数（计算下溢）。相反，用对数似然：对数概率的和。最大化对数似然在数学上等价于最大化似然，但数值稳定。大多数软件用迭代算法（牛顿-拉夫逊、BFGS），计算对数似然函数的梯度（斜率）。梯度指向「上坡」朝向更高似然。算法在那方向迈步，更新参数，重算梯度，再迈步，直到梯度接近零（最大值）。这是梯度下降或上升。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> You're hiking in fog and want to reach the mountain peak. You can't see the whole mountain, but you can feel the ground's slope. You take a step in the uphill direction, feel the new slope, step again. Each step you're higher. Eventually, you reach the peak where the ground is flat (gradient = 0). MLE does the same thing, replacing "mountain" with "likelihood surface" and "steps" with "parameter updates."</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>你在雾中登山，想到达山峰。看不到整座山，但能感受地面斜率。向上坡方向迈步，感受新斜率，再迈步。每迈一步都更高。最终到达地面平坦的峰顶（梯度 = 0）。MLE 做同样的事，把「山」换成「似然曲面」，把「步」换成「参数更新」。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Model Comparison: AIC and BIC</h3>
    <h3 data-lang="zh">模型比较：AIC 和 BIC</h3>
    <p class="method-desc" data-lang="en">Two models, both estimated via MLE, usually have different log-likelihoods. A larger log-likelihood means better fit. But adding predictors always increases log-likelihood, even if those predictors are noise. You need a criterion that penalizes model complexity. AIC (Akaike Information Criterion) = −2 × log-likelihood + 2 × number of parameters. BIC (Bayesian Information Criterion) = −2 × log-likelihood + log(n) × number of parameters. Lower AIC or BIC is better. BIC penalizes complexity more heavily (especially for large n) and is preferred for model selection. AIC is better for prediction. When in doubt, use BIC.</p>
    <p class="method-desc" data-lang="zh">两个模型都通过 MLE 估计，通常有不同的对数似然。较大的对数似然意味着更好的拟合。但添加预测变量总增加对数似然，即使预测变量是噪音。需要惩罚模型复杂性的准则。AIC（赤池信息准则）= −2 × 对数似然 + 2 × 参数数。BIC（贝叶斯信息准则）= −2 × 对数似然 + log(n) × 参数数。AIC 或 BIC 低更好。BIC 更严重地惩罚复杂性（特别是大 n），模型选择时更优。AIC 对预测更好。有疑问时用 BIC。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> AIC and BIC are like academic grading curves. Raw test scores (log-likelihood) might improve if you ask more questions, but that doesn't mean the student learned more — some of the new questions might be trick questions. AIC and BIC penalize too many questions (parameters), giving a fairer comparison. BIC is stricter about penalizing (like a harder grading standard), AIC is more lenient.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>AIC 和 BIC 就像学术评分曲线。原始测试分数（对数似然）可能因提问更多而改进，但不意味着学生学得更多——新问题可能是技巧问题。AIC 和 BIC 惩罚太多问题（参数），提供更公平的比较。BIC 惩罚时更严格（像更难的评分），AIC 更宽松。</div>
    <div class="method-when" data-lang="en"><strong>When to use AIC:</strong> Prediction-focused research where you care about out-of-sample fit. When comparing many non-nested models. <strong>When to use BIC:</strong> Theory-focused research where you want the most parsimonious explanation (fewest parameters). Default choice when in doubt, especially with large samples. Do not use AIC or BIC to compare models estimated on different datasets or with different outcome transformations.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 AIC：</strong>预测导向研究，关心样本外拟合。比较许多非嵌套模型时。<strong>何时用 BIC：</strong>理论导向研究，想要最简洁的解释（参数最少）。有疑问时的默认，特别是大样本。别用 AIC 或 BIC 比较不同数据集或结果转换不同的模型。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Model 1: predicting turnout from age, education, income (3 parameters). Log-likelihood = −150. AIC = 306, BIC = 319. Model 2: same as Model 1 plus party identification, ideology, interest in politics (6 parameters). Log-likelihood = −140. AIC = 292, BIC = 318. AIC prefers Model 2 (lower). BIC prefers Model 1 (simpler, better penalized). Theory suggests Model 1 is correct, so BIC's preference is reassuring.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>模型 1：从年龄、教育、收入预测投票（3 参数）。对数似然 = −150。AIC = 306，BIC = 319。模型 2：模型 1 加党派、意识形态、政治兴趣（6 参数）。对数似然 = −140。AIC = 292，BIC = 318。AIC 偏好模型 2（低）。BIC 偏好模型 1（简洁、惩罚好）。理论说模型 1 对，所以 BIC 的偏好令人放心。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Limitations and Diagnostic Concerns</h3>
    <h3 data-lang="zh">局限和诊断关注</h3>
    <p class="method-desc" data-lang="en">MLE assumes the model specification is correct. If you've chosen the wrong functional form, included too few predictors, or misspecified the error distribution, MLE will find the best parameters for that wrong model, but estimates will be biased. MLE can also encounter numerical problems: non-convergence (algorithm can't find a maximum), separation (in binary logit, the outcome perfectly predicts from predictors, yielding infinite coefficients), or unstable estimates (the likelihood surface is nearly flat). Check convergence messages, examine residual plots, and use diagnostics (influence measures, collinearity checks) as you would with OLS.</p>
    <p class="method-desc" data-lang="zh">MLE 假设模型规范正确。如果选了错的函数形式、预测变量太少或误指定误差分布，MLE 会找该错模型的最佳参数，但估计有偏差。MLE 也可能遇到数值问题：非收敛（算法找不到最大值）、分离（在二元 logit，结果完美预测自预测变量，产生无限系数）或不稳定估计（似然曲面几乎平）。检查收敛消息，看残差图，用诊断（影响措施、共线性检查），就像用 OLS 一样。</p>
  </div>

</div>

  <div class="method-section">
    <h3 data-lang="en">Choosing a Link Function: Decision Framework</h3>
    <h3 data-lang="zh">选择链接函数：决策框架</h3>
    <div data-lang="en" style="overflow-x:auto;margin:16px 0;">
      <table class="paradigm-table">
        <tr>
          <th>Outcome Variable Type</th>
          <th>Model</th>
          <th>Link Function</th>
          <th>R Code</th>
          <th>When to Use</th>
        </tr>
        <tr>
          <td>Binary (0/1)</td>
          <td>Logit</td>
          <td>logit (log-odds)</td>
          <td><code>glm(y ~ x, family=binomial(link="logit"))</code></td>
          <td>Default for binary outcomes; coefficients have intuitive log-odds interpretation</td>
        </tr>
        <tr>
          <td>Binary (0/1)</td>
          <td>Probit</td>
          <td>normal CDF</td>
          <td><code>glm(y ~ x, family=binomial(link="probit"))</code></td>
          <td>Rarely needed; assume latent normal variable; results nearly identical to logit</td>
        </tr>
        <tr>
          <td>Count (0, 1, 2, ...)</td>
          <td>Poisson</td>
          <td>log</td>
          <td><code>glm(y ~ x, family=poisson(link="log"))</code></td>
          <td>When mean ≈ variance; rare in practice; check for overdispersion</td>
        </tr>
        <tr>
          <td>Count (overdispersed)</td>
          <td>Negative Binomial</td>
          <td>log</td>
          <td><code>glm.nb(y ~ x)</code> or <code>glm(y ~ x, family=MASS::negative.binomial(theta))</code></td>
          <td>Default for count data; allows variance > mean; most social science counts</td>
        </tr>
        <tr>
          <td>Ordinal (1, 2, 3, 4)</td>
          <td>Ordered Logit</td>
          <td>logit (cumulative)</td>
          <td><code>MASS::polr(factor(y) ~ x, method="logistic")</code></td>
          <td>Survey response scales (Likert); assumes proportional odds</td>
        </tr>
        <tr>
          <td>Proportion [0, 1]</td>
          <td>Beta Regression</td>
          <td>logit</td>
          <td><code>betareg::betareg(y ~ x)</code></td>
          <td>When outcome is a rate/proportion; excludes exact 0 and 1</td>
        </tr>
      </table>
    </div>
    <div data-lang="zh" style="overflow-x:auto;margin:16px 0;">
      <table class="paradigm-table">
        <tr>
          <th>因变量类型</th>
          <th>模型</th>
          <th>链接函数</th>
          <th>R 代码</th>
          <th>何时使用</th>
        </tr>
        <tr>
          <td>二元 (0/1)</td>
          <td>Logit</td>
          <td>logit（对数赔率）</td>
          <td><code>glm(y ~ x, family=binomial(link="logit"))</code></td>
          <td>二元结果的默认选择；系数有直观的对数赔率解释</td>
        </tr>
        <tr>
          <td>二元 (0/1)</td>
          <td>Probit</td>
          <td>正态 CDF</td>
          <td><code>glm(y ~ x, family=binomial(link="probit"))</code></td>
          <td>很少需要；假设潜变量正态；结果几乎与 logit 相同</td>
        </tr>
        <tr>
          <td>计数 (0, 1, 2, ...)</td>
          <td>泊松</td>
          <td>log</td>
          <td><code>glm(y ~ x, family=poisson(link="log"))</code></td>
          <td>当平均值 ≈ 方差时；实践中很少；检查过度离散</td>
        </tr>
        <tr>
          <td>计数（过度离散）</td>
          <td>负二项</td>
          <td>log</td>
          <td><code>glm.nb(y ~ x)</code> 或 <code>glm(y ~ x, family=MASS::negative.binomial(theta))</code></td>
          <td>计数数据的默认选择；允许方差 > 平均值；大多数社科计数</td>
        </tr>
        <tr>
          <td>有序 (1, 2, 3, 4)</td>
          <td>有序 Logit</td>
          <td>logit（累积）</td>
          <td><code>MASS::polr(factor(y) ~ x, method="logistic")</code></td>
          <td>调查反应量表（李克特）；假设比例赔率</td>
        </tr>
        <tr>
          <td>比例 [0, 1]</td>
          <td>贝塔回归</td>
          <td>logit</td>
          <td><code>betareg::betareg(y ~ x)</code></td>
          <td>当结果是比率/比例时；排除恰好 0 和 1</td>
        </tr>
      </table>
    </div>
    <p class="method-desc" data-lang="en"><strong>How to choose:</strong> Start with the outcome variable's nature. Is it binary, ordered, or a count? Pick the corresponding row. The choice of link function within a family (logit vs. probit for binary) usually matters less than getting the family right. When in doubt, use logit for binary and negative binomial for counts. Most disagreements between practitioners center on family selection (e.g., is my outcome really continuous enough for OLS, or should I use ordered logit?), not link function.</p>
    <p class="method-desc" data-lang="zh"><strong>如何选择：</strong>从因变量的本质开始。是二元、有序的还是计数的？选择对应的行。链接函数在一个家族内的选择（如二元的 logit 对比 probit）通常不如选对家族重要。有疑问时，对二元用 logit，对计数用负二项。从业者之间的大多数分歧集中在家族选择（比如，我的结果真的足够连续以至于能用 OLS，还是应该用有序 logit？），而不是链接函数。</p>
  </div>

<hr class="section-divider">
<!-- GUIDES -->
<div class="section" id="mle-guides">
  <h2 data-lang="en">Guides</h2>
  <h2 data-lang="zh">配套指南</h2>
  <div class="m-list">
    <a class="m-card" href="/methods/guides/reg-mle-likelihood.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">Likelihood & MLE Interactive</div>
        <div class="m-title" data-lang="zh">似然与 MLE 交互式演示</div>
        <div class="m-desc" data-lang="en">Visualize the binomial likelihood curve and normal likelihood surface with interactive sliders.</div>
        <div class="m-desc" data-lang="zh">通过交互式滑块可视化二项似然曲线和正态似然曲面。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
    <a class="m-card" href="/methods/guides/reg-mle-glmselect.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">GLM Link Function Selector</div>
        <div class="m-title" data-lang="zh">GLM 链接函数选择器</div>
        <div class="m-desc" data-lang="en">Interactive tool: pick your outcome type and get the right GLM family, link function, and R code.</div>
        <div class="m-desc" data-lang="zh">交互工具：选择你的因变量类型，获得正确的 GLM 族、链接函数和 R 代码。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
  </div>
</div>

<hr class="section-divider">
<!-- RESOURCES -->
<hr class="section-divider">
<div class="section" id="mle-resources">
  <h2 data-lang="en">Resources</h2>
  <h2 data-lang="zh">资源</h2>
  <div class="method-section">
    <p class="method-desc" data-lang="en"><strong>R packages:</strong> glm() in base R for binary logit/probit; ordered() and polr() from MASS for ordered logit; glm(..., family=poisson) for Poisson; glm.nb() from MASS for negative binomial; zeroinfl() from pscl for zero-inflated models. <strong>Stata:</strong> logit, probit, ologit, poisson, nbreg, zinb. <strong>Key references:</strong> Long & Freese (2014) on GLM interpretation; King (1989) on multinomial models. Always compute marginal effects (margins in Stata, marginaleffects or prediction packages in R).</p>
    <p class="method-desc" data-lang="zh"><strong>R 包：</strong>基础 R 的 glm() 二元 logit/probit；MASS 的 ordered() 和 polr() 有序 logit；glm(..., family=poisson) 泊松；MASS 的 glm.nb() 负二项；pscl 的 zeroinfl() 零膨胀。<strong>Stata：</strong>logit、probit、ologit、poisson、nbreg、zinb。<strong>关键参考：</strong>Long & Freese (2014) 关于 GLM 解释；King (1989) 关于多项式。总是计算边际效应（Stata margins，R marginaleffects 或 prediction 包）。</p>
  </div>
</div>

<!-- PAGE NAV -->
<div class="page-nav">
  <a class="pn-link pn-prev" href="/methods/reg-ols.html">
    <span class="pn-arrow">&larr;</span>
    <span><span class="pn-title">Regression Analysis</span></span>
  </a>
  <a class="pn-link pn-next" href="/methods/reg-causal.html">
    <span><span class="pn-title">Causal Inference</span></span>
    <span class="pn-arrow">&rarr;</span>
  </a>
</div>
