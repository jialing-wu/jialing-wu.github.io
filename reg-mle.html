---
layout: methods-course
title: "MLE & Generalized Linear Models"
breadcrumb: "Statistics"
bilingual: true
prev:
  url: reg-ols.html
  title: "Regression Analysis"
next:
  url: reg-causal.html
  title: "Causal Inference"
---

<style>
.problem-index{margin:0 0 8px;padding:16px 20px;border:1px solid var(--parchment);border-radius:4px;background:var(--warm)}
.problem-index-title{font-family:var(--sans);font-size:10px;font-weight:700;letter-spacing:.12em;text-transform:uppercase;color:var(--gold);margin-bottom:12px}
.problem-index a{display:block;font-size:14.5px;line-height:2;color:var(--ink-faded);text-decoration:none;transition:color .2s}
.problem-index a:hover{color:var(--red)}
.problem-index a .pi-arrow{font-family:var(--sans);font-size:11px;color:var(--gold);margin-left:6px}
</style>

<!-- HEADER -->
<div class="method-header">
  <h1>Maximum Likelihood Estimation &amp; Generalized Linear Models</h1>
  <div class="method-meta">Statistics &middot; Intermediate 04</div>
</div>

<!-- INTRO CARDS -->
<div class="intro-cards">
  <div class="intro-card">
    <div class="card-label" data-lang="en">What Is This?</div>
    <div class="card-label" data-lang="zh">这一页讲什么？</div>
    <div data-lang="en"><p>OLS assumes your outcome is continuous and unbounded. But what if you're modeling whether a bill passes (0/1), whether someone is employed (0/1), or how many protests occur in a country? Generalized Linear Models extend regression to handle these outcomes.</p></div>
    <div data-lang="zh"><p>OLS 假设结果变量是连续且无界的。但如果你在建模某法案是否通过（0/1）、某人是否就业（0/1）或一个国家发生多少次抗议呢？广义线性模型将回归扩展以处理这些结果变量。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Prerequisites</div>
    <div class="card-label" data-lang="zh">前置知识</div>
    <div data-lang="en"><p>Regression Analysis (this page). Some exposure to probability distributions helpful.</p></div>
    <div data-lang="zh"><p>回归分析（上一页）。了解概率分布会有帮助。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Software &amp; Tools</div>
    <div class="card-label" data-lang="zh">软件工具</div>
    <div data-lang="en"><p>R (glm, MASS, pscl) or Stata (logit, probit, poisson).</p></div>
    <div data-lang="zh"><p>R（glm、MASS、pscl）或 Stata（logit、probit、poisson）。</p></div>
  </div>
</div>

<!-- PROBLEM INDEX -->
<div class="problem-index">
  <div class="problem-index-title" data-lang="en">What problem are you facing?</div>
  <div class="problem-index-title" data-lang="zh">你遇到了什么问题？</div>
  <a href="#mle-s1" data-lang="en">My outcome is binary (0 or 1) — what model do I use? <span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#mle-s1" data-lang="zh">我的结果变量是二元的（0 或 1）——用什么模型？<span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#mle-s2" data-lang="en">My outcome is ordered categories — how do I model that? <span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#mle-s2" data-lang="zh">我的结果变量是有序类别——怎么建模？<span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#mle-s3" data-lang="en">My outcome is a count — what handles that? <span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#mle-s3" data-lang="zh">我的结果变量是计数数据——用什么方法？<span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#mle-s4" data-lang="en">How does maximum likelihood estimation actually work? <span class="pi-arrow">&rarr; &sect;4</span></a>
  <a href="#mle-s4" data-lang="zh">最大似然估计究竟是怎么工作的？<span class="pi-arrow">&rarr; &sect;4</span></a>
</div>

<hr class="section-divider">

<!-- SECTION 1 -->
<div class="section" id="mle-s1">
  <h2 data-lang="en">My Outcome Is Binary (0 or 1) — What Model Do I Use?</h2>
  <h2 data-lang="zh">我的结果变量是二元的（0 或 1）——用什么模型？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You're modeling whether a legislator votes yes (1) or no (0) on a bill. You use OLS and get predicted probabilities like −0.15 and 1.47. That's nonsense — probabilities must fall between 0 and 1. Worse, OLS assumes the error term has constant variance, but for binary outcomes the variance of errors is lowest near 0 or 1 and highest near 0.5. You need a model that respects the bounded nature of probabilities.</p>
    <p data-lang="zh"><strong>问题：</strong>你在建模一个立法委员是否投票赞成（1）或反对（0）某法案。你用 OLS 得到预测概率像 −0.15 和 1.47。这毫无意义——概率必须在 0 和 1 之间。更糟的是，OLS 假设误差项方差恒定，但对二元结果，误差的方差在 0 或 1 附近最低，在 0.5 附近最高。你需要一个尊重概率有界性质的模型。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Why OLS Fails for Binary Outcomes</h3>
    <h3 data-lang="zh">为什么 OLS 对二元结果失败</h3>
    <p class="method-desc" data-lang="en">Ordinary least squares predicts a linear function of your predictors. That linear function can be negative or greater than 1. For a binary outcome, you need the inverse: given predictors, what is the probability Y=1? This requires a function that is always bounded between 0 and 1, and that's nonlinear.</p>
    <p class="method-desc" data-lang="zh">普通最小二乘法预测的是预测变量的线性函数。那个线性函数可以是负数或大于 1。对于二元结果，你需要的是反向关系：给定预测变量，Y=1 的概率是多少？这需要一个总是在 0 和 1 之间的函数，而且是非线性的。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Imagine predicting whether a coin lands heads based on how hard you flip it. A linear model might predict "flipping twice as hard gives twice the probability of heads" — but that's wrong. You can't have probability > 1. Instead, each doubling of force might increase probability from 0.4 to 0.65 to 0.85, following an S-shaped curve. That's what logit and probit do.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>想象基于你有多用力地翻转硬币来预测它是否落在正面。线性模型可能预测「翻转两倍的力导致正面的概率增加两倍」——但那是错的。你不能有概率 > 1。相反，每次力的翻倍可能会让概率从 0.4 增加到 0.65 再到 0.85，遵循一个 S 形曲线。这就是 logit 和 probit 做的事。</div>
    <div class="method-when" data-lang="en"><strong>When to use:</strong> Whenever your outcome is binary (0/1 coding is standard). Examples: voting yes/no, employed/unemployed, law passed/not passed, approved/rejected. <strong>When NOT to use:</strong> Do not use OLS for binary outcomes, even though software will run it. Do not use binary logit/probit if your outcome is continuous, ordered categories, or counts — use the appropriate GLM.</div>
    <div class="method-when" data-lang="zh"><strong>何时用：</strong>当你的结果变量是二元的（0/1 编码是标准的）。例子：投票赞成/反对、就业/失业、法案通过/未通过、获批/拒绝。<strong>何时不用：</strong>不要为二元结果用 OLS，即使软件能运行它。如果你的结果变量是连续的、有序类别或计数，也不要用二元 logit/probit——用相应的 GLM。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Green and Gerber's GOTV field experiments. Binary outcome: did the voter turn out (1) or not (0)? Treatment: random assignment to receive a nonpartisan mailer, canvassing visit, or nothing. A logit model lets you ask: how does a canvassing visit shift the probability of turnout? If baseline turnout is 40% and canvassing raises it to 52%, that's a 12 percentage-point effect.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>Green 和 Gerber 的 GOTV 田间实验。二元结果：选民是否投票（1）或未投票（0）？处理：随机分配收到无党派信函、电话拜访或无处理。logit 模型让你问：电话拜访如何改变投票概率？如果基准投票率是 40%，电话拜访将其提高到 52%，那是 12 个百分点的效应。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Logit vs. Probit: Practically Equivalent, Logit More Interpretable</h3>
    <h3 data-lang="zh">Logit 与 Probit：基本等价，Logit 更易解释</h3>
    <p class="method-desc" data-lang="en">Both logit and probit use S-shaped curves to map linear predictors onto the probability scale [0, 1]. The difference is the specific curve: logit uses the logistic cumulative distribution function (CDF); probit uses the normal CDF. In practice, they give nearly identical results unless you have extreme predicted probabilities (very close to 0 or 1). Use logit because its coefficients have a natural interpretation: the log-odds scale.</p>
    <p class="method-desc" data-lang="zh">Logit 和 probit 都使用 S 形曲线将线性预测变量映射到概率尺度 [0, 1]。区别是具体的曲线：logit 使用逻辑累积分布函数（CDF）；probit 使用正态 CDF。实际上，它们给出几乎相同的结果，除非你有极端的预测概率（非常接近 0 或 1）。使用 logit，因为它的系数有一个自然的解释：对数赔率尺度。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Both logit and probit are like translators converting a linear forecast into a probability. One translator (logit) uses one conversion method, the other (probit) uses a slightly different method. For most practical purposes, they say the same thing — it's rare to see substantive differences.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>Logit 和 probit 都像翻译者，将线性预测转换为概率。一个翻译（logit）使用一个转换方法，另一个（probit）使用稍微不同的方法。对于大多数实际目的，他们说的是同一件事——很少看到实质性的差异。</div>
    <div class="method-when" data-lang="en"><strong>When to use logit:</strong> Default choice. Easier to interpret coefficients (log-odds). When you care about marginal effects at specific probability levels. <strong>When to use probit:</strong> Rarely needed. Some prefer it when they assume underlying latent variable is normally distributed (e.g., latent propensity to vote). Results are usually similar enough that logit is preferred for simplicity.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 logit：</strong>默认选择。系数更易解释（对数赔率）。当你关心特定概率水平的边际效应时。<strong>何时用 probit：</strong>很少需要。有些人喜欢它，当他们假设潜在隐变量呈正态分布（例如，潜在投票倾向）。结果通常相似到足以让 logit 因为简洁而被优选。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Predicting whether a country democratizes (1) or stays authoritarian (0) based on development, inequality, and regional diffusion. A logit coefficient of +0.45 for GDP per capita means: a one-unit increase in log(GDP) raises the log-odds of democratization by 0.45. At the sample mean probability of 0.35, that translates to roughly a 10 percentage-point increase in the probability of democratization.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>基于发展、不平等和区域扩散预测一个国家是否民主化（1）或保持专制（0）。GDP 人均 +0.45 的 logit 系数意味着：log(GDP) 增加一个单位将民主化的对数赔率提高 0.45。在样本均值概率 0.35 处，这大致转化为民主化概率增加约 10 个百分点。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Computing and Interpreting Marginal Effects</h3>
    <h3 data-lang="zh">计算和解释边际效应</h3>
    <p class="method-desc" data-lang="en">The logit coefficient is not the effect on the probability of Y=1. The effect depends on the starting probability: a one-unit increase in X has a bigger effect on probability when the baseline is near 0.5 than when it's near 0 or 1. That's why you must compute marginal effects. The Average Marginal Effect (AME) is the average across all individuals in your sample: take the partial derivative of the predicted probability with respect to X, compute it for each person, then average. This tells you the typical effect of a one-unit change in X on the probability of Y=1.</p>
    <p class="method-desc" data-lang="zh">Logit 系数不是对 Y=1 概率的效应。效应取决于起始概率：在基准概率接近 0.5 时，X 增加一个单位对概率的影响比在基准接近 0 或 1 时更大。这就是为什么你必须计算边际效应。平均边际效应（AME）是对你样本中所有个人的平均：取预测概率对 X 的偏导数，对每个人计算，然后平均。这告诉你 X 增加一个单位对 Y=1 概率的典型效应。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> A logit coefficient is like the "steepness" of a curve at all points combined. But the actual impact of a change in X depends where you start on the curve: near the flat parts (0 or 1), the effect is small; near the steep part (0.5), the effect is large. Marginal effects let you ask: at typical probability levels in my data, how much does a one-unit change move the needle?</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>Logit 系数像是曲线在所有点的「陡峭程度」的组合。但 X 变化的实际影响取决于你在曲线上的起始位置：在平坦部分（0 或 1），效应很小；在陡峭部分（0.5），效应很大。边际效应让你问：在我数据中的典型概率水平，一个单位的变化能移动指针多少？</div>
    <div class="method-when" data-lang="en"><strong>When to use AME:</strong> When you want to communicate a one-unit effect in probability-point terms (e.g., "college education raises the probability of voting by 8 percentage points"). This is often clearer to audiences than log-odds. <strong>When NOT to use:</strong> Do not report only the logit coefficient without computing marginal effects; readers cannot interpret it directly in probability terms.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 AME：</strong>当你想以概率点数条款传达一个单位效应（例如，「大学教育将投票概率提高 8 个百分点」）。这对观众来说通常比对数赔率更清楚。<strong>何时不用：</strong>不要仅报告 logit 系数而不计算边际效应；读者无法直接以概率条款解释它。</p>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Studying voter turnout. A logit model shows income has a coefficient of +0.08. That's uninterpretable to most people. But computing the AME: a $10,000 increase in annual income is associated with a 3.2 percentage-point increase in the probability of voting. That is clear and actionable.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究选民投票率。logit 模型显示收入系数为 +0.08。这对大多数人来说难以解释。但计算 AME：年收入增加 $10,000 与投票概率增加 3.2 个百分点相关。这清晰可操作。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Limits of Binary Logit</h3>
    <h3 data-lang="zh">二元 Logit 的局限</h3>
    <p class="method-desc" data-lang="en">Binary logit assumes you have perfectly coded 0/1 outcomes. It cannot handle measurement error in the outcome, partially observed outcomes, or ordinal outcomes where categories are ordered but intervals between them are not equal. It also assumes no unobserved confounding — logit is no more causal than OLS. A high predictive probability does not prove causation. For causal inference, you need experimental design or quasi-experimental methods (covered later in this course).</p>
    <p class="method-desc" data-lang="zh">二元 logit 假设你有完美编码的 0/1 结果。它无法处理结果中的测量误差、部分观察的结果或序数结果，其中类别是有序但它们之间的间隔不相等。它也假设没有未观察的混淆——logit 不比 OLS 更因果。高预测概率不能证明因果关系。对于因果推断，你需要实验设计或准实验方法（本课程稍后介绍）。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Logit is a powerful tool for modeling the relationship between predictors and a binary outcome, but it doesn't magically prove causation. It's like building a detailed map of where crime happens and which neighborhoods have more police patrols — the map might be very accurate, but you can't tell from the map alone whether police cause lower crime or vice versa.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>Logit 是一个强大的工具，用于建模预测变量与二元结果之间的关系，但它不能神奇地证明因果性。这就像建立一份详细的地图，显示犯罪发生在何处以及哪些社区有更多警察巡逻——地图可能非常准确，但仅从地图你无法判断警察是否导致犯罪率更低或反之。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 2 -->
<div class="section" id="mle-s2">
  <h2 data-lang="en">My Outcome Is Ordered Categories — How Do I Model That?</h2>
  <h2 data-lang="zh">我的结果变量是有序类别——怎么建模？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You ask survey respondents: "How much do you trust the government?" Responses are: 1=Not at all, 2=A little, 3=Somewhat, 4=Very much. You know these are ordered (4 > 3 > 2 > 1), but the jump from 1 to 2 is not the same as from 3 to 4. Using OLS treats the distances as equal. Using binary logit treats 3 and 4 as the same category and ignores information. You need ordered logit, which respects the ordering while allowing unequal intervals between categories.</p>
    <p data-lang="zh"><strong>问题：</strong>你问调查受访者：「你在多大程度上信任政府？」回答是：1=根本不相信，2=有点相信，3=比较相信，4=非常相信。你知道这些是有序的（4 > 3 > 2 > 1），但从 1 到 2 的跳跃与从 3 到 4 的不同。使用 OLS 将距离视为相等。使用二元 logit 将 3 和 4 视为同一类别并忽略信息。你需要有序 logit，它尊重顺序同时允许类别之间的不相等间隔。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Why OLS Fails for Ordered Outcomes</h3>
    <h3 data-lang="zh">为什么 OLS 对有序结果失败</h3>
    <p class="method-desc" data-lang="en">OLS assumes equal spacing: the difference between 1 and 2 equals the difference between 2 and 3. But that's not true for attitudes. The difference between "not at all trust" and "a little trust" might be much larger than between "somewhat" and "very much." Ordered logit builds in the assumption that categories are ordered without assuming equal spacing. It estimates thresholds (cutoff points) that separate categories, allowing those thresholds to be unequally spaced.</p>
    <p class="method-desc" data-lang="zh">OLS 假设等距：1 和 2 之间的差异等于 2 和 3 之间的差异。但对于态度来说，这不成立。「根本不相信」和「有点相信」之间的差异可能比「比较」和「非常」之间大得多。有序 logit 建立了类别有序的假设，而不假设等距。它估计分离类别的阈值（截断点），允许这些阈值不等距。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Letter grades A, B, C, D, F are ordered — an A is better than a B — but the difference in actual learning between A and B is not the same as between B and C. Similarly, the "distance" between two grade cutoffs (say, 90 vs. 80 vs. 70) reflects pedagogical judgment, not equal intervals. Ordered logit respects this: it finds where the cutoff between each grade lies, allowing those cutoffs to be unequally spaced.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>字母等级 A、B、C、D、F 是有序的——A 比 B 好——但 A 和 B 之间的实际学习差异与 B 和 C 之间的不同。同样，两个等级截断（比如 90 对 80 对 70）之间的「距离」反映了教学判断，不是等距。有序 logit 尊重这一点：它找到每个等级之间的截断位置，允许那些截断不等距。</div>
    <div class="method-when" data-lang="en"><strong>When to use:</strong> Likert scales (strongly disagree to strongly agree), satisfaction ratings (very dissatisfied to very satisfied), education levels (primary, secondary, tertiary), income brackets. Any survey item with ordinal response categories. <strong>When NOT to use:</strong> Do not use ordered logit if categories are truly nominal (Democrat, Republican, Independent) with no natural ordering. Do not use if your outcome is truly continuous (e.g., actual income in dollars) — use OLS instead.</div>
    <div class="method-when" data-lang="zh"><strong>何时用：</strong>李克特量表（强烈反对到强烈同意）、满意度评级（非常不满意到非常满意）、教育水平（小学、中学、大学）、收入阶段。任何具有序数响应类别的调查题项。<strong>何时不用：</strong>如果类别确实名义（民主党、共和党、无党派）且没有自然顺序，不要使用有序 logit。如果你的结果确实是连续的（例如，实际收入（美元））——改用 OLS。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Pew surveys on immigration attitudes. Question: "Do you think immigration is a very big problem, big problem, small problem, or not a problem?" Respondents span all four categories. An ordered logit model reveals how education, age, and media exposure shift respondents toward more positive views. The model estimates: not a problem cutoff at 2.1, small problem at 0.8, big problem at −0.4. This means "big problem" respondents are those with predicted latent attitudes between 0.8 and −0.4.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>皮尤关于移民态度的调查。问题：「你认为移民是一个非常大的问题、大问题、小问题还是不是问题？」受访者跨越所有四个类别。有序 logit 模型显示教育、年龄和媒体接触如何将受访者转向更积极的观点。该模型估计：不是问题截断在 2.1、小问题在 0.8、大问题在 −0.4。这意味着「大问题」受访者是那些预测潜在态度在 0.8 和 −0.4 之间的。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Proportional Odds Assumption</h3>
    <h3 data-lang="zh">比例赔率假设</h3>
    <p class="method-desc" data-lang="en">Ordered logit assumes proportional odds: the effect of a predictor on the log-odds of being in a higher category is the same across all category boundaries. For instance, education's effect on jumping from "not at all" to "a little" trust is the same as its effect on jumping from "somewhat" to "very much." This is a strong assumption. If it's violated, different predictors have different effects at different thresholds. You can test this with a Brant test or visualize by running separate binary logits for each threshold.</p>
    <p class="method-desc" data-lang="zh">有序 logit 假设比例赔率：预测变量对处于更高类别的对数赔率的影响在所有类别边界上相同。例如，教育对从「根本不」到「有点」信任的跳跃的影响与它对从「比较」到「非常」的跳跃的影响相同。这是一个强假设。如果它被违反，不同的预测变量在不同的阈值处有不同的效应。你可以用 Brant 检验来测试这一点，或通过为每个阈值运行单独的二元 logit 来可视化。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Proportional odds says: the boost to your career from having a college degree is the same whether you start from "unemployed," "part-time worker," or "manager." It's a simplifying assumption — sometimes the degree helps you get hired but doesn't help you move from middle management to director. When it breaks, you need partial proportional odds models (allowing different effects at different thresholds).</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>比例赔率说：从拥有大学学位获得的职业提升无论你从「失业」、「兼职工作者」还是「经理」开始都是相同的。这是一个简化假设——有时学位帮助你找到工作但不能帮助你从中层管理到董事。当它破裂时，你需要部分比例赔率模型（允许在不同阈值处有不同的效应）。</div>
    <div class="method-when" data-lang="en"><strong>When proportional odds holds:</strong> Most Likert scales, when the effect of a predictor is consistent across the rating scale. <strong>When it fails:</strong> Highly polarized opinion items where the effect of education on moving from "strongly disagree" to "disagree" differs from moving from "strongly agree" to "agree." Use Brant test (p > 0.05 supports proportional odds). If violated, use partial proportional odds models.</div>
    <div class="method-when" data-lang="zh"><strong>当比例赔率成立：</strong>大多数李克特量表，当预测变量的效应在评级量表中一致时。<strong>当它失败：</strong>高度两极分化的观点题项，其中教育对从「强烈反对」到「反对」的移动的效应与从「强烈同意」到「同意」的不同。使用 Brant 检验（p > 0.05 支持比例赔率）。如果违反，使用部分比例赔率模型。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Interpreting Predicted Probabilities</h3>
    <h3 data-lang="zh">解释预测概率</h3>
    <p class="method-desc" data-lang="en">Unlike binary logit, ordered logit gives you a set of probabilities: P(Y=1), P(Y=2), P(Y=3), P(Y=4) for a four-category outcome. These probabilities sum to 1. The model tells you: given this person's characteristics (education, age, etc.), what is the distribution of their likely response? You often present these as predicted probability plots, showing how the probability of each category changes as a key predictor varies.</p>
    <p class="method-desc" data-lang="zh">与二元 logit 不同，有序 logit 给你一组概率：P(Y=1)、P(Y=2)、P(Y=3)、P(Y=4) 对于四类别结果。这些概率和为 1。该模型告诉你：鉴于这个人的特征（教育、年龄等），他们可能回答的分布是什么？你通常将这些呈现为预测概率图，显示当关键预测变量变化时，每个类别的概率如何变化。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> It's like asking a weather service: "What's the probability of rain, clouds, or sunshine tomorrow?" They don't give you one number; they give you a distribution. With ordered logit, you do the same: as education increases, the probability of "very much trust" government increases, while "not at all trust" decreases. The total always sums to 100%.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>就像问天气服务：「明天下雨、多云或阳光的概率是多少？」他们不给你一个数字；他们给你一个分布。使用有序 logit，你做同样的事情：随着教育增加，「非常信任」政府的概率增加，而「根本不相信」减少。总数始终加起来为 100%。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 3 -->
<div class="section" id="mle-s3">
  <h2 data-lang="en">My Outcome Is Count Data — What Handles That?</h2>
  <h2 data-lang="zh">我的结果变量是计数数据——用什么方法？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You're studying the number of violent conflicts per country-year. Your data: 0, 0, 1, 0, 3, 0, 0, 2, 0, 5. You run OLS and the model predicts −0.2 conflicts for some countries. That's impossible — you can't have negative conflicts. Worse, count data is often zero-inflated (many zeros) and highly right-skewed (a few countries with many conflicts). OLS assumes normally distributed errors and constant variance. Count data violates both. You need Poisson or negative binomial regression.</p>
    <p data-lang="zh"><strong>问题：</strong>你在研究每个国家年份的暴力冲突数量。你的数据：0, 0, 1, 0, 3, 0, 0, 2, 0, 5。你运行 OLS，模型对某些国家预测 −0.2 个冲突。这是不可能的——你不能有负数个冲突。更糟的是，计数数据通常是零膨胀的（许多零）且高度右偏（少数国家有许多冲突）。OLS 假设正态分布的误差和恒定方差。计数数据违反两者。你需要泊松或负二项回归。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Why Counts Violate OLS Assumptions</h3>
    <h3 data-lang="zh">为什么计数违反 OLS 假设</h3>
    <p class="method-desc" data-lang="en">Count data (number of events, number of bills passed, number of protests) is non-negative, integer-valued, and often has many zeros. OLS doesn't respect these constraints. It predicts real numbers, including negatives and decimals. Additionally, count data is often heteroskedastic in a particular way: when the mean count is low, the variance is low; when the mean is high, the variance is high. The variance increases with the mean, violating OLS's constant variance assumption.</p>
    <p class="method-desc" data-lang="zh">计数数据（事件数量、通过的法案数量、抗议数量）是非负的、整数值的，通常有很多零。OLS 不尊重这些约束。它预测实数，包括负数和小数。此外，计数数据通常以特定方式是异方差的：当平均计数很低时，方差很低；当平均值很高时，方差很高。方差随平均值增加，违反 OLS 的恒定方差假设。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Imagine counting car accidents on a highway each day. Most days have zero or one accident. Occasionally you have five. OLS might predict "on average, −0.1 accidents" — absurd. Poisson says: "given traffic volume and weather, the expected number of accidents is 1.2, and the variance in accidents is also about 1.2." That's much more realistic.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>想象每天计算高速公路上的汽车事故。大多数日子有零或一个事故。偶尔你有五个。OLS 可能预测「平均来说，−0.1 个事故」——荒谬。泊松说：「鉴于交通量和天气，预期事故数为 1.2，事故中的方差也约为 1.2。」那更现实得多。</div>
    <div class="method-when" data-lang="en"><strong>When to use count models:</strong> Number of legislative bills passed, number of protest events, number of war casualties, number of arrests, number of patents filed. <strong>When NOT to use:</strong> Do not use Poisson or negative binomial for continuous outcomes. Do not use for ordered categories (use ordered logit). Do not use if counts are so large (say, over 1000) that normality is a good approximation — OLS is fine then.</div>
    <div class="method-when" data-lang="zh"><strong>何时用计数模型：</strong>通过的立法法案数、抗议事件数、战争伤亡人数、逮捕人数、申请的专利数。<strong>何时不用：</strong>不要为连续结果使用泊松或负二项回归。不要为有序类别使用（使用有序 logit）。如果计数非常大（比如说，超过 1000），正态性是一个很好的近似，不要使用——那时 OLS 很好。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Studying protest frequency in democracies. Outcome: number of significant protest events per country-year. Predictors: unemployment rate, repression level, previous year's protests. Most country-years see zero protests (baseline low); unemployment spikes increase the rate to 1–3. A negative binomial model accommodates this zero-inflation and finds that a one-percentage-point increase in unemployment raises expected protests by 0.8 events.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究民主国家的抗议频率。结果：每个国家年份的重大抗议事件数。预测变量：失业率、镇压水平、前一年的抗议。大多数国家年份看不到抗议（基线很低）；失业率飙升将比率增加到 1-3。负二项模型容纳这种零膨胀，并发现失业率增加一个百分点会使预期抗议增加 0.8 个事件。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Poisson vs. Negative Binomial: When Mean ≠ Variance</h3>
    <h3 data-lang="zh">泊松与负二项：当平均值 ≠ 方差时</h3>
    <p class="method-desc" data-lang="en">Poisson regression assumes the mean equals the variance. This is restrictive. In real data, count outcomes often exhibit overdispersion: the variance exceeds the mean. Example: conflicts per country average 2, but variance is 12. Negative binomial regression relaxes this assumption, adding a dispersion parameter that allows variance to exceed the mean. Use negative binomial as the default unless you've confirmed that mean equals variance (rarely true in practice). You can test for overdispersion with a likelihood ratio test or visual inspection of residuals.</p>
    <p class="method-desc" data-lang="zh">泊松回归假设平均值等于方差。这很严格。在真实数据中，计数结果通常表现出过度离散：方差超过平均值。例子：每个国家的冲突平均为 2，但方差为 12。负二项回归放松了这一假设，添加了一个离散参数，允许方差超过平均值。除非你已确认平均值等于方差（在实践中很少成立），否则使用负二项作为默认值。你可以使用似然比检验或残差可视化检验过度离散。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Poisson is like assuming a well-behaved assembly line where variation is predictable. Negative binomial acknowledges that some processes are messier: sometimes you overproduce, sometimes underproduce, and the variation isn't as neat as Poisson predicts. It's a more flexible model for messy real-world data.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>泊松就像假设一条表现良好的装配线，其中变化是可预测的。负二项承认某些过程更混乱：有时你超生产，有时欠生产，变化不像泊松预测的那样整洁。对于混乱的现实世界数据来说，它是一个更灵活的模型。</div>
    <div class="method-when" data-lang="en"><strong>When Poisson suffices:</strong> Rare events with low means and modest dispersion. <strong>When to use negative binomial:</strong> Overdispersed count data (most real applications). Conduct a Poisson model first, then test for overdispersion using a dispersion test; if p < 0.05, use negative binomial instead.</div>
    <div class="method-when" data-lang="zh"><strong>当泊松足够：</strong>罕见事件，低平均值和适度离散。<strong>何时使用负二项：</strong>过度离散的计数数据（大多数实际应用）。首先进行泊松模型，然后使用离散检验测试过度离散；如果 p < 0.05，改用负二项。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Zero-Inflated Models</h3>
    <h3 data-lang="zh">零膨胀模型</h3>
    <p class="method-desc" data-lang="en">Many count outcomes have more zeros than Poisson or negative binomial predicts. This is zero-inflation: two processes generate the data. First, a binary process decides whether an event can occur (e.g., "did this country experience conflict initiation risk?"). Second, if yes, a count process determines how many. Zero-inflated negative binomial (ZINB) models this explicitly: it estimates one set of predictors for the binary decision and another for the count magnitude. Use ZINB when you have theoretical or empirical reason to believe zero has a different mechanism than non-zero counts.</p>
    <p class="method-desc" data-lang="zh">许多计数结果的零比泊松或负二项预测的多。这是零膨胀：两个过程生成数据。首先，一个二元过程决定事件是否可能发生（例如，「这个国家是否经历冲突启动风险？」）。其次，如果是，计数过程确定有多少个。零膨胀负二项（ZINB）模型明确模拟这一点：它为二元决定估计一组预测变量，为计数幅度估计另一组。当你有理论或实证理由相信零有不同的机制与非零计数时，使用 ZINB。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> ZINB is like asking: "Does this country even experience conflict?" (binary) and, if yes, "How many conflicts?" (count). Some countries are structurally peaceful and always report zero (first process). Others can have conflict, and the number depends on economic and political factors (second process). ZINB separates these mechanisms.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>ZINB 就像问：「这个国家是否甚至经历冲突？」（二元）和，如果是，「有多少个冲突？」（计数）。有些国家在结构上是和平的，总是报告零（第一个过程）。其他国家可能有冲突，数量取决于经济和政治因素（第二个过程）。ZINB 将这些机制分开。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 4 -->
<div class="section" id="mle-s4">
  <h2 data-lang="en">How Does Maximum Likelihood Estimation Actually Work?</h2>
  <h2 data-lang="zh">最大似然估计究竟是怎么工作的？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You've learned about logit, probit, Poisson, and negative binomial. They all use Maximum Likelihood Estimation (MLE). But how does it work? OLS has a simple closed-form solution: multiply X'X inverted by X'y. MLE has no simple formula. Instead, an algorithm searches for parameter values that maximize the likelihood — the probability of observing your data given those parameters. Understanding this is key to diagnosing model failures and comparing models.</p>
    <p data-lang="zh"><strong>问题：</strong>你已经了解了 logit、probit、泊松和负二项。他们都使用最大似然估计（MLE）。但它是如何工作的？OLS 有一个简单的闭式解：将 X'X 求逆乘以 X'y。MLE 没有简单的公式。相反，一个算法搜索最大化似然的参数值——给定这些参数观察你的数据的概率。理解这一点是诊断模型失败和比较模型的关键。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Likelihood Function: Making Your Data Probable</h3>
    <h3 data-lang="zh">似然函数：使你的数据概率最大</h3>
    <p class="method-desc" data-lang="en">The likelihood is the probability of observing your data given a set of parameter values. For logit, suppose person i voted (Y=1) with probability p_i = logit(beta_0 + beta_1 * X_i). The likelihood contribution is p_i for person i if they voted, or (1 − p_i) if they didn't. The total likelihood is the product of all individual contributions. Higher likelihood means the parameters you chose make your observed data more probable. MLE finds the parameter values that maximize this product.</p>
    <p class="method-desc" data-lang="zh">似然是给定一组参数值观察你的数据的概率。对于 logit，假设人 i 以概率 p_i = logit(beta_0 + beta_1 * X_i) 投票（Y=1）。如果他们投票，似然贡献是 p_i，如果他们没有投票则是 (1 − p_i)。总似然是所有个人贡献的乘积。较高的似然意味着你选择的参数使你观察的数据更可能。MLE 找到最大化这个乘积的参数值。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Imagine tuning a radio. Each dial position (parameter value) corresponds to a particular strength of reception for a station (likelihood). You're searching for the dial position that gives the clearest, strongest signal. That's the position (parameters) where your observed static and clear-voice pattern is most probable. You can't hear all dial positions at once; you adjust the dial gradually until the signal is clearest. That's what MLE's optimization algorithm does.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>想象调整收音机。每个拨号位置（参数值）对应于某个电台的特定接收强度（似然）。你在搜索给予最清晰、最强信号的拨号位置。那是你观察到的静电和清晰语音模式最可能的位置（参数）。你不能一次听到所有拨号位置；你逐渐调整拨号，直到信号最清晰。这就是 MLE 的优化算法所做的。</div>
    <div class="method-when" data-lang="en"><strong>When this intuition matters:</strong> When interpreting convergence warnings (the algorithm couldn't find parameters that made the data much more probable). When comparing nested models (the larger maximum likelihood is better). When understanding why some datasets yield unstable estimates (the likelihood surface is flat, making it hard to find a unique maximum).</div>
    <div class="method-when" data-lang="zh"><strong>这个直觉何时重要：</strong>当解释收敛警告（算法无法找到使数据更可能的参数）。当比较嵌套模型（较大的最大似然更好）。当理解为什么某些数据集产生不稳定的估计（似然曲面是平坦的，难以找到唯一的最大值）。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> You model voting with a logit. You observe: 200 people voted, 100 didn't. Parameter guess 1: all coefficients = 0, giving probability 0.5 for everyone. Likelihood = 0.5^300. Parameter guess 2: coefficients chosen to give 80% probability to voters and 20% to non-voters. Likelihood = 0.8^200 × 0.2^100, which is much higher. MLE finds parameter guess 2 (or better).</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>你用 logit 建模投票。你观察：200 人投票，100 人未投票。参数猜测 1：所有系数 = 0，给每个人 0.5 的概率。似然 = 0.5^300。参数猜测 2：选择的系数给投票者 80% 的概率，非投票者 20%。似然 = 0.8^200 × 0.2^100，要高得多。MLE 找到参数猜测 2（或更好的）。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Log-Likelihood and Gradient-Based Optimization</h3>
    <h3 data-lang="zh">对数似然和基于梯度的优化</h3>
    <p class="method-desc" data-lang="en">Multiplying many probabilities yields numbers very close to zero (computational underflow). Instead, we work with log-likelihood: the sum of log-probabilities. Maximizing the log-likelihood is mathematically equivalent to maximizing the likelihood, but numerically stable. Most software uses iterative algorithms (Newton-Raphson, BFGS) that compute the gradient (slope) of the log-likelihood function. The gradient points "uphill" toward higher likelihood. The algorithm takes steps in that direction, updating parameters, recomputing the gradient, and stepping again until the gradient is near zero (a maximum). This is gradient descent or ascent.</p>
    <p class="method-desc" data-lang="zh">将许多概率相乘会产生非常接近零的数字（计算下溢）。相反，我们使用对数似然：对数概率的和。最大化对数似然在数学上等价于最大化似然，但数值稳定。大多数软件使用迭代算法（牛顿-拉夫逊、BFGS），计算对数似然函数的梯度（斜率）。梯度指向「上坡」朝向更高的似然。该算法在那个方向上迈步，更新参数，重新计算梯度，并再次迈步，直到梯度接近零（最大值）。这是梯度下降或上升。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> You're hiking in fog and want to reach the mountain peak. You can't see the whole mountain, but you can feel the ground's slope. You take a step in the uphill direction, feel the new slope, step again. Each step you're higher. Eventually, you reach the peak where the ground is flat (gradient = 0). MLE does the same thing, replacing "mountain" with "likelihood surface" and "steps" with "parameter updates."</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>你在雾中登山，想到达山峰。你看不到整座山，但你可以感受地面的斜率。你在上坡方向迈步，感受新的斜率，再迈步。每迈一步你都更高。最终，你到达地面平坦的峰顶（梯度 = 0）。MLE 做同样的事情，用「似然曲面」替换「山」，用「参数更新」替换「步骤」。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Model Comparison: AIC and BIC</h3>
    <h3 data-lang="zh">模型比较：AIC 和 BIC</h3>
    <p class="method-desc" data-lang="en">Two models, both estimated via MLE, usually have different log-likelihoods. A larger log-likelihood means better fit. But adding predictors always increases log-likelihood, even if those predictors are noise. You need a criterion that penalizes model complexity. AIC (Akaike Information Criterion) = −2 × log-likelihood + 2 × number of parameters. BIC (Bayesian Information Criterion) = −2 × log-likelihood + log(n) × number of parameters. Lower AIC or BIC is better. BIC penalizes complexity more heavily (especially for large n) and is preferred for model selection. AIC is better for prediction. When in doubt, use BIC.</p>
    <p class="method-desc" data-lang="zh">两个模型，都通过 MLE 估计，通常有不同的对数似然。较大的对数似然意味着更好的拟合。但添加预测变量总是增加对数似然，即使这些预测变量是噪音。你需要一个惩罚模型复杂性的准则。AIC（赤池信息准则）= −2 × 对数似然 + 2 × 参数数量。BIC（贝叶斯信息准则）= −2 × 对数似然 + log(n) × 参数数量。较低的 AIC 或 BIC 更好。BIC 更严重地惩罚复杂性（尤其是对于大 n），在模型选择中更优选。AIC 对预测更好。有疑问时，使用 BIC。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> AIC and BIC are like academic grading curves. Raw test scores (log-likelihood) might improve if you ask more questions, but that doesn't mean the student learned more — some of the new questions might be trick questions. AIC and BIC penalize too many questions (parameters), giving a fairer comparison. BIC is stricter about penalizing (like a harder grading standard), AIC is more lenient.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>AIC 和 BIC 就像学术评分曲线。原始测试分数（对数似然）可能会随着你提出更多问题而改进，但这并不意味着学生学到更多——一些新问题可能是技巧性问题。AIC 和 BIC 惩罚太多问题（参数），提供更公平的比较。BIC 在惩罚时更严格（像更难的评分标准），AIC 更宽松。</div>
    <div class="method-when" data-lang="en"><strong>When to use AIC:</strong> Prediction-focused research where you care about out-of-sample fit. When comparing many non-nested models. <strong>When to use BIC:</strong> Theory-focused research where you want the most parsimonious explanation (fewest parameters). Default choice when in doubt, especially with large samples. Do not use AIC or BIC to compare models estimated on different datasets or with different outcome transformations.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 AIC：</strong>预测导向的研究，你关心样本外拟合。比较许多非嵌套模型时。<strong>何时用 BIC：</strong>理论导向的研究，你想要最简洁的解释（参数最少）。有疑问时的默认选择，尤其是大样本。不要用 AIC 或 BIC 比较在不同数据集上估计或结果转换不同的模型。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Model 1: predicting turnout from age, education, income (3 parameters). Log-likelihood = −150. AIC = 306, BIC = 319. Model 2: same as Model 1 plus party identification, ideology, interest in politics (6 parameters). Log-likelihood = −140. AIC = 292, BIC = 318. AIC prefers Model 2 (lower). BIC prefers Model 1 (simpler, better penalized). Theory suggests Model 1 is correct, so BIC's preference is reassuring.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>模型 1：从年龄、教育、收入预测投票（3 个参数）。对数似然 = −150。AIC = 306，BIC = 319。模型 2：与模型 1 相同加上党派认同、意识形态、对政治的兴趣（6 个参数）。对数似然 = −140。AIC = 292，BIC = 318。AIC 优选模型 2（较低）。BIC 优选模型 1（更简洁、更好惩罚）。理论表明模型 1 是正确的，所以 BIC 的偏好令人放心。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Limitations and Diagnostic Concerns</h3>
    <h3 data-lang="zh">局限和诊断关注</h3>
    <p class="method-desc" data-lang="en">MLE assumes the model specification is correct. If you've chosen the wrong functional form, included too few predictors, or misspecified the error distribution, MLE will find the best parameters for that wrong model, but estimates will be biased. MLE can also encounter numerical problems: non-convergence (algorithm can't find a maximum), separation (in binary logit, the outcome perfectly predicts from predictors, yielding infinite coefficients), or unstable estimates (the likelihood surface is nearly flat). Check convergence messages, examine residual plots, and use diagnostics (influence measures, collinearity checks) as you would with OLS.</p>
    <p class="method-desc" data-lang="zh">MLE 假设模型规范是正确的。如果你选择了错误的函数形式、包含的预测变量太少或误指定了误差分布，MLE 会找到该错误模型的最佳参数，但估计会有偏差。MLE 也可能遇到数值问题：非收敛（算法找不到最大值）、分离（在二元 logit 中，结果完美预测来自预测变量，产生无限系数）或不稳定的估计（似然曲面几乎平坦）。检查收敛消息，检查残差图，使用诊断（影响措施、共线性检查），就像你用 OLS 一样。</p>
  </div>

</div>

<hr class="section-divider">
<!-- GUIDES -->
<div class="section" id="mle-guides">
  <h2 data-lang="en">Guides</h2>
  <h2 data-lang="zh">配套指南</h2>
  <div class="m-list">
    <a class="m-card" href="/methods/guides/reg-mle-likelihood.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">Likelihood & MLE Interactive</div>
        <div class="m-title" data-lang="zh">似然函数与 MLE 交互式演示</div>
        <div class="m-desc" data-lang="en">Visualize the binomial likelihood curve and normal likelihood surface with interactive sliders.</div>
        <div class="m-desc" data-lang="zh">通过交互式滑块可视化二项似然曲线和正态似然曲面。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
  </div>
</div>

<hr class="section-divider">
<!-- RESOURCES -->
<hr class="section-divider">
<div class="section" id="mle-resources">
  <h2 data-lang="en">Resources</h2>
  <h2 data-lang="zh">资源</h2>
  <div class="method-section">
    <p class="method-desc" data-lang="en"><strong>R packages:</strong> glm() in base R for binary logit/probit; ordered() and polr() from MASS for ordered logit; glm(..., family=poisson) for Poisson; glm.nb() from MASS for negative binomial; zeroinfl() from pscl for zero-inflated models. <strong>Stata:</strong> logit, probit, ologit, poisson, nbreg, zinb. <strong>Key references:</strong> Long & Freese (2014) on GLM interpretation; King (1989) on multinomial models. Always compute marginal effects (margins in Stata, marginaleffects or prediction packages in R).</p>
    <p class="method-desc" data-lang="zh"><strong>R 包：</strong>基础 R 中的 glm() 用于二元 logit/probit；MASS 中的 ordered() 和 polr() 用于有序 logit；glm(..., family=poisson) 用于泊松；MASS 中的 glm.nb() 用于负二项；pscl 中的 zeroinfl() 用于零膨胀模型。<strong>Stata：</strong>logit、probit、ologit、poisson、nbreg、zinb。<strong>关键参考：</strong>Long & Freese (2014) 关于 GLM 解释；King (1989) 关于多项式模型。始终计算边际效应（Stata 中的 margins，R 中的 marginaleffects 或预测包）。</p>
  </div>
</div>

<!-- PAGE NAV -->
<div class="page-nav">
  <a class="pn-link pn-prev" href="/methods/reg-ols.html">
    <span class="pn-arrow">&larr;</span>
    <span><span class="pn-title">Regression Analysis</span></span>
  </a>
  <a class="pn-link pn-next" href="/methods/reg-causal.html">
    <span><span class="pn-title">Causal Inference</span></span>
    <span class="pn-arrow">&rarr;</span>
  </a>
</div>
