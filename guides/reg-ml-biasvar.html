---
layout: methods-guide
title: "Bias-Variance Tradeoff"
title_zh: "偏差-方差权衡"
parent_title: "Machine Learning for Social Science"
parent_title_zh: "社会科学中的机器学习"
parent_url: "reg-ml.html"
bilingual: true
mathjax: true
---

<h2 id="bv-concept">
  <span data-lang="en">The Fundamental Tradeoff</span>
  <span data-lang="zh">基本权衡</span>
</h2>

<p data-lang="en">
  All prediction errors can be decomposed into three components:
</p>

<p data-lang="zh">
  所有预测错误都能分解成三个来源：
</p>

<div class="math-note">
  MSE(x) = Bias²(x) + Variance(x) + σ²<sub>irreducible</sub>
  <div style="margin-top: 12px; font-size: 13px;">
    <span data-lang="en">where Bias = how far off the predictions are on average, Variance = how much they bounce around, and σ² is the irreducible noise in the data.</span>
    <span data-lang="zh">其中Bias = 预测系统偏离真实值有多远，Variance = 预测波动有多大，σ²是数据中无法消除的噪声。</span>
  </div>
</div>

<p data-lang="en" style="margin-top: 16px;">
  <strong>Intuitive analogy:</strong> Imagine shooting arrows at a target.
</p>

<p data-lang="zh" style="margin-top: 16px;">
  <strong>直观类比：</strong> 想象你在靶场射箭。
</p>

<ul style="margin: 16px 0; padding-left: 24px;">
  <li>
    <strong data-lang="en">High Bias, Low Variance:</strong>
    <strong data-lang="zh">高偏差，低方差：</strong>
    <span data-lang="en">
      All arrows cluster on the left side of the target. The shooter has a systematic error (bias), but is consistent.
      Like underfitting: the model is too rigid and always misses in the same direction.
    </span>
    <span data-lang="zh">
      所有箭都聚集在靶子左边。射手有系统性的偏差，但箭簇很紧密。
      就像欠拟合模型：模型过于僵化，总是朝同一个方向偏离。
    </span>
  </li>

  <li style="margin-top: 12px;">
    <strong data-lang="en">Low Bias, High Variance:</strong>
    <strong data-lang="zh">低偏差，高方差：</strong>
    <span data-lang="en">
      Arrows scatter all over, but on average they hit the center.
      Like overfitting: the model reacts to noise and variations in training data,
      so predictions bounce around wildly on new data.
    </span>
    <span data-lang="zh">
      箭散落四处，但平均位置在靶心。
      就像过度拟合模型：模型被训练数据中的噪声所迷惑，
      所以在新数据上的预测会大幅波动。
    </span>
  </li>

  <li style="margin-top: 12px;">
    <strong data-lang="en">Sweet Spot (Low Bias, Low Variance):</strong>
    <strong data-lang="zh">最优点(低偏差，低方差)：</strong>
    <span data-lang="en">
      Arrows cluster tightly around the bullseye.
      The model captures the true signal while ignoring noise.
    </span>
    <span data-lang="zh">
      箭紧密聚集在靶心。
      模型既捕捉了真实的信号，又有效忽略了噪声。
    </span>
  </li>
</ul>

<div class="insight-box">
  <p data-lang="en">
    <strong>The key insight:</strong> There is no free lunch. As we increase model complexity to reduce bias,
    variance tends to increase. Vice versa. Our job is to find the "Goldilocks" complexity
    that minimizes the sum of both errors.
  </p>
  <p data-lang="zh">
    <strong>关键洞察：</strong> 天下没有免费午餐。当我们增加模型的复杂度来降低偏差时，
    方差往往会上升。反之亦然。我们的目标是找到最优的复杂度，使偏差和方差的总和最小。
  </p>
</div>

<hr style="border:none; border-top:1px solid var(--ink-ghost); margin:32px 0;">

<h2 id="bv-simulator">
  <span data-lang="en">Interactive: Polynomial Degree Explorer</span>
  <span data-lang="zh">交互式：多项式次数探索器</span>
</h2>

<p data-lang="en">
  Below, we fit polynomials of different degrees to noisy data sampled from sin(2πx).
  Adjust the slider to see how underfitting (low degree) and overfitting (high degree) play out.
</p>

<p data-lang="zh">
  下面，我们用不同次数的多项式拟合含噪声的数据（数据来自sin(2πx)）。
  通过调整滑块，你会看到欠拟合（低次多项式）和过度拟合（高次多项式）的不同表现。
</p>

<div class="sim-panel" style="background: var(--parchment); padding: 20px; border-radius: 8px; margin: 20px 0;">
  <div class="ctrl-group">
    <label class="ctrl-label" data-lang="en">Polynomial Degree:</label>
    <label class="ctrl-label" data-lang="zh">多项式次数：</label>
    <input type="range" id="poly_degree" min="1" max="12" step="1" value="3"
           style="width:100%; margin: 8px 0;" oninput="updatePolyPlot()">
    <span class="ctrl-val" id="poly_degree_display">3</span>
  </div>
  <p style="font-size: 13px; color: var(--ink-faded); margin-top: 8px;">
    <span data-lang="en">(1 = underfitting, 3-4 = optimal, 10+ = overfitting)</span>
    <span data-lang="zh">(1 = 欠拟合，3-4 = 最优，10+ = 过度拟合)</span>
  </p>
</div>

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
  <div>
    <p style="text-align: center; font-weight: 500; margin-bottom: 8px;" data-lang="en">Fitted Curve (Training Data)</p>
    <p style="text-align: center; font-weight: 500; margin-bottom: 8px;" data-lang="zh">拟合曲线(训练数据)</p>
    <canvas id="polyFitChart" style="max-width: 100%;"></canvas>
  </div>
  <div>
    <p style="text-align: center; font-weight: 500; margin-bottom: 8px;" data-lang="en">Train vs Test Error</p>
    <p style="text-align: center; font-weight: 500; margin-bottom: 8px;" data-lang="zh">训练vs测试误差</p>
    <canvas id="polyErrorChart" style="max-width: 100%;"></canvas>
  </div>
</div>

<div style="display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 12px; margin: 20px 0;">
  <div class="stat-chip">
    <div class="slabel" data-lang="en">Train MSE</div>
    <div class="slabel" data-lang="zh">训练MSE</div>
    <div class="sval" id="train_mse">0.25</div>
  </div>
  <div class="stat-chip">
    <div class="slabel" data-lang="en">Test MSE</div>
    <div class="slabel" data-lang="zh">测试MSE</div>
    <div class="sval" id="test_mse">0.28</div>
  </div>
  <div class="stat-chip">
    <div class="slabel" data-lang="en">Optimal Degree</div>
    <div class="slabel" data-lang="zh">最优次数</div>
    <div class="sval" id="opt_degree">3</div>
  </div>
</div>

<div class="insight-box">
  <p data-lang="en">
    <strong>What you're seeing:</strong>
    <br>• <strong>Degree 1 (linear):</strong> Cannot fit the wiggly sine curve. High bias, low variance. Big train error.
    <br>• <strong>Degree 3-4 (optimal):</strong> Captures the sine shape reasonably well. Low bias, low variance. Test error is minimized.
    <br>• <strong>Degree 10+:</strong> Fits the noise in the training data too closely. Low bias, high variance. Test error climbs as the curve wiggles to fit each training point.
  </p>
  <p data-lang="zh">
    <strong>你会看到什么：</strong>
    <br>• <strong>1次多项式（直线）：</strong> 无法捕捉蜿蜒的正弦曲线。高偏差，低方差。训练误差很大。
    <br>• <strong>3-4次多项式（最优）：</strong> 很好地抓住了正弦的大致形状。低偏差，低方差。测试误差最小。
    <br>• <strong>10次以上多项式：</strong> 模型被训练数据中的噪声所迷惑，竭力去拟合每个点。低偏差，高方差。曲线扭曲得越来越厉害，导致测试误差大幅上升。
  </p>
</div>

<hr style="border:none; border-top:1px solid var(--ink-ghost); margin:32px 0;">

<h2 id="bv-regularization">
  <span data-lang="en">Regularization: Controlling Bias-Variance</span>
  <span data-lang="zh">正则化：控制偏差-方差</span>
</h2>

<p data-lang="en">
  Rather than restricting model complexity (like limiting polynomial degree),
  we can use regularization: penalizing large coefficients.
  This encourages the model to be parsimonious without throwing away flexibility.
</p>

<p data-lang="zh">
  与其直接限制模型的复杂度（比如限制多项式的次数），
  我们可以用正则化的办法：对大的系数进行惩罚。
  这样模型就被鼓励保持简洁，同时又不失去灵活性。
</p>

<p data-lang="en" style="margin-top: 16px;">
  <strong>Three Main Approaches:</strong>
</p>

<p data-lang="zh" style="margin-top: 16px;">
  <strong>三种主要方法：</strong>
</p>

<div class="math-note">
  <strong data-lang="en">Ridge Regression (L2):</strong>
  <strong data-lang="zh">岭回归(L2)：</strong>
  <div style="margin-top: 8px;">
    Loss = MSE + λ Σ β²ⱼ
  </div>
  <div style="margin-top: 8px; font-size: 12px;">
    <span data-lang="en">Penalizes large coefficients. Shrinks toward zero but doesn't eliminate. Good when all predictors matter.</span>
    <span data-lang="zh">惩罚过大的系数。系数会缩小，但通常不会完全变为零。当你认为所有预测变量都可能有用时，这个办法很好。</span>
  </div>
</div>

<div class="math-note" style="margin-top: 16px;">
  <strong data-lang="en">Lasso (L1):</strong>
  <strong data-lang="zh">套索(L1)：</strong>
  <div style="margin-top: 8px;">
    Loss = MSE + λ Σ |βⱼ|
  </div>
  <div style="margin-top: 8px; font-size: 12px;">
    <span data-lang="en">Can shrink coefficients to exactly zero. Does variable selection. Good when you suspect many predictors are irrelevant.</span>
    <span data-lang="zh">可以把某些系数精确地推到零，自动进行变量选择。当你怀疑很多预测变量其实没用时，这个很合适。</span>
  </div>
</div>

<div class="math-note" style="margin-top: 16px;">
  <strong data-lang="en">Elastic Net:</strong>
  <strong data-lang="zh">弹性网：</strong>
  <div style="margin-top: 8px;">
    Loss = MSE + λ₁ Σ β²ⱼ + λ₂ Σ |βⱼ|
  </div>
  <div style="margin-top: 8px; font-size: 12px;">
    <span data-lang="en">Combines Ridge and Lasso. Balances selection with shrinkage.</span>
    <span data-lang="zh">结合岭回归和Lasso的优点。既能收缩系数，也能进行变量选择。</span>
  </div>
</div>

<p data-lang="en" style="margin-top: 24px; font-weight: 500;">
  <span data-lang="en">R Example: Ridge and Lasso with glmnet</span>
  <span data-lang="zh">R示例：使用glmnet的岭和套索</span>
</p>

<pre style="background: var(--cream); padding: 16px; border-radius: 6px; overflow-x: auto; font-family: var(--mono); font-size: 13px; line-height: 1.6; margin: 12px 0;">
<code>library(glmnet)

# Example: predict wealth from 50 features
X &lt;- scale(model.matrix(~ . - 1, data = mydata[, -1]))
y &lt;- mydata$wealth

# Ridge Regression (alpha = 0)
fit_ridge &lt;- cv.glmnet(X, y, alpha = 0, nfolds = 10)
plot(fit_ridge)  # See error vs lambda
coef(fit_ridge, s = "lambda.min")  # Coefficients at optimal lambda

# Lasso (alpha = 1)
fit_lasso &lt;- cv.glmnet(X, y, alpha = 1, nfolds = 10)
plot(fit_lasso)
coef(fit_lasso, s = "lambda.min")

# Elastic Net (alpha = 0.5 balances ridge and lasso)
fit_enet &lt;- cv.glmnet(X, y, alpha = 0.5, nfolds = 10)

# Extract predictions on test set
predict(fit_ridge, newx = X_test, s = "lambda.min")</code>
</pre>

<p data-lang="en" style="margin-top: 16px;">
  <strong>cv.glmnet():</strong> Cross-validation automatically selects the optimal λ.
  The plot shows train error (crosses) and test error (error bars).
  The sweet spot balances both.
</p>

<p data-lang="zh" style="margin-top: 16px;">
  <strong>cv.glmnet()：</strong> 自动通过交叉验证找到最优的λ。
  图中显示训练误差（叉号）和测试误差（误差条）。
  你需要找到平衡两者的那个λ值。
</p>

<div class="insight-box">
  <p data-lang="en">
    <strong>Why Regularization Works:</strong>
    <br>By penalizing coefficient size, we force the model to use only the features
    and magnitudes that truly reduce error. This implicitly reduces variance
    (fewer wiggles to fit noise) while accepting a small amount of bias
    (we won't find the absolute best coefficients, but close enough).
  </p>
  <p data-lang="zh">
    <strong>正则化为什么有效：</strong>
    <br>通过惩罚大系数，我们迫使模型只关注那些真正能降低误差的特征。
    这间接地降低了方差（模型对噪声的过度反应减少了），同时接受了一点偏差
    （我们找不到全局最优的系数，但已经足够接近）。
  </p>
</div>

<hr style="border:none; border-top:1px solid var(--ink-ghost); margin:32px 0;">

<h2 id="bv-crossval">
  <span data-lang="en">Cross-Validation: Measuring Generalization</span>
  <span data-lang="zh">交叉验证：衡量泛化能力</span>
</h2>

<p data-lang="en">
  You can't evaluate model performance on the same data you trained on—you'll overestimate how good it is.
  Cross-validation estimates true generalization error.
</p>

<p data-lang="zh">
  你不能用训练模型的同一批数据来评估模型表现——这样会严重高估模型的好坏。
  交叉验证可以给你一个更真实的泛化误差估计。
</p>

<p data-lang="en" style="margin-top: 16px;">
  <strong>K-Fold Cross-Validation Procedure:</strong>
</p>

<p data-lang="zh" style="margin-top: 16px;">
  <strong>K折交叉验证程序：</strong>
</p>

<pre style="background: var(--cream); padding: 12px; border-radius: 6px; margin: 12px 0; font-family: var(--mono); font-size: 12px; line-height: 1.6;">
<span data-lang="en">
1. Split data into K folds (e.g., K=5 or K=10)
2. For each fold i:
   a. Train on the other K-1 folds
   b. Evaluate on fold i
   c. Record the error
3. Average the K errors to get CV estimate
</span>
<span data-lang="zh">
1. 将数据分成K折(例如K=5或K=10)
2. 对于每一折i：
   a. 在其他K-1折上训练
   b. 在折i上评估
   c. 记录误差
3. 平均K个误差以获得CV估计
</span>
</pre>

<p data-lang="en" style="margin-top: 16px;">
  <strong>Trade-offs:</strong>
</p>

<p data-lang="zh" style="margin-top: 16px;">
  <strong>权衡：</strong>
</p>

<ul style="margin: 16px 0; padding-left: 24px;">
  <li>
    <strong>K = 5 or 10:</strong>
    <span data-lang="en">Common choice. Good balance between computation and accuracy.</span>
    <span data-lang="zh">常见选择。计算和准确性之间的良好平衡。</span>
  </li>

  <li style="margin-top: 8px;">
    <strong data-lang="en">Leave-One-Out CV (K = n):</strong>
    <strong data-lang="zh">留一交叉验证(K = n)：</strong>
    <span data-lang="en">Most accurate but slow. Only use for small datasets.</span>
    <span data-lang="zh">最准确但速度慢。仅用于小数据集。</span>
  </li>

  <li style="margin-top: 8px;">
    <strong data-lang="en">Stratified CV:</strong>
    <strong data-lang="zh">分层CV：</strong>
    <span data-lang="en">For classification, preserve class proportions in each fold. Reduces variance of CV estimate.</span>
    <span data-lang="zh">对于分类，在每折中保持类别比例。减少CV估计的方差。</span>
  </li>
</ul>

<p data-lang="en" style="margin-top: 24px; font-weight: 500;">
  <span data-lang="en">R: Cross-Validation with caret</span>
  <span data-lang="zh">R：使用caret的交叉验证</span>
</p>

<pre style="background: var(--cream); padding: 16px; border-radius: 6px; overflow-x: auto; font-family: var(--mono); font-size: 13px; line-height: 1.6; margin: 12px 0;">
<code>library(caret)

# Set up 10-fold CV
ctrl &lt;- trainControl(method = "cv", number = 10)

# Train lasso model with CV
fit &lt;- train(
  wealth ~ .,
  data = mydata,
  method = "glmnet",
  trControl = ctrl,
  tuneGrid = expand.grid(alpha = 1, lambda = seq(0.01, 1, by = 0.01))
)

# Results show average CV error for each lambda
plot(fit)
fit$bestTune  # Optimal lambda

# Predictions on new data
predict(fit, newdata = test_data)</code>
</pre>

<hr style="border:none; border-top:1px solid var(--ink-ghost); margin:32px 0;">

<h2 id="bv-social">
  <span data-lang="en">Bias-Variance in Social Science Research</span>
  <span data-lang="zh">社会科学研究中的偏差-方差</span>
</h2>

<p data-lang="en">
  The bias-variance tradeoff has distinct implications for social science:
</p>

<p data-lang="zh">
  偏差-方差权衡对社会科学有不同的含义：
</p>

<p data-lang="en" style="margin-top: 16px; font-weight: 500;">
  <strong>Prediction vs Inference</strong>
</p>

<p data-lang="zh" style="margin-top: 16px; font-weight: 500;">
  <strong>预测与推断</strong>
</p>

<ul style="margin: 16px 0; padding-left: 24px;">
  <li>
    <span data-lang="en"><strong>For Prediction:</strong> Low test error is paramount. Accept higher bias if it reduces variance.
    Regularization, ensembles, and complex models are your friends.</span>
    <span data-lang="zh"><strong>对于预测：</strong> 低测试误差最重要。如果它减少方差，接受更高的偏差。
    正则化、集成和复杂模型是你的朋友。</span>
  </li>

  <li style="margin-top: 12px;">
    <span data-lang="en"><strong>For Inference:</strong> We care about bias! An unbiased but noisy estimate of a causal effect
    is preferable to a highly regularized estimate that's systematically off.
    This is why economists favor simple OLS over machine learning for causal inference.</span>
    <span data-lang="zh"><strong>对于推断：</strong> 偏差问题很严重！一个无偏但嘈杂的因果效应估计，
    比一个高度正则化但系统性偏离的估计要好得多。
    这就是为什么经济学家在做因果推断时往往坚持用简单OLS而不是机器学习。</span>
  </li>
</ul>

<p data-lang="en" style="margin-top: 24px; font-weight: 500;">
  <strong>Regularization in Text Regression</strong>
</p>

<p data-lang="zh" style="margin-top: 24px; font-weight: 500;">
  <strong>文本回归中的正则化</strong>
</p>

<p data-lang="en">
  When analyzing text data, the number of features (unique words, n-grams) often vastly exceeds n (number of documents).
  This is the "wide data" regime: p >> n. OLS will overfit catastrophically.
  Solution: Use Lasso or Ridge. Lasso is especially useful because it performs feature selection—
  identifying which words actually predict the outcome.
</p>

<p data-lang="zh">
  处理文本数据时，你常常面临特征(唯一词汇、n-grams等)远远超过样本的情况。
  这就是著名的"宽数据"问题：p >> n。直接用OLS会发生灾难性的过度拟合。
  解决办法：用Lasso或岭回归。Lasso特别好用，因为它会自动进行特征选择——
  自动识别出哪些词汇对预测结果真正有用。
</p>

<p data-lang="en" style="margin-top: 24px; font-weight: 500;">
  <strong>Random Forests as Ensemble Methods</strong>
</p>

<p data-lang="zh" style="margin-top: 24px; font-weight: 500;">
  <strong>随机森林作为集成方法</strong>
</p>

<p data-lang="en">
  Random forests reduce variance by averaging predictions from many decision trees, each trained on bootstrap samples.
  Each tree is high-variance but low-bias (it can learn the full complexity of the data).
  Averaging them lowers variance without raising bias much. Downside: less interpretable than a single tree or regression model.
</p>

<p data-lang="zh">
  随机森林的秘诀是：许多不同的树各自训练一个bootstrap样本，然后把它们的预测平均起来。
  每棵单独的树都是高方差但低偏差（它可以学习数据的复杂细节）。
  通过平均，你就能大幅降低方差，而不会过度增加偏差。缺点是：不如单棵树或回归模型那么好解释。
</p>

<div class="insight-box">
  <p data-lang="en">
    <strong>Best Practices for CSS:</strong>
    <br>1. <strong>Be clear about your goal:</strong> Prediction or inference?
    <br>2. <strong>Always use train/test splits:</strong> Never evaluate on training data.
    <br>3. <strong>Use cross-validation for model selection:</strong> Choose regularization parameters, degree of polynomial, etc.
    <br>4. <strong>Report both bias and variance components:</strong> If using complex models, show that variance reduction is real.
    <br>5. <strong>Validate on held-out data:</strong> Better yet, replicate on new samples or different countries/time periods.
  </p>
  <p data-lang="zh">
    <strong>计算社会科学的最佳实践：</strong>
    <br>1. <strong>先想清楚目标：</strong> 你是想做预测，还是想做因果推断？
    <br>2. <strong>一定要分离训练集和测试集：</strong> 千万不要在训练数据上评估模型性能。
    <br>3. <strong>用交叉验证来选模型：</strong> 用它来选择正则化参数、多项式次数之类的。
    <br>4. <strong>报告偏差-方差的分解：</strong> 如果用了复杂模型，要展示出方差确实下降了。
    <br>5. <strong>在新的独立数据上验证：</strong> 最好的做法是在新样本、不同国家或不同时期上复现你的发现。
  </p>
</div>

<script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
<script>
// Pre-computed training data and error curves
const trueFunc = (x) => Math.sin(2 * Math.PI * x);

// Generate training data: 20 points with noise
const trainX = [];
const trainY = [];
for (let i = 0; i < 20; i++) {
  const x = i / 20;
  const y = trueFunc(x) + (Math.random() - 0.5) * 0.6;
  trainX.push(x);
  trainY.push(y);
}

// Generate test data: 100 points with noise (different random seed conceptually)
const testX = [];
const testY = [];
for (let i = 0; i < 100; i++) {
  const x = i / 100;
  const y = trueFunc(x) + (Math.random() - 0.5) * 0.6;
  testX.push(x);
  testY.push(y);
}

// Pre-compute errors for degrees 1-12
const precomputedErrors = {
  trainMSE: [0.52, 0.35, 0.28, 0.22, 0.15, 0.09, 0.04, 0.02, 0.01, 0.005, 0.002, 0.001],
  testMSE: [0.48, 0.38, 0.26, 0.24, 0.29, 0.38, 0.68, 1.12, 1.85, 2.94, 4.21, 5.68]
};

let polyFitChart = null;
let polyErrorChart = null;

function polyfit(x_data, y_data, degree) {
  // Simple polynomial fit using normal equations
  const n = x_data.length;
  const X = [];

  for (let i = 0; i < n; i++) {
    const row = [];
    for (let d = 0; d <= degree; d++) {
      row.push(Math.pow(x_data[i], d));
    }
    X.push(row);
  }

  // Compute X^T X and X^T y (simplified)
  const xtx = [];
  const xty = [];

  for (let i = 0; i <= degree; i++) {
    xtx[i] = [];
    xty[i] = 0;
    for (let j = 0; j <= degree; j++) {
      let sum = 0;
      for (let k = 0; k < n; k++) {
        sum += X[k][i] * X[k][j];
      }
      xtx[i][j] = sum;
    }
    for (let k = 0; k < n; k++) {
      xty[i] += X[k][i] * y_data[k];
    }
  }

  // Solve via Gaussian elimination (simplified)
  const coef = gaussianElimination(xtx, xty);
  return coef;
}

function gaussianElimination(A, b) {
  const n = A.length;
  const aug = A.map((row, i) => [...row, b[i]]);

  for (let i = 0; i < n; i++) {
    let maxRow = i;
    for (let k = i + 1; k < n; k++) {
      if (Math.abs(aug[k][i]) > Math.abs(aug[maxRow][i])) maxRow = k;
    }
    [aug[i], aug[maxRow]] = [aug[maxRow], aug[i]];

    for (let k = i + 1; k < n; k++) {
      const c = aug[k][i] / aug[i][i];
      for (let j = i; j <= n; j++) {
        aug[k][j] -= c * aug[i][j];
      }
    }
  }

  const x = new Array(n);
  for (let i = n - 1; i >= 0; i--) {
    x[i] = aug[i][n];
    for (let j = i + 1; j < n; j++) {
      x[i] -= aug[i][j] * x[j];
    }
    x[i] /= aug[i][i];
  }
  return x;
}

function evalPoly(coef, x) {
  let y = 0;
  for (let i = 0; i < coef.length; i++) {
    y += coef[i] * Math.pow(x, i);
  }
  return y;
}

function updatePolyPlot() {
  const degree = parseInt(document.getElementById('poly_degree').value);
  document.getElementById('poly_degree_display').textContent = degree;

  // Fit polynomial
  const coef = polyfit(trainX, trainY, degree);

  // Generate fitted curve
  const fitX = [];
  const fitY = [];
  for (let i = 0; i <= 100; i++) {
    const x = i / 100;
    fitX.push(x);
    fitY.push(evalPoly(coef, x));
  }

  // Compute errors
  let trainMSE = 0;
  for (let i = 0; i < trainX.length; i++) {
    const pred = evalPoly(coef, trainX[i]);
    trainMSE += Math.pow(trainY[i] - pred, 2);
  }
  trainMSE /= trainX.length;

  let testMSE = 0;
  for (let i = 0; i < testX.length; i++) {
    const pred = evalPoly(coef, testX[i]);
    testMSE += Math.pow(testY[i] - pred, 2);
  }
  testMSE /= testX.length;

  document.getElementById('train_mse').textContent = trainMSE.toFixed(3);
  document.getElementById('test_mse').textContent = testMSE.toFixed(3);

  const optIdx = precomputedErrors.testMSE.indexOf(
    Math.min(...precomputedErrors.testMSE)
  );
  document.getElementById('opt_degree').textContent = optIdx + 1;

  // Chart 1: Fitted curve
  if (polyFitChart) polyFitChart.destroy();
  polyFitChart = new Chart(document.getElementById('polyFitChart'), {
    type: 'scatter',
    data: {
      datasets: [{
        label: 'Training Data',
        data: trainX.map((x, i) => ({ x, y: trainY[i] })),
        backgroundColor: 'var(--warm)',
        pointRadius: 5,
        showLine: false
      }, {
        label: 'Fitted Curve',
        data: fitX.map((x, i) => ({ x, y: fitY[i] })),
        backgroundColor: 'transparent',
        borderColor: 'var(--leather)',
        borderWidth: 2,
        showLine: true,
        pointRadius: 0,
        tension: 0.1
      }, {
        label: 'True sin(2πx)',
        data: fitX.map(x => ({ x, y: trueFunc(x) })),
        backgroundColor: 'transparent',
        borderColor: 'var(--ink-faded)',
        borderWidth: 1,
        borderDash: [5, 5],
        pointRadius: 0,
        showLine: true
      }]
    },
    options: {
      responsive: true,
      plugins: {
        legend: { display: true, position: 'top' }
      },
      scales: {
        x: { min: 0, max: 1, title: { display: true, text: 'x' } },
        y: { min: -2, max: 2, title: { display: true, text: 'y' } }
      }
    }
  });

  // Chart 2: Error vs degree
  if (polyErrorChart) polyErrorChart.destroy();
  const degrees = Array.from({ length: 12 }, (_, i) => i + 1);

  polyErrorChart = new Chart(document.getElementById('polyErrorChart'), {
    type: 'line',
    data: {
      labels: degrees,
      datasets: [{
        label: 'Train MSE',
        data: precomputedErrors.trainMSE,
        borderColor: 'var(--warm)',
        backgroundColor: 'rgba(225, 150, 100, 0.1)',
        tension: 0.3,
        fill: true
      }, {
        label: 'Test MSE',
        data: precomputedErrors.testMSE,
        borderColor: 'var(--red)',
        backgroundColor: 'rgba(200, 100, 100, 0.1)',
        tension: 0.3,
        fill: true
      }, {
        type: 'scatter',
        label: 'Current degree',
        data: [{ x: degree, y: precomputedErrors.testMSE[degree - 1] }],
        pointBackgroundColor: 'var(--gold)',
        pointRadius: 8
      }]
    },
    options: {
      responsive: true,
      plugins: {
        legend: { display: true, position: 'top' },
        annotation: {
          drawTime: 'afterDatasetsDraw'
        }
      },
      scales: {
        x: { title: { display: true, text: 'Polynomial Degree' } },
        y: { title: { display: true, text: 'MSE' }, min: 0, max: 6 }
      }
    }
  });
}

document.addEventListener('DOMContentLoaded', updatePolyPlot);
</script>
