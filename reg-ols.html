---
layout: methods-course
title: "Regression Analysis"
breadcrumb: "Statistics"
bilingual: true
prev:
  url: reg-dataviz.html
  title: "Data Visualization"
next:
  url: reg-mle.html
  title: "MLE & Generalized Linear Models"
---

<style>
.problem-index{margin:0 0 8px;padding:16px 20px;border:1px solid var(--parchment);border-radius:4px;background:var(--warm)}
.problem-index-title{font-family:var(--sans);font-size:10px;font-weight:700;letter-spacing:.12em;text-transform:uppercase;color:var(--gold);margin-bottom:12px}
.problem-index a{display:block;font-size:14.5px;line-height:2;color:var(--ink-faded);text-decoration:none;transition:color .2s}
.problem-index a:hover{color:var(--red)}
.problem-index a .pi-arrow{font-family:var(--sans);font-size:11px;color:var(--gold);margin-left:6px}
</style>

<!-- HEADER -->
<div class="method-header">
  <h1>Regression Analysis</h1>
  <div class="method-meta">Statistics &middot; Foundations 03</div>
</div>

<!-- INTRO CARDS -->
<div class="intro-cards">
  <div class="intro-card">
    <div class="card-label" data-lang="en">What Is This?</div>
    <div class="card-label" data-lang="zh">这一页讲什么？</div>
    <div data-lang="en"><p>Regression is the workhorse of quantitative social science. It lets you estimate how much X changes Y, while holding other things constant. This page covers OLS from first intuition to diagnostics.</p></div>
    <div data-lang="zh"><p>回归是量化社会科学的主力工具。它让你估计 X 变化多少会改变 Y，同时控制其他因素。本页从直觉到诊断，完整介绍 OLS。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Prerequisites</div>
    <div class="card-label" data-lang="zh">前置知识</div>
    <div data-lang="en"><p>Statistical Foundations (§3 on hypothesis testing). Basic algebra.</p></div>
    <div data-lang="zh"><p>统计基础（§3 假设检验）。基础代数。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Software &amp; Tools</div>
    <div class="card-label" data-lang="zh">软件工具</div>
    <div data-lang="en"><p>R (lm, modelsummary) or Stata (reg).</p></div>
    <div data-lang="zh"><p>R（lm、modelsummary）或 Stata（reg）。</p></div>
  </div>
</div>

<!-- PROBLEM INDEX -->
<div class="problem-index">
  <div class="problem-index-title" data-lang="en">What problem are you facing?</div>
  <div class="problem-index-title" data-lang="zh">你遇到了什么问题？</div>
  <a href="#ols-s1" data-lang="en">Can X predict Y — and by how much? <span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#ols-s1" data-lang="zh">X 能预测 Y 吗？能预测多少？<span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#ols-s2" data-lang="en">What if other variables are confounding my result? <span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#ols-s2" data-lang="zh">如果其他变量在干扰我的结果怎么办？<span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#ols-s3" data-lang="en">How do I know if my regression assumptions hold? <span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#ols-s3" data-lang="zh">我怎么知道回归假设是否成立？<span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#ols-s4" data-lang="en">Does the effect of X on Y change across contexts? <span class="pi-arrow">&rarr; &sect;4</span></a>
  <a href="#ols-s4" data-lang="zh">X 对 Y 的影响会随情境变化吗？<span class="pi-arrow">&rarr; &sect;4</span></a>
</div>

<hr class="section-divider">

<!-- SECTION 1 -->
<div class="section" id="ols-s1">
  <h2 data-lang="en">OLS Basics: The Line That Minimizes Errors</h2>
  <h2 data-lang="zh">OLS 基础：最小化误差的那条线</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You have data on years of education and annual income for 200 people. You want to estimate: on average, how much does each additional year of education increase income? Regression answers this. It finds the best-fit line through the scatter of points, such that the vertical distance from each point to the line (the error) is minimized. The slope of that line is your answer: the average increase in Y per unit increase in X.</p>
    <p data-lang="zh"><strong>问题：</strong>你有200个人的教育年数和年收入数据。你想估计：平均而言，每多一年教育能带来多少收入增长？回归可以帮你答这个问题。它在散点图中找出最佳拟合线，让每个数据点到这条线的垂直距离（误差）尽可能小。这条线的斜率就是答案：X每增加一个单位，Y平均增加多少。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Regression Equation and Interpretation</h3>
    <h3 data-lang="zh">回归方程与解释</h3>
    <p class="method-desc" data-lang="en">Simple regression (one predictor X) takes the form: Y = β₀ + β₁X + ε, where β₀ is the intercept (the predicted value of Y when X=0), β₁ is the slope (the change in Y for each unit increase in X), and ε is the error term (the deviation between predicted and actual Y). Ordinary Least Squares (OLS) finds the values of β₀ and β₁ that minimize the sum of squared errors.</p>
    <p class="method-desc" data-lang="zh">简单回归（一个预测变量X）形式是：Y = β₀ + β₁X + ε，其中β₀是截距（当X=0时Y的预测值），β₁是斜率（每单位X增加时Y变化多少），ε是误差项（预测值与实际Y的偏差）。普通最小二乘法（OLS）找到让平方误差和最小的β₀和β₁值。</p>
    <p class="method-desc" data-lang="en">The slope β₁ is the most important quantity. In the education-income example, suppose β₁ = 3.5 (thousand dollars). This means: each additional year of education is associated with, on average, an additional $3,500 in annual income. This is conditional on the model specification; if important variables are missing, the estimate may be biased. The intercept β₀ is often not meaningful (e.g., "predicted income when education = 0 years" is nonsensical for an adult). Report the intercept for completeness, but focus interpretation on the slope.</p>
    <p class="method-desc" data-lang="zh">斜率β₁是最重要的。在教育-收入例子中，假设β₁=3.5（千美元）。这表示：每多一年教育，平均关联年收入增加3500美元。这是基于特定模型规范得出的；如果重要变量漏掉了，估计会有偏差。截距β₀常常没有实际意义（比如"教育=0年时的预测收入"对成人就无意义）。完整性上要报告截距，但解释重点要放在斜率上。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> A regression line is your best guess, given incomplete information. If someone tells you "I studied for the exam," you might guess a higher score than if they say "I didn't study," but you are not certain. A regression line quantifies this guess. The slope tells you, on average, how much the outcome changes with the predictor, accounting for random variation.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>回归线是你的最佳猜测，给定不完整信息。如果有人说"我为考试学习了"，你可能猜比他们说"我没学习"更高分数，但你不确定。回归线量化这个猜测。斜率告诉你，平均而言，结果如何随预测变化，说明随机变异。</div>
    <p class="method-desc" data-lang="en">R-squared (R²) measures how well the regression line fits the data. R² ranges from 0 to 1 and represents the proportion of variance in Y explained by X. An R² of 0.3 means X explains 30% of variation in Y; 70% is explained by other factors or random noise. High R² (0.7+) is uncommon in social science — human behavior is inherently variable. A low R² does not invalidate a regression; it just means your predictor is one of many factors.</p>
    <p class="method-desc" data-lang="zh">R平方（R²）衡量回归线拟合数据的好坏程度。R²范围从0到1，代表X能解释Y变异的比例。R²=0.3表示X解释了30%的Y变异；其余70%由其他因素或随机误差解释。高R²（0.7+）在社会科学很少见——人的行为本身就易变。低R²不会使回归失效；它只是说你的预测器是影响结果的众多因素中的一个。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Use regression when you want to estimate how much one variable affects another. Regression is ideal for observational data where you cannot run experiments. Always report β₁, its standard error, and a confidence interval. Do not rely solely on R² to judge model quality; focus on whether the effect size is practically meaningful. Do not interpret regression as causal without strong theoretical justification; correlation and confounding variables remain threats.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>当你想估计一个变量影响另一个变量有多大时用回归。回归对无法做实验的观察数据特别理想。总是报告β₁、它的标准误和置信区间。不要只靠R²判断模型质量；要关注效应大小是否有实际意义。没有强理论依据就不要把回归结果解读成因果关系；相关和混淆变量还是会构成威胁。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher models the effect of years of education on annual income using 500 survey respondents. The regression yields: Income = 25,000 + 3,500 × Education. The intercept ($25k) represents predicted baseline income (perhaps from non-education factors). The slope ($3.5k per year) means each additional year of education is associated with $3,500 more annual income. If education also correlates with family wealth (a confounder), this estimate is biased upward — it conflates education's effect with wealth's effect. Adding family income as a control variable in multiple regression can address this.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究者用500个调查受访者对年收入建模教育年数的效应。回归产生：收入=25000+3500×教育。截距（25000美元）代表预测基线收入（也许来自非教育因素）。斜率（每年3500美元）意味着每额外教育年数关联年收入多3500美元。如果教育也与家庭财富相关（混淆），这个估计有向上偏（它混淆了教育效应与财富效应）。在多元回归中添加家庭收入作为控制变量可以解决这个。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Statistical Inference: Significance and Confidence</h3>
    <h3 data-lang="zh">统计推断：显著性与置信</h3>
    <p class="method-desc" data-lang="en">Every regression coefficient comes with a standard error (SE), which quantifies sampling variability. The larger the sample or the less noise in the data, the smaller the SE. A 95% confidence interval around β₁ tells you the range of values likely to contain the true population parameter. If the confidence interval excludes zero, the effect is statistically significant at p &lt; 0.05.</p>
    <p class="method-desc" data-lang="zh">每个回归系数都伴随一个标准误（SE），它量化了抽样变异的大小。样本越大或数据噪声越少，SE就越小。围绕β₁的95%置信区间告诉你真实总体参数可能落在的值域范围。如果置信区间不包含零，那么该效应在p &lt; 0.05时达到统计显著。</p>
    <p class="method-desc" data-lang="en">A t-statistic is the ratio of the coefficient to its standard error: t = β₁ / SE(β₁). Large t-statistics (|t| &gt; 1.96 for large samples) correspond to small p-values. But remember the lessons from the Statistical Foundations chapter: p &lt; 0.05 does not mean the effect is large or practically important. A regression coefficient can be statistically significant but economically trivial. Report both statistical significance and effect size (the actual coefficient value) so readers can judge importance themselves.</p>
    <p class="method-desc" data-lang="zh">t统计量是系数除以其标准误：t = β₁ / SE(β₁)。大的t统计量（|t| &gt; 1.96对于大样本）对应小的P值。但回想一下统计基础章的要点：p &lt; 0.05并不意味着效应很大或实际有意义。回归系数可以在统计上显著，但在实际上微不足道。要报告统计显著性和效应大小（实际系数值），让读者自己判断其重要性。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Report p-values and confidence intervals for all coefficients, but do not treat p &lt; 0.05 as the arbitration of truth. A p-value of 0.049 is not meaningfully different from 0.051. In publications, highlight the coefficient magnitude and its practical meaning. If a coefficient's confidence interval is very wide, interpret cautiously — the precision is low. If coefficients from different studies point in the same direction, even if individually p &gt; 0.05, that convergence is evidence.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>为所有系数报告P值和置信区间，但不要把p &lt; 0.05当作真理的仲裁。P值0.049与0.051没有有意义不同。在出版中，突出系数大小及其实际含义。如果系数的置信区间很宽，谨慎解释——精度低。如果不同研究的系数指向同一方向，即使个别p &gt; 0.05，那个融合是证据。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Five OLS Assumptions</h3>
    <h3 data-lang="zh">五个OLS假设</h3>
    <p class="method-desc" data-lang="en">For OLS estimates to be valid, five key assumptions must (roughly) hold. (1) Linearity: The relationship between X and Y is linear (or you have correctly specified the functional form). (2) Independence: Observations are independent (not repeated measures or clustered data). (3) Homoskedasticity: The variance of errors is constant across all values of X (errors do not get bigger as X increases). (4) Normality: Errors are normally distributed. (5) No omitted variables: All important predictors are included in the model.</p>
    <p class="method-desc" data-lang="zh">要使OLS估计有效，需要满足五个关键假设（大致上）。(1)线性：X和Y之间的关系是线性的（或者你正确指定了函数形式）。(2)独立：观察值相互独立（不是重复测量，也不是分组数据）。(3)同方差：误差的方差在X的所有取值上保持恒定（误差不会随X增加而变大）。(4)正态性：误差服从正态分布。(5)无遗漏变量：所有重要的预测变量都包含在模型中。</p>
    <p class="method-desc" data-lang="en">These assumptions are often violated in real data. That does not mean regression is useless — it means you must check assumptions and adjust your inference accordingly. Violation of assumptions does not invalidate the point estimates (β₁ is still unbiased under most violations) but may invalidate confidence intervals and p-values. The good news: large sample sizes and robust standard errors can mitigate many problems.</p>
    <p class="method-desc" data-lang="zh">这些假设在真实数据中经常被违反。但这不意味着回归没有用——只是说你需要检查这些假设，并相应调整你的推断。假设被违反不会使点估计失效（β₁在大多数违反情况下仍是无偏的），但可能会使置信区间和P值失效。好消息是：大的样本量和稳健的标准误可以缓解很多问题。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Always check OLS assumptions graphically (residual plots, Q-Q plots). Do not assume assumptions hold just because your sample is large — large samples can have heteroskedasticity or other problems. If assumptions are severely violated, consider transformations (log, square root) or alternative methods (robust regression, quantile regression). Report standard errors clearly and note any assumption violations in your methods section.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>总是用图形方式检查OLS假设（残差图、Q-Q图）。不要只因为样本量大就认为假设自动满足——即使是大样本也可能有异方差或其他问题。如果假设被严重违反，考虑进行转换（对数、平方根）或使用替代方法（稳健回归、分位数回归）。明确报告标准误，并在方法部分说明任何假设违反。</div>
  </div>
</div>

<hr class="section-divider">

<!-- SECTION 2 -->
<div class="section" id="ols-s2">
  <h2 data-lang="en">Multiple Regression: Controlling for Confounders</h2>
  <h2 data-lang="zh">多元回归：控制混淆变量</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You estimate that education increases income by $3,500 per year, but you suspect this is biased. Wealthier families produce children with both more education and higher incomes, independent of education's causal effect. You need to disentangle education's true effect from the effect of family wealth. Multiple regression includes additional predictor variables (controls), and the coefficient on education changes to represent its effect while holding wealth constant. This is the power of multiple regression: isolating causal effects by controlling for confounding variables.</p>
    <p data-lang="zh"><strong>问题：</strong>你估计教育每年增加收入3500美元，但你怀疑这个估计有偏差。较富裕的家庭既能让孩子接受更多教育，也自然有更高的收入，而这与教育本身的因果效应是独立的。你需要把教育的真实效应从家庭财富的影响中分离出来。多元回归通过加入额外的预测变量（控制变量），让教育系数调整为在保持财富不变的条件下的效应。这正是多元回归的强大之处：通过控制混淆变量来隔离因果效应。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Multiple Regression Model and "Holding Constant"</h3>
    <h3 data-lang="zh">多元回归模型与"保持恒定"</h3>
    <p class="method-desc" data-lang="en">Multiple regression takes the form: Y = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ + ε. Each coefficient βᵢ represents the change in Y for a one-unit increase in Xᵢ, holding all other X variables constant. This "holding constant" is done mathematically by the regression algorithm — it partitions the variance in Y into components attributable to each X variable separately.</p>
    <p class="method-desc" data-lang="zh">多元回归的形式是：Y = β₀ + β₁X₁ + β₂X₂ + ... + βₖXₖ + ε。每个系数βᵢ表示当Xᵢ增加一个单位时，在其他所有X变量保持不变的情况下，Y会变化多少。这种"保持不变"是由回归算法在数学上实现的——它把Y的变异分解成分别归因于各个X变量的成分。</p>
    <p class="method-desc" data-lang="en">Suppose you regress income on education alone: Income = 25,000 + 3,500 × Education. Now add family wealth as a control: Income = 20,000 + 2,000 × Education + 0.8 × Wealth. The education coefficient dropped from $3,500 to $2,000. This means: after accounting for family wealth, each additional year of education is associated with $2,000 more income (not $3,500). The difference, $1,500, is the portion of the original effect attributable to family wealth (confounding).</p>
    <p class="method-desc" data-lang="zh">假设你只用教育来预测收入：收入=25000+3500×教育。现在加上家庭财富作为控制变量：收入=20000+2000×教育+0.8×财富。教育系数从3500美元降到了2000美元。这表示：当我们考虑了家庭财富的影响后，每多一年教育平均关联收入增加2000美元（而不是3500美元）。两者相差的1500美元就是原始效应中由家庭财富造成的混淆部分。</p>
    <p class="method-desc" data-lang="en">But multiple regression can only control for variables you include. If an important confounder is unmeasured (e.g., parental intelligence, which affects both education and income), you cannot control for it, and bias remains. This is the problem of omitted variable bias (OVB): any variable excluded from the model that correlates with both Y and included X variables biases the coefficient estimates.</p>
    <p class="method-desc" data-lang="zh">但多元回归只能控制你包含在模型中的变量。如果有重要的混淆变量没被测量（比如父母的智力，既影响孩子的教育也影响收入），你就无法控制它，偏差仍然存在。这就是遗漏变量偏差（OVB）问题：任何被你排除在模型外、但与结果Y以及模型中的X变量都相关的变量，都会使系数估计产生偏差。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> A blind taste test of wine removes confounding from the label (price, prestige). Without controlling for the label, you might think expensive wine is better, but that could reflect psychology, not taste. Adding visual cues back (color, packaging) re-introduces confounding. Multiple regression is like running many experiments: you observe how Y changes as you vary X₁ while keeping X₂ constant (statistically), then repeat for other X variables.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>葡萄酒的盲味测试移除标签的混淆（价格、声望）。不控制标签，你可能认为贵葡萄酒更好，但那可能反映心理学，不是味道。添加视觉提示回（颜色、包装）重新引入混淆。多元回归像运行许多实验：你观察当你变化X₁同时保持X₂恒定（统计上）Y如何变化，然后对其他X变量重复。</div>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Use multiple regression when you have measured potential confounders. Include controls that are theoretically motivated — do not throw every available variable into the model hoping something sticks. Too many controls can introduce multicollinearity (see below) and reduce precision. A common guideline: include controls for variables that plausibly cause Y. Do not include mediators (variables that are caused by X and cause Y, through which X's effect flows); including mediators breaks the causal chain you want to estimate.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>当你已经测量了潜在的混淆变量时，用多元回归。要包含那些在理论上有正当理由的控制变量——不要把所有能找到的变量都扔进模型，期望其中某些能起作用。太多的控制变量会导致多重共线性（见下面）并降低精度。通用的指导原则是：包含那些有充分理由影响Y的变量。不要包含中介变量（被X导致、又导致Y的变量，通过这个变量X的效应才得以传递）；包含中介变量会破坏你想估计的因果路径。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher studies whether voting for environmental parties increases environmental behavior. In a simple regression, the coefficient is strong and positive: β = 0.8 (on a 0–4 scale of behavior). But voters for environmental parties may already care about the environment (selection, not causal effect). Adding a control for "baseline environmental concern" reduces the coefficient to β = 0.2. The change reveals confounding: much of the original association reflected prior attitudes, not the voting behavior's causal effect. The controlled coefficient (0.2) better estimates the true causal effect.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究者研究投票环保政党是否增加环保行为。在简单回归中，系数强且正：β=0.8（在0-4行为尺度）。但环保政党投票者可能已经关心环保（选择，不是因果效应）。添加"基线环保关注"控制将系数减少到β=0.2。变化揭示混淆：原始关联大部分反映先前态度，不是投票行为的因果效应。受控系数（0.2）更好地估计真实因果效应。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Multicollinearity: When Controls Go Wrong</h3>
    <h3 data-lang="zh">多重共线性：控制何时出错</h3>
    <p class="method-desc" data-lang="en">Multicollinearity occurs when two or more predictor variables are highly correlated with each other. This does not bias the coefficients (they are still unbiased estimates), but it inflates their standard errors, making confidence intervals wider and p-values larger. In the extreme case, if two variables are perfectly correlated, the regression cannot distinguish their separate effects.</p>
    <p class="method-desc" data-lang="zh">多重共线性发生在两个或多个预测变量彼此高度相关的时候。这不会使系数有偏差（它们仍然是无偏估计），但会膨胀它们的标准误，导致置信区间更宽、P值更大。在极端情况下，如果两个变量完全相关，回归就无法分辨它们各自独立的效应。</p>
    <p class="method-desc" data-lang="en">A simple example: if you include both temperature in Celsius and temperature in Fahrenheit as predictors, they are perfectly collinear (one is a linear transformation of the other). The regression will break — you cannot estimate separate effects because the variables are redundant. A practical example: if you include both unemployment rate and employment rate (which sum to the total labor force), they are nearly collinear. The regression will estimate very uncertain effects for each.</p>
    <p class="method-desc" data-lang="zh">一个简单的例子：如果你既包含摄氏温度又包含华氏温度作为预测，它们就完全共线（一个是另一个的线性变换）。回归会出问题——你无法估计它们的独立效应，因为这两个变量是冗余的。一个现实的例子：如果你同时包含失业率和就业率（它们加起来等于总劳动力），它们就几乎共线。回归会给出每个变量都极其不确定的效应估计。</p>
    <p class="method-desc" data-lang="en">How to detect multicollinearity? Calculate the variance inflation factor (VIF) for each variable. VIF &gt; 5 indicates problematic multicollinearity; VIF &gt; 10 is severe. Alternatively, look at correlation matrices — if two predictors have a correlation above 0.8, worry. The solution: drop one of the correlated variables, create a composite variable (e.g., average standardized scores), or use regularization methods (ridge regression, lasso) that shrink highly uncertain coefficients.</p>
    <p class="method-desc" data-lang="zh">怎么检测多重共线性？计算每个变量的方差膨胀因子（VIF）。VIF &gt; 5表示存在问题的多重共线性；VIF &gt; 10说明严重。或者看相关矩阵——如果两个预测器间的相关超过0.8，就要警惕。解决办法：舍弃一个相关的变量、创建一个合成变量（比如平均标准化分数），或者用能缩小那些高度不确定系数的正则化方法（岭回归、套索回归）。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Always check for multicollinearity when you have many controls. Do not include redundant variables (e.g., both "years of education" and "highest degree attained" if they measure the same thing). If you must include correlated variables, acknowledge the uncertainty in your results. Do not over-interpret individual coefficients when multicollinearity is present — focus on overall model fit and key predictors with lower VIF.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>当有很多控制变量时，要总是检查多重共线性。不要包含冗余变量（比如既有"教育年数"又有"最高学位"，如果它们测量的是同一件事）。如果必须包含相关的变量，要承认结果的不确定性。当存在多重共线性时，不要过度解释单个系数——要把重点放在整体模型拟合和那些VIF较低的关键预测变量上。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher models crime rates with predictors: poverty, unemployment, and per-capita income. These three variables are highly correlated (richer areas have lower poverty and unemployment). Each coefficient will be estimated with high uncertainty. The researcher might find the poverty coefficient is not statistically significant, not because poverty does not affect crime, but because unemployment and income capture that effect. The solution: drop one variable or combine them into a single "economic disadvantage" index. This improves interpretability and reduces multicollinearity.</div>
    <div data-lang="zh"><strong>社会科学例子：</strong>研究者用预测建模犯罪率：贫困、失业和人均收入。这三个变量高度相关（较富地区有较低贫困和失业）。每个系数将用高度不确定估计。研究者可能发现贫困系数不统计显著，不是因为贫困不影响犯罪，而是失业和收入捕获那个效应。解决：放弃一个变量或把它们合并成单个"经济劣势"指数。这改进了解释性和减少了多重共线性。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Bad Controls: Mistakes in Model Specification</h3>
    <h3 data-lang="zh">坏的控制：模型规范中的错误</h3>
    <p class="method-desc" data-lang="en">Not all variables should be controlled for. A "bad control" is a variable that is caused by the main predictor X. If you include it, you block the causal path from X to Y that you are trying to estimate. For example: Does education cause income? Suppose you regress income on education and also control for "job quality." But job quality is caused by education (more education → better jobs). By controlling for job quality, you remove part of education's effect, underestimating the true causal effect.</p>
    <p class="method-desc" data-lang="zh">并不是所有变量都应该被控制。"坏控制"是指被主要预测变量X导致的变量。如果你包含它，就会切断从X到Y的因果途径，而这正是你想要估计的。例如：教育会导致收入增加吗？假设你在预测收入时既用教育，也控制"工作质量"。但工作质量本身是由教育导致的（更多教育→更好的工作）。这样的话，通过控制工作质量，你就把教育的一部分效应去掉了，低估了教育的真实因果效应。</p>
    <p class="method-desc" data-lang="en">Another bad control is a "collider": a variable caused by both X and Y. If you condition on a collider (control for it), you create spurious correlation between X and Y even if none exists. This is subtle and often overlooked. An example: Does parental education affect children's educational attainment? Both parental and child education affect whether the child attends a selective school. If you analyze only children who attend selective schools (a collider), you artificially introduce correlation between parental and child education.</p>
    <p class="method-desc" data-lang="zh">另一种坏控制是"碰撞变量"：一个既被X导致、也被Y导致的变量。如果你对这个碰撞变量进行控制，即使X和Y之间本来没有关系，你也会在它们之间创造出虚假的相关。这很微妙，常常被人忽视。例子：父母的教育程度会影响孩子的教育成就吗？父母和孩子的教育程度都会影响孩子是否能进入顶尖学校。如果你只分析进入顶尖学校的孩子（这是碰撞变量），你就会人为地在父母教育和孩子教育之间引入相关关系。</p>
    <p class="method-desc" data-lang="en">The rule: include controls that cause both X and Y (true confounders). Exclude variables caused by X (mediators, unless you specifically want to study indirect effects). Exclude colliders. Think about causal direction before including a variable. Drawing a directed acyclic graph (DAG) of your variables and causal relationships helps clarify which variables to control for.</p>
    <p class="method-desc" data-lang="zh">规则是：包含那些既影响X又影响Y的控制变量（真正的混淆）。排除那些被X导致的变量（中介，除非你特别想研究间接效应）。排除碰撞变量。在包含一个变量之前，要思考清楚它的因果方向。绘制你的变量和它们因果关系的有向无环图（DAG）可以帮助你理清应该控制哪些变量。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Before specifying your regression model, list potential confounders, mediators, and colliders. Include confounders. Exclude mediators unless you are studying mediation. Be cautious about controlling for post-treatment variables (those measured after X occurs) — they often act as colliders. When in doubt, show results with and without a controversial control variable and discuss both.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>在规定你的回归模型之前，先列出潜在的混淆变量、中介变量和碰撞变量。要包含混淆变量。除非你专门在研究中介效应，否则要排除中介变量。小心处理处理后的变量（即在X发生之后才测量的变量）——它们经常充当碰撞变量的角色。如果不确定，可以同时展示有和没有某个有争议的控制变量的结果，并讨论两种情况。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher studies whether attending university increases political engagement. Controls might include: age (confounder: older people both go to university and are more engaged), income (confounder), and parental education (confounder). Bad controls: employment (mediated by university — university causes employment), or "political interest at age 25" (outcome, not a control). The researcher should include age, income, and parental education but exclude employment and later political interest measures.</div>
    <div data-lang="zh"><strong>社会科学例子：</strong>研究者研究上大学是否增加政治参与。控制可能包括：年龄（混淆：年长人既上大学也更参与）、收入（混淆）和父母教育（混淆）。坏控制：就业（由大学中介——大学导致就业），或"25岁时政治兴趣"（结果，不是控制）。研究者应包含年龄、收入和父母教育但排除就业和后来的政治兴趣措施。</div>
  </div>
</div>

<hr class="section-divider">

<!-- SECTION 3 -->
<div class="section" id="ols-s3">
  <h2 data-lang="en">Regression Diagnostics: Are the Assumptions Met?</h2>
  <h2 data-lang="zh">回归诊断：假设是否成立？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You run a regression and get coefficient estimates. But have you checked whether the data satisfy OLS assumptions? Violations of assumptions do not invalidate the point estimates but may invalidate standard errors, p-values, and confidence intervals. Residual plots (plots of errors) visually reveal assumption violations. This section covers how to diagnose problems and what to do about them.</p>
    <p data-lang="zh"><strong>问题：</strong>你跑了回归并得到了系数估计。但你检查过数据是否满足OLS的假设吗？假设被违反不会使点估计失效，但可能会使标准误、P值和置信区间失效。残差图（误差的图表）可以直观地揭示假设是否被违反。这一部分讲的是如何诊断问题以及应该怎么应对。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Residual Plots: What Each Pattern Means</h3>
    <h3 data-lang="zh">残差图：每个模式代表什么</h3>
    <p class="method-desc" data-lang="en">A residual is the difference between observed and predicted Y: e = Y - Ŷ. A residual plot graphs fitted values (predicted Y) on the x-axis and residuals on the y-axis. Key patterns tell you what is wrong: (1) A funnel shape (residuals spread wider as fitted values increase) indicates heteroskedasticity: error variance is not constant. (2) A curved pattern suggests nonlinearity: the relationship between X and Y is not linear. (3) Points above/below a horizontal line indicate outliers or extreme values.</p>
    <p class="method-desc" data-lang="zh">残差是观察到的Y值与预测Y值的差：e = Y - Ŷ。在残差图中，x轴绘制的是拟合值（预测Y），y轴绘制的是残差。不同的关键模式告诉你出现了什么问题：(1)漏斗形（残差随着拟合值增加而越来越分散）表示异方差：误差的方差不是恒定的。(2)曲线模式暗示非线性：X和Y之间的关系并不是线性的。(3)远离水平线的点表示异常值或极端值。</p>
    <p class="method-desc" data-lang="en">A good residual plot shows random scatter around zero with no pattern. Bad residual plots show systematic patterns. For example, in a study of income, heteroskedasticity is common: high-income individuals have much more variation in spending than low-income individuals. If you regress spending on income, the residual plot shows a funnel — the assumption of homoskedasticity is violated.</p>
    <p class="method-desc" data-lang="zh">好的残差图在零附近随机散布，没有任何模式。不好的残差图则显示系统性的模式。例如，在收入研究中，异方差很常见：高收入人群的支出变异远大于低收入人群。如果你用收入来预测支出，残差图就会呈现漏斗形——说明同方差假设被违反了。</p>
    <p class="method-desc" data-lang="en">A Q-Q plot (quantile-quantile plot) tests normality of residuals. It plots the quantiles of the residuals against the quantiles of a normal distribution. If residuals are normally distributed, points fall on a diagonal line. Deviations from the line (especially at the tails) indicate non-normality. Non-normality is less critical than other violations (with large samples, the Central Limit Theorem makes inference robust even if residuals are non-normal), but it is worth noting.</p>
    <p class="method-desc" data-lang="zh">Q-Q图（分位数对分位数图）用来检验残差是否服从正态分布。它将残差的分位数与正态分布的分位数进行对比。如果残差服从正态分布，点会落在对角线上。偏离对角线（特别是在尾部的偏离）表示非正态性。非正态性比其他假设的违反不那么关键（对于大样本，中心极限定理让推断即使在残差非正态时也很稳健），但仍然值得注意。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Always plot residuals. If heteroskedasticity is apparent, use robust standard errors (which adjust for varying error variance). If nonlinearity is evident, try transforming X (log, polynomial) or using a nonlinear model. If outliers are extreme, investigate whether they are data errors; if valid, either keep them (with robust methods) or report results with and without them. Large samples are forgiving — some heteroskedasticity or non-normality is tolerable if the effect is not driven by outliers.</div>
    <div data-lang="zh"><strong>何时用/不用：</strong>一定要绘制残差。如果异方差明显，就用稳健标准误（它们会调整误差方差不恒定的问题）。如果非线性明显，尝试对X进行转换（对数、多项式）或使用非线性模型。如果异常值很极端，要调查它们是否是数据录入错误；如果它们是真实的，可以选择保留它们（用稳健的方法）或报告有和没有这些异常值的结果。大样本通常比较宽容——只要效应不是由异常值驱动的，一定程度的异方差或非正态是可以接受的。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Heteroskedasticity and Robust Standard Errors</h3>
    <h3 data-lang="zh">异方差与稳健标准误</h3>
    <p class="method-desc" data-lang="en">Heteroskedasticity is the violation of the constant-variance assumption. It is extremely common in social science data. For example, wealthy people's spending varies much more than poor people's spending (everyone poor spends most of their income on basics; wealthy people have discretion). Income inequality studies almost always have heteroskedasticity: high-income earners vary more in their behavior.</p>
    <p class="method-desc" data-lang="zh">异方差是恒定方差假设的违反。在社会科学数据中极其常见。例如，富人的支出变异远大于穷人（穷人把大部分收入都花在基本需求上；富人有支配权选择怎么花钱）。在研究收入不平等时几乎总会遇到异方差：高收入群体的行为变异更大。</p>
    <p class="method-desc" data-lang="en">The problem with heteroskedasticity: it invalidates standard hypothesis tests. The standard error formula assumes constant variance, so it is wrong when variance is not constant. The good news: you do not need to re-estimate coefficients. Just use robust standard errors (also called Huber-White standard errors). These adjust for heteroskedasticity and give you valid confidence intervals and p-values even when error variance varies.</p>
    <p class="method-desc" data-lang="zh">异方差的问题在于：它会使标准的假设检验失效。标准误的公式假设方差恒定，所以当方差实际上不恒定时，公式就错了。好消息是：你不需要重新估计系数。只需使用稳健标准误（也叫Huber-White标准误）。这些方法会调整异方差的影响，即使误差方差不恒定，也能给你有效的置信区间和P值。</p>
    <p class="method-desc" data-lang="en">Robust SEs are almost always larger than ordinary SEs because they are more conservative (honest). Confidence intervals become wider, and some effects lose statistical significance. This is a feature, not a bug — it reflects the true uncertainty. In modern practice, you should always report robust SEs (they hurt when the OLS assumption holds and help when it does not, making them a good default). Some researchers report both ordinary and robust SEs to show the sensitivity of results to the choice.</p>
    <p class="method-desc" data-lang="zh">稳健SE几乎总是大于普通SE，因为它们更保守（更诚实）。置信区间会变宽，有些效应可能失去统计显著性。这不是缺陷，而是特点——它准确反映了真实的不确定性。在现代实践中，你应该总是报告稳健SE（当OLS假设成立时它们不会有坏处，当假设不成立时就能帮助你，所以它们是个很好的默认选择）。有些研究者会同时报告普通SE和稳健SE，以展示结果对这个选择的敏感程度。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Use robust standard errors by default in any regression. In R, use the sandwich or lmtest packages; in Stata, use regress ..., robust. Always report robust SEs in publications. If robust and ordinary SEs differ drastically, investigate heteroskedasticity further. Do not assume homoskedasticity holds just because you have a small sample — small samples can have heteroskedasticity too.</div>
    <div class="method-when" data-lang="zh"><strong>何时用/不用：</strong>在任何回归中都要默认使用稳健标准误。在R中，使用sandwich或lmtest包；在Stata中，使用regress ...，robust命令。在发表论文时，总是要报告稳健SE。如果稳健SE和普通SE差异很大，要进一步调查异方差的存在。不要仅因为样本小就假设同方差自动成立——小样本中也完全可能存在异方差。</div>
    <div data-lang="zh"><strong>何时用/不用：</strong>在任何回归中默认使用稳健标准误。在R中，使用sandwich或lmtest包；在Stata中，使用regress ...，robust。总是在出版中报告稳健SE。如果稳健和普通SE差异很大，进一步调查异方差。不要仅因样本小就假设同方差成立——小样本也可以有异方差。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher studies how family income affects children's educational outcomes. Ordinary regression yields: Outcome = 20 + 0.15 × Income, SE = 0.03, p &lt; 0.001. Robust SEs are: SE = 0.08. The robust confidence interval is [−0.01, 0.31], compared to the ordinary CI [0.09, 0.21]. The ordinary SE suggested tight precision; the robust SE reveals greater uncertainty. This is because families with high income have more variability in children's outcomes (heteroskedasticity). The robust result is more honest about the true precision.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究者研究家庭收入如何影响儿童教育成果。普通回归给出：成果=20+0.15×收入，SE=0.03，p&lt;0.001。稳健SE是：SE=0.08。稳健置信区间是[−0.01, 0.31]，宽于普通CI [0.09, 0.21]。普通SE暗示精度很高；稳健SE则揭示了更大的不确定性。这是因为高收入家庭的儿童教育成果变异更大（异方差存在）。稳健结果对真实精度更诚实。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Outliers, Influential Points, and Robustness Checks</h3>
    <h3 data-lang="zh">异常值、有影响点与稳健性检查</h3>
    <p class="method-desc" data-lang="en">An outlier is an observation with a large residual (far from the regression line). An influential point is an outlier that, if removed, substantially changes the regression coefficients. Some influential points are legitimate data; others are errors or special cases that distort the model. Identifying influential points is important because a few extreme cases can drive conclusions.</p>
    <p class="method-desc" data-lang="zh">异常值是指残差很大的观察（远离回归线）。有影响力的点是指一个异常值，如果被移除，会大幅改变回归系数。有些有影响力的点是合法的数据；其他的则是错误或特殊情况，会扭曲模型。识别有影响力的点很重要，因为少数极端情况可能会主导你的结论。</p>
    <p class="method-desc" data-lang="en">Methods for detecting influential points: Calculate Cook's distance (a measure of how much removing an observation changes the regression). Cook's distance &gt; 4/n (where n is sample size) suggests influence. Alternatively, look at leverage (how far an observation is from the center of X) and studentized residuals (standardized residuals accounting for leverage). An observation with high leverage and large residual is influential.</p>
    <p class="method-desc" data-lang="zh">检测有影响力点的方法：计算Cook距离（这是衡量移除某个观察会多大程度地改变回归的指标）。Cook距离&gt; 4/n（其中n是样本大小）暗示该点有影响。另一个方法是看杠杆（观察点到X的中心有多远）和学生化残差（考虑杠杆的标准化残差）。同时具有高杠杆和大残差的观察点就是有影响力的点。</p>
    <p class="method-desc" data-lang="en">What to do about influential points? First, investigate. Is it a data entry error? A special case (e.g., a billionaire in a sample of typical income earners)? If it is an error, fix or remove it. If it is real but extreme, you have options: report results both with and without the point; use robust regression (which downweights extreme points); or use quantile regression (which fits a line through the median rather than the mean). Transparency is key: inform readers about influential cases, even if you ultimately include them.</p>
    <p class="method-desc" data-lang="zh">关于有影响力点要怎么处理？首先，要调查一下。它是数据输入错误吗？还是一个特殊情况（比如在典型收入赚者样本中出现了一个亿万富翁）？如果是错误，就修复或删除它。如果是真实但极端的，你有几个选择：报告包含和不包含这个点的结果；使用稳健回归（它会减少极端点的权重）；或者使用分位数回归（它通过中位数而不是平均值来拟合线）。透明度很关键：要告知读者有这样的有影响力的情况存在，即使你最终决定保留它们。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Always check for influential points in your regression. Report Cook's distance or leverage diagnostics in appendices. When influential points exist, show results with and without them. If excluding influential points substantially changes your conclusions, that is important — readers need to know the findings are not robust. Use robust regression or quantile regression when extreme outliers are present and legitimate.</div>
    <div data-lang="zh"><strong>何时用/不用：</strong>一定要检查你的回归中是否存在有影响力的点。在附录中报告Cook距离或杠杆诊断。当存在有影响力的点时，要同时显示有和没有它们的结果。如果排除有影响力的点会大幅改变你的结论，这很重要——读者需要知道你的发现并不稳健。当确实存在极端异常值且它们是合法的时，就用稳健回归或分位数回归。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher studies the relationship between government spending and economic growth using 50 countries. Ordinary OLS gives: Growth = 2 + 0.3 × Spending. But one country (Norway, with very high spending and high growth) is highly influential. Removing Norway yields: Growth = 2 + 0.05 × Spending (a much weaker relationship). Both results are valid, but they tell different stories. The honest approach: report both, investigate why Norway is different (oil wealth causes both), and discuss whether the relationship holds for typical economies.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究者用50个国家来研究政府支出和经济增长的关系。普通OLS回归给出：增长=2+0.3×支出。但有一个国家（挪威，支出和增长都非常高）高度有影响。移除挪威后的结果：增长=2+0.05×支出（弱得多的关系）。两个结果都是有效的，但讲述的故事完全不同。诚实的做法是：同时报告两个结果，调查为什么挪威这么特殊（石油财富同时导致高支出和高增长），并讨论这个关系是否在典型经济体中成立。</div>
  </div>
</div>

<hr class="section-divider">

<!-- SECTION 4 -->
<div class="section" id="ols-s4">
  <h2 data-lang="en">Interactions and Nonlinearity</h2>
  <h2 data-lang="zh">交互项与非线性</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You estimate that economic growth increases government approval by 1% per point of growth. But does this relationship hold equally for all countries? Maybe in democracies, where voters hold leaders accountable and see economic conditions clearly, growth matters more. In autocracies, voters have less information and may credit growth less to leaders. The effect of X on Y might depend on another variable Z. This is an interaction: the effect is not constant but varies across contexts.</p>
    <p data-lang="zh"><strong>问题：</strong>你估计经济增长每增加一个百分点，就会让政府赞成度增加1%。但这个关系对所有国家都一样成立吗？也许在民主制国家，选民对领导人的表现有制衡力，也能清楚看到经济状况，所以增长的作用更大。在独裁制国家，选民信息较少，可能不太会把经济增长的功劳归给领导人。X对Y的效应可能取决于另一个变量Z。这就是交互效应：效应不是固定的，而是在不同背景下变化。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Interaction Terms: What the Coefficient Means</h3>
    <h3 data-lang="zh">交互项：系数代表什么</h3>
    <p class="method-desc" data-lang="en">An interaction term is a product of two variables: X₁ × X₂. When included in a regression, it tests whether the effect of X₁ on Y depends on the level of X₂. The model is: Y = β₀ + β₁X₁ + β₂X₂ + β₃(X₁ × X₂) + ε. The coefficient β₃ is the interaction effect: it tells you how much the effect of X₁ changes as X₂ increases by one unit.</p>
    <p class="method-desc" data-lang="zh">交互项是两个变量的乘积：X₁ × X₂。在回归中包含交互项，可以检验X₁对Y的效应是否依赖于X₂的取值。模型是：Y = β₀ + β₁X₁ + β₂X₂ + β₃(X₁ × X₂) + ε。系数β₃就是交互效应：它告诉你当X₂增加一个单位时，X₁的效应会改变多少。</p>
    <p class="method-desc" data-lang="en">Example: Economic Growth (X₁) and Democracy (X₂, coded 0=autocracy, 1=democracy) predict Approval (Y). The regression yields: Approval = 40 + 2 × Growth + 5 × Democracy + 1.5 × (Growth × Democracy). Interpretation: In an autocracy (Democracy=0), each point of growth increases approval by 2 percentage points. In a democracy (Democracy=1), each point of growth increases approval by 2 + 1.5 = 3.5 percentage points. The interaction coefficient (1.5) quantifies how much larger growth's effect is in democracies.</p>
    <p class="method-desc" data-lang="zh">例子：经济增长（X₁）和民主化程度（X₂，编码为0=独裁制，1=民主制）预测政府赞成度（Y）。回归结果是：赞成度=40+2×增长+5×民主化+1.5×（增长×民主化）。解释一下：在独裁制国家（民主化=0），每增加一个百分点的增长，赞成度增加2个百分点。在民主制国家（民主化=1），每增加一个百分点的增长，赞成度增加2+1.5=3.5个百分点。交互系数（1.5）量化了经济增长的效应在民主制国家中有多大的放大。</p>
    <p class="method-desc" data-lang="en">A common mistake: interpreting the main effects (β₁ and β₂) as unconditional effects when an interaction is present. They are not. β₁ (the effect of X₁) is conditional on X₂=0. If X₂=0 is not a meaningful value (e.g., "zero democracy" when democracy is coded 0-10), the main effect is hard to interpret. Solution: center variables by subtracting their mean before creating the interaction. Then the main effects represent effects at the mean value of the other variable, which is often meaningful.</p>
    <p class="method-desc" data-lang="zh">常见的错误是：当模型中有交互项时，把主效应（β₁和β₂）理解成无条件的效应。但它们不是。β₁（X₁的效应）是在X₂=0这个条件下的。如果X₂=0不是一个有意义的值（比如"零民主化"当民主化被编码为0-10），那主效应就很难解释。解决办法是：在创建交互项之前，通过减去每个变量的平均值来中心化它们。这样的话，主效应就代表在另一个变量均值处的效应，这通常更有意义。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> The effect of rain on mood depends on your circumstances. Rain helps a farmer (needed for crops) but ruins a picnicker's day (unwanted). Rain is X, your role is Z, and mood is Y. The main effect of rain (β₁) alone does not tell the full story; you need the interaction to know that rain's effect differs for farmers versus picnickers.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>雨对心情的效应取决于你的情况。雨帮助农民（作物需要）但毁了野餐者的日子（不需要）。雨是X，你的角色是Z，心情是Y。仅雨的主要效应（β₁）不讲完整故事；你需要交互知道雨的效应对农民对野餐者不同。</div>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Include an interaction only if you have a theoretical reason to expect it. Do not add interactions exploratory (searching for significance). If you include an interaction, always include the main effects (X₁ and X₂) even if they are not significant. Center variables before creating interactions to make main effects interpretable. Report both the interaction coefficient and simple slopes (the effect of X₁ at different levels of X₂) for clarity.</div>
    <div data-lang="zh"><strong>何时用/不用：</strong>只有当你有理论根据预期会存在交互效应时，才包含交互项。不要探索性地添加交互项（为了寻找统计显著）。如果包含了交互项，一定要同时包含主效应（X₁和X₂），即使它们不显著。在创建交互项之前要对变量进行中心化，以便主效应更易解释。要报告交互系数以及简单斜率（即X₁在X₂不同取值下的效应），这样会更清楚。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Nonlinearity: When a Straight Line Is Not Enough</h3>
    <h3 data-lang="zh">非线性：当直线不够</h3>
    <p class="method-desc" data-lang="en">Some relationships are not linear. Education's effect on income may flatten at high education levels (diminishing returns): the jump from 8th grade to 12th grade education increases income substantially, but the jump from a bachelor's to a master's degree does less. You can model nonlinearity in two ways: (1) Polynomial terms: include X² or even X³ to fit curves; (2) Log transformation: if Y (but not X) is right-skewed, use log(Y) as the outcome.</p>
    <p class="method-desc" data-lang="zh">有些关系并不是线性的。教育对收入的效应在教育水平较高时可能会变平（收益递减）：从8年级升到12年级，收入增幅很大，但从学士学位升到硕士学位，收入增幅就小得多。你可以用两种方式来模型化非线性关系：(1)多项式项：加入X²甚至X³来拟合曲线；(2)对数变换：如果Y（而不是X）呈右偏分布，可以用log(Y)作为结果变量。</p>
    <p class="method-desc" data-lang="en">Polynomial regression (adding X² or higher powers) can fit curved relationships. For example: Income = 25,000 + 3,000 × Education − 150 × Education². The negative coefficient on Education² means the effect of education decreases at higher levels. But be careful: polynomials can overfit (wobble at the edges) and are hard to interpret. Log transformations are often better. If income is right-skewed, regressing log(Income) on Education linearizes the relationship and is more interpretable: the coefficient on Education gives the percentage change in income per year of education (e.g., β = 0.08 means 8% increase per year).</p>
    <p class="method-desc" data-lang="zh">多项式回归（添加X²或更高次项）可以拟合弯曲的关系。例如：收入=25000+3000×教育-150×教育²。教育²项的负系数表示教育的效应在更高的教育水平处会递减。但要小心：多项式可能会过拟（在数据边缘出现摆动），而且也难以解释。对数转换通常效果更好。如果收入右偏分布，用教育来预测log（收入）可以线性化关系，也更容易解释：教育的系数给出的是教育每增加一年，收入的百分比增长（比如β=0.08表示每年增加8%）。</p>
    <p class="method-desc" data-lang="en">How to detect nonlinearity? Look at scatter plots with LOESS smoothers (from the Data Visualization chapter). If the LOESS curve deviates substantially from a straight line, nonlinearity is present. Residual plots also reveal nonlinearity: if residuals show a curved pattern (positive, then negative, then positive again), fit a polynomial. Always plot predictions from nonlinear models to ensure they make intuitive sense — some complex models produce nonsensical predictions outside the data range.</p>
    <p class="method-desc" data-lang="zh">怎么检测非线性？看带LOESS平滑器的散点图（来自数据可视化章）。如果LOESS曲线大幅偏离直线，就说明存在非线性。残差图也能揭示非线性：如果残差显示弧形模式（先正再负再正），就拟合多项式。一定要绘制来自非线性模型的预测值，以确保它们在直观上有意义——有些复杂模型会在数据范围之外产生无意义的预测。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> Use log transformations when data are skewed and the relationship makes sense on a log scale. Avoid polynomial terms unless scatter plots clearly suggest curvature. If you use polynomials, include lower-order terms (do not skip from X to X³). Test whether nonlinearity improves fit using a model comparison test. Report predictions or marginal effects (how Y changes as X changes) rather than just coefficients, as nonlinear models are harder to interpret from coefficients alone.</div>
    <div data-lang="zh"><strong>何时用/不用：</strong>当数据偏斜且关系在对数尺度上有意义时，使用对数转换。除非散点图清楚地显示曲率，否则避免使用多项式项。如果要使用多项式，要包含低阶项（不要从X直接跳到X³）。用模型比较检验来测试非线性是否改进了拟合。报告预测值或边际效应（即当X变化时Y是如何变化的），而不仅仅是系数，因为非线性模型仅从系数很难理解。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher studies how political experience (years in office) affects electoral success (vote share). A linear model estimates: VoteShare = 45 + 1.2 × Experience. But a scatter plot reveals that experience helps candidates with 0–15 years (slope steep) but provides little benefit beyond that (slope flat). A quadratic model fits better: VoteShare = 45 + 2 × Experience − 0.08 × Experience². The negative quadratic term captures diminishing returns: early experience boosts electoral prospects substantially, but additional experience past a threshold adds little. This nonlinear insight would be missed by a simple linear regression.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究者研究政治经验（任职年数）如何影响选举成功（得票率）。线性模型的估计是：得票率=45+1.2×经验。但散点图显示，对于有0-15年经验的候选人，经验的帮助很大（斜率陡），但超过15年的经验帮助就很小了（斜率平）。二次模型拟合得更好：得票率=45+2×经验-0.08×经验²。负的二次项捕捉了收益递减：早期的经验能大幅提升选举前景，但超过某个阈值后，额外的经验收益就微乎其微了。这种非线性的洞察力在简单的线性回归中是发现不了的。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Distinguishing Theory-Driven from Exploratory Analyses</h3>
    <h3 data-lang="zh">区分理论驱动与探索性分析</h3>
    <p class="method-desc" data-lang="en">Adding interactions and nonlinear terms should be driven by theory, not data exploration. A theory-driven analysis specifies the model before seeing results: "I predict that the effect of growth on approval is stronger in democracies, so I will include a Growth × Democracy interaction." An exploratory analysis comes after looking at the data: "I plotted growth vs. approval for each regime type and noticed the slopes differ, so I added an interaction."</p>
    <p class="method-desc" data-lang="zh">添加交互项和非线性项应该由理论驱动，而不是数据探索。理论驱动的分析要在看到结果之前就规定模型："我的理论预测增长对政府赞成的影响在民主制国家会更强，所以我将包含增长×民主化的交互项。"而探索性分析是在看过数据之后："我为不同的体制类型分别绘制了增长与赞成的关系，注意到斜率有所不同，所以我添加了交互项。"</p>
    <p class="method-desc" data-lang="en">The distinction matters because exploratory models are prone to overfitting: if you test many interactions and keep those that are significant, you will find spurious effects by chance. The solution: pre-register your analysis (commit to a model before seeing data), label exploratory findings as such, and use lower p-value thresholds for exploratory results (p &lt; 0.01 or p &lt; 0.001 instead of p &lt; 0.05). If exploratory analyses are interesting, replicate them in a new sample before publishing.

    </p>
    <p class="method-desc" data-lang="zh">这个区分很重要，因为探索性模型容易过拟：如果你测试许多交互项并只保留那些显著的，你就会通过随机机会发现虚假的效应。解决办法是：预注册你的分析（在看数据之前就承诺模型），清楚地标记探索性发现，对探索性结果使用更严格的P值阈值（p &lt; 0.01或p &lt; 0.001，而不是p &lt; 0.05）。如果探索性分析很有趣，要在新的样本上复制它们，然后再发表。</p>
    <p class="method-desc" data-lang="zh">区分重要因为探索性模型容易过度拟合：如果你测试许多交互并保留显著的，你会通过机会发现虚假效应。解决：预注册你的分析（看数据前承诺模型），标记探索性发现为此，对探索性结果用更低P值阈值（p &lt; 0.01或p &lt; 0.001而不是p &lt; 0.05）。如探索性分析有趣，在发表前在新样本中复制它们。</p>
    <div class="method-when" data-lang="en"><strong>When to use / not use:</strong> When possible, pre-register your regression model (coefficients, interactions, controls, transformations) before analyzing data. Clearly label post-hoc analyses and exploratory findings. In publications, separate confirmatory analyses (pre-specified) from exploratory analyses (data-driven). When reporting exploratory results, be honest about the multiple testing problem: if you tested 20 interactions, expect one to be "significant" by chance. Apply Bonferroni or similar corrections to exploratory p-values.</div>
    <div data-lang="zh"><strong>何时用/不用：</strong>尽可能在分析数据之前预注册你的回归模型（包括系数、交互、控制变量、转换等）。清楚地标记事后分析和探索性发现。在发表时，要分开报告确认分析（预指定的）和探索性分析（数据驱动的）。报告探索性结果时，对多重检验问题要诚实：如果你测试了20个交互项，就应该期待其中一个通过随机机会而"显著"。对探索性的P值应用Bonferroni或其他类似的纠正。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> A researcher pre-registers a hypothesis: "Economic growth's effect on government approval is moderated by regime type (democracy vs. autocracy)." The analysis includes a Growth × Regime interaction, specified before data analysis. The interaction is significant (p = 0.03) and aligns with theory. This is confirmatory. Later, the researcher explores whether the effect also depends on whether the country is in a natural-resource-rich region. A second interaction (Growth × Resources) is "significant" (p = 0.04). But this was exploratory, tested after seeing results. The researcher should report both but label the first as confirmatory and the second as exploratory. The replication standard for the exploratory finding should be higher (pre-register in a future study).</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>研究者预注册了一个假设："经济增长对政府赞成度的影响受到体制类型（民主vs独裁）的调节。"分析包含增长×体制的交互项，这是在数据分析之前就规定的。交互项显著（p=0.03），且与理论一致。这是确认性分析。后来，研究者进一步探索增长的效应是否还取决于该国是否位于天然资源丰富的地区。第二个交互项（增长×资源）"显著"（p=0.04）。但这是探索性的，是在看过结果之后才测试的。研究者应该把这两个都报告，但要标明第一个是确认性的，第二个是探索性的。对于探索性发现的复制标准应该更高（要在后续研究中预注册）。</div>
  </div>
</div>

<hr class="section-divider">
<!-- GUIDES -->
<div class="section" id="ols-guides">
  <h2 data-lang="en">Guides</h2>
  <h2 data-lang="zh">配套指南</h2>
  <div class="m-list">
    <a class="m-card" href="/methods/guides/reg-ols-visual.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">OLS Regression Playground</div>
        <div class="m-title" data-lang="zh">OLS 回归互动演示</div>
        <div class="m-desc" data-lang="en">Drag data points and watch the regression line, residuals, and R² update live.</div>
        <div class="m-desc" data-lang="zh">拖动数据点，实时看回归线、残差和 R² 如何变化。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
    <a class="m-card" href="/methods/guides/reg-ols-diagnostics.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">OLS Diagnostics Walkthrough</div>
        <div class="m-title" data-lang="zh">OLS 诊断实战</div>
        <div class="m-desc" data-lang="en">Step-by-step guide to checking all six OLS assumptions with R code and diagnostic plots.</div>
        <div class="m-desc" data-lang="zh">逐步检查六大 OLS 假设，含 R 代码和诊断图示。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
  </div>
</div>

<hr class="section-divider">
<!-- RESOURCES -->
<hr class="section-divider">
<div class="section" id="ols-resources">
  <h2 data-lang="en">Resources</h2>
  <h2 data-lang="zh">资源</h2>
  <h3 data-lang="en">Key References</h3>
  <h3 data-lang="zh">关键参考</h3>
  <ul class="resource-list">
    <li data-lang="en"><strong>Angrist, J. D., & Pischke, J.-S. (2008).</strong> <em>Mostly Harmless Econometrics: An Empiricist's Companion.</em> Princeton University Press.</li>
    <li data-lang="zh"><strong>安格里斯特与皮施克(2008)。</strong><em>基本上无伤计量经济学：经验研究者伴侣。</em>普林斯顿大学出版社。</li>
    <li data-lang="en"><strong>Pearl, J., & Mackenzie, D. (2018).</strong> <em>The Book of Why: The New Science of Cause and Effect.</em> Basic Books.</li>
    <li data-lang="zh"><strong>珀尔与麦肯锡(2018)。</strong><em>为什么之书：因果新科学。</em>Basic Books。</li>
    <li data-lang="en"><strong>Rotnitzky, A., & Robins, J. M. (2005).</strong> "Semiparametric Regression Estimation of Mean Treatment Effects." <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, 67(2), 301–323.</li>
    <li data-lang="zh"><strong>罗特尼茨基与罗宾斯(2005)。</strong>"平均处理效应的半参数回归估计。" <em>英国皇家统计学会杂志：B系列(统计方法)</em>，67(2)，301–323。</li>
    <li data-lang="en"><strong>Wooldridge, J. M. (2010).</strong> <em>Econometric Analysis of Cross Section and Panel Data (2nd ed.)</em>. MIT Press.</li>
    <li data-lang="zh"><strong>伍尔德里奇(2010)。</strong><em>截面和面板数据的计量经济分析(第2版)</em>。麻省理工学院出版社。</li>
  </ul>
</div>

<!-- PAGE NAV -->
<div class="page-nav">
  <a class="pn-link pn-prev" href="/methods/reg-dataviz.html">
    <span class="pn-arrow">&larr;</span>
    <span><span class="pn-title">Data Visualization</span></span>
  </a>
  <a class="pn-link pn-next" href="/methods/reg-mle.html">
    <span><span class="pn-title">MLE & Generalized Linear Models</span></span>
    <span class="pn-arrow">&rarr;</span>
  </a>
</div>
