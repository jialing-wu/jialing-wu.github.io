---
layout: methods-guide
title: "ERGM: Mathematical Foundations"
title_zh: "ERGM：数学基础"
parent_title: "Network Analysis"
parent_title_zh: "网络分析"
parent_url: "reg-network.html"
bilingual: true
mathjax: true
---

<!-- ═══════════════════════════════════════════════════
     LEVEL 1 — INTUITION (no math, pure story)
     ═══════════════════════════════════════════════════ -->

<h2 id="what-is-ergm">
  <span data-lang="en">What Is ERGM? (No Math Version)</span>
  <span data-lang="zh">ERGM 是什么？（零数学版）</span>
</h2>

<p data-lang="en">
Imagine you have a map of "who asks whom for advice" in an office. You want to understand <strong>why</strong> these connections exist. Is it because people ask those who are similar to them? Because advice is reciprocal? Because some people are just popular?
</p>

<p data-lang="zh">
想象你有一张办公室里"谁找谁请教"的关系图。你想理解这些连线<strong>为什么</strong>存在。是因为人们找和自己相似的人？因为请教是互相的？还是因为某些人就是受欢迎？
</p>

<p data-lang="en">
<strong>ERGM (Exponential Random Graph Model)</strong> is a statistical tool that answers this. Think of it as <strong>logistic regression, but for networks</strong>. In regular logistic regression, you predict whether an event happens (yes/no) based on some factors. In ERGM, you predict whether a tie between two people exists (yes/no) based on network patterns and personal traits.
</p>

<p data-lang="zh">
<strong>ERGM（指数随机图模型）</strong>就是回答这个问题的统计工具。你可以把它理解为<strong>给网络做的逻辑回归</strong>。普通逻辑回归根据一些因素预测某件事是否发生（是/否）。ERGM 根据网络模式和个人特征预测两个人之间的关系是否存在（是/否）。
</p>

<p data-lang="en">
<strong>Key difference from regular regression:</strong> In regular regression, each observation is independent. In a network, ties are <em>not</em> independent — whether A asks B for advice depends on whether A also asks C, whether B asks A back, etc. ERGM handles this interdependence.
</p>

<p data-lang="zh">
<strong>和普通回归的关键区别：</strong>普通回归里每个观测是独立的。但在网络里，关系<em>不是</em>独立的——A 是否请教 B，取决于 A 是否也请教了 C、B 是否反过来也请教 A，等等。ERGM 就是专门处理这种相互依赖的。
</p>

<h3 id="analogy">
  <span data-lang="en">An Analogy</span>
  <span data-lang="zh">一个类比</span>
</h3>

<p data-lang="en">
Think of a network like a jigsaw puzzle. Regular regression looks at each piece individually. ERGM looks at how pieces fit <em>together</em> — it asks: "Given the overall pattern of the puzzle, what makes this particular piece more likely to connect to that one?"
</p>

<p data-lang="zh">
把网络想象成一幅拼图。普通回归一块一块地看每个拼图碎片。ERGM 看的是碎片怎么<em>组合在一起</em>——它问的是："考虑到拼图的整体模式，是什么让这块碎片更可能和那块连在一起？"
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 2 — CONCEPTS (light math, building blocks)
     ═══════════════════════════════════════════════════ -->

<h2 id="why-not-simple-regression">
  <span data-lang="en">Why Can't We Just Use Regular Regression?</span>
  <span data-lang="zh">为什么不能直接用普通回归？</span>
</h2>

<p data-lang="en">
The simplest network model is the <strong>random graph</strong> (Bernoulli model): every pair of people has the same probability <em>p</em> of being connected, and each connection is independent. This is basically logistic regression with no predictors — just a coin flip for each pair.
</p>

<p data-lang="zh">
最简单的网络模型是<strong>随机图</strong>（伯努利模型）：每对人之间有相同的概率 <em>p</em> 产生连线，每条连线互相独立。这基本上就是没有预测变量的逻辑回归——对每对人掷一次硬币。
</p>

<p data-lang="en">
But real networks have patterns that random graphs cannot produce:
</p>

<p data-lang="zh">
但真实网络有随机图产生不了的规律：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Reciprocity:</strong> If I ask you for advice, you're likely to ask me too. In a random graph, this would just be coincidence.</li>
  <li data-lang="zh"><strong>互惠性：</strong>我找你请教，你很可能也来找我。在随机图里这只是巧合。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Clustering:</strong> If A knows B and B knows C, then A and C are likely to know each other ("friends of friends"). Random graphs don't produce this.</li>
  <li data-lang="zh"><strong>聚类：</strong>如果 A 认识 B，B 认识 C，那 A 和 C 也很可能认识（"朋友的朋友"）。随机图不会产生这种结构。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Popularity:</strong> Some people get asked by many others (hubs). Random graphs give everyone roughly the same number of connections.</li>
  <li data-lang="zh"><strong>人气差异：</strong>有些人被很多人请教（中心节点）。随机图让每个人的连接数差不多。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Homophily:</strong> People connect with those similar to them (same age, same skill level). Random graphs ignore personal traits entirely.</li>
  <li data-lang="zh"><strong>同质性：</strong>人们和相似的人交往（同年龄、同技能水平）。随机图完全忽略个人特征。</li>
</ul>

<p data-lang="en">
<strong>ERGM solves this</strong> by letting the probability of each tie depend on these network patterns — not just individual traits, but the <em>structure</em> of the network itself.
</p>

<p data-lang="zh">
<strong>ERGM 的解决方法</strong>是让每条关系的概率取决于这些网络模式——不仅是个人特征，还有网络<em>本身的结构</em>。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 3 — THE CORE FORMULA (accessible math)
     ═══════════════════════════════════════════════════ -->

<h2 id="ergm-formula">
  <span data-lang="en">The Core Formula</span>
  <span data-lang="zh">核心公式</span>
</h2>

<p data-lang="en">
ERGM says: the probability of observing a particular network depends on how many "interesting patterns" it contains. Written as a formula:
</p>

<p data-lang="zh">
ERGM 说：观察到某个特定网络的概率，取决于它包含多少"有趣的模式"。写成公式：
</p>

<div class="math-note">
P(Y = y) = exp(θ₁ × g₁(y) + θ₂ × g₂(y) + ... ) / κ
</div>

<p data-lang="en">
Don't panic — let's unpack each piece:
</p>

<p data-lang="zh">
别慌——我们一个一个拆解：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>g₁(y), g₂(y), ...</strong> = <strong>network statistics</strong>. These are things you count in the network: how many ties? how many reciprocal pairs? how many triangles? how many ties between people of the same job level? Each g is just a count.</li>
  <li data-lang="zh"><strong>g₁(y), g₂(y), ...</strong> = <strong>网络统计量</strong>。就是你在网络里能数的东西：多少条关系？多少对互惠？多少个三角形？多少条关系发生在同职级的人之间？每个 g 就是一个计数。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>θ₁, θ₂, ...</strong> = <strong>parameters (weights)</strong>. These are what ERGM estimates. A positive θ means "more of this pattern makes the network more likely." A negative θ means "more of this pattern makes it less likely." Just like coefficients in logistic regression.</li>
  <li data-lang="zh"><strong>θ₁, θ₂, ...</strong> = <strong>参数（权重）</strong>。这是 ERGM 要估计的。正的 θ 意味着"这个模式越多，网络越可能"。负的 θ 意味着"越多越不可能"。和逻辑回归的系数一样。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>exp(...)</strong> = just makes all values positive (probabilities must be positive).</li>
  <li data-lang="zh"><strong>exp(...)</strong> = 只是让所有值变成正数（概率必须为正）。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>κ</strong> = a <strong>normalizing constant</strong> that makes all probabilities sum to 1. You never compute it directly (more on this later).</li>
  <li data-lang="zh"><strong>κ</strong> = 一个<strong>归一化常数</strong>，让所有概率加起来等于 1。你永远不用直接算它（后面会解释为什么）。</li>
</ul>

<h3 id="reading-like-regression">
  <span data-lang="en">Reading ERGM Like Logistic Regression</span>
  <span data-lang="zh">像读逻辑回归一样读 ERGM</span>
</h3>

<p data-lang="en">
The really useful version of the formula is the <strong>tie-level</strong> version. For any specific pair (i, j), the probability that they are connected is:
</p>

<p data-lang="zh">
真正实用的版本是<strong>关系层面</strong>的公式。对于任意一对 (i, j)，他们之间有关系的概率是：
</p>

<div class="math-note">
log(odds of tie i→j) = θ₁ × Δg₁ + θ₂ × Δg₂ + ...
</div>

<p data-lang="en">
where <strong>Δg</strong> ("change statistic") = how much each network statistic changes if we add this one tie. This is exactly logistic regression form! The only difference: in regular logistic regression, predictors are fixed numbers (age, income). In ERGM, predictors <em>change</em> depending on the rest of the network.
</p>

<p data-lang="zh">
其中 <strong>Δg</strong>（"变化统计量"）= 如果我们加上这一条关系，每个网络统计量变化多少。这和逻辑回归的形式完全一样！唯一区别：普通逻辑回归的预测变量是固定数字（年龄、收入）。ERGM 的预测变量会随网络其他部分<em>变化</em>。
</p>

<p data-lang="en">
<strong>Example:</strong> Suppose your model has reciprocity and homophily on tenure. For the pair (Alice → Bob):
</p>

<p data-lang="zh">
<strong>举例：</strong>假设模型包含互惠性和工龄同质性。对于 (Alice → Bob) 这对：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en">Δg<sub>reciprocity</sub> = 1 if Bob already asks Alice (adding Alice→Bob creates a mutual pair), 0 otherwise</li>
  <li data-lang="zh">Δg<sub>互惠</sub> = 1（如果 Bob 已经找 Alice 请教，加上 Alice→Bob 就形成互惠对），否则为 0</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en">Δg<sub>tenure homophily</sub> = 1 if Alice and Bob have similar tenure, 0 otherwise</li>
  <li data-lang="zh">Δg<sub>工龄同质性</sub> = 1（如果 Alice 和 Bob 工龄相近），否则为 0</li>
</ul>

<p data-lang="en">
If θ<sub>reciprocity</sub> = 2.0 and θ<sub>tenure</sub> = 0.5, and Bob already asks Alice and they have similar tenure, then: log(odds) = 2.0 × 1 + 0.5 × 1 = 2.5, meaning Alice is quite likely to ask Bob.
</p>

<p data-lang="zh">
如果 θ<sub>互惠</sub> = 2.0，θ<sub>工龄</sub> = 0.5，而且 Bob 已经找 Alice 请教、两人工龄相近，那么：log(odds) = 2.0 × 1 + 0.5 × 1 = 2.5，说明 Alice 很可能会找 Bob 请教。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 4 — HOW IT'S ESTIMATED (MCMC intuition)
     ═══════════════════════════════════════════════════ -->

<h2 id="estimation">
  <span data-lang="en">How ERGM Finds the Parameters</span>
  <span data-lang="zh">ERGM 怎么找到参数</span>
</h2>

<p data-lang="en">
Here's the computational challenge: that normalizing constant κ requires summing over <strong>every possible network</strong>. For 10 people, there are 2<sup>45</sup> ≈ 35 trillion possible networks. For 100 people, the number is unimaginable. You simply cannot enumerate them all.
</p>

<p data-lang="zh">
计算上的挑战是：归一化常数 κ 需要对<strong>所有可能的网络</strong>求和。10 个人就有 2<sup>45</sup> ≈ 35 万亿种可能的网络。100 个人的话，数字大到无法想象。你根本没法一个一个列举。
</p>

<p data-lang="en">
<strong>Solution: MCMC (Markov Chain Monte Carlo).</strong> Instead of checking every possible network, the computer does this:
</p>

<p data-lang="zh">
<strong>解决办法：MCMC（马尔可夫链蒙特卡罗）。</strong>不逐一检查每个网络，计算机这样做：
</p>

<ol style="margin-left: 2rem;">
  <li data-lang="en">Start with the observed network and an initial guess for θ.</li>
  <li data-lang="zh">从观察到的网络和初始猜测的 θ 开始。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en">Randomly add or remove one tie at a time, creating simulated networks.</li>
  <li data-lang="zh">每次随机加上或去掉一条关系，生成模拟网络。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en">Compare: do the simulated networks' statistics match the real network? If the simulations have too few triangles compared to reality, increase θ<sub>triangle</sub>. If too many, decrease it.</li>
  <li data-lang="zh">比较：模拟网络的统计量和真实网络一致吗？如果模拟的三角形太少，就增大 θ<sub>三角形</sub>。太多就减小。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en">Repeat until the simulated networks look like the real one. Then θ is your answer.</li>
  <li data-lang="zh">重复，直到模拟网络看起来像真实网络。这时候的 θ 就是你的答案。</li>
</ol>

<p data-lang="en">
<strong>In plain language:</strong> ERGM finds parameters by repeatedly asking "if these were the true rules of the network, would we see something like the real network?" It tweaks the rules until the answer is yes.
</p>

<p data-lang="zh">
<strong>通俗说：</strong>ERGM 反复问"如果这些是网络的真实规则，我们能看到类似真实网络的东西吗？"然后不断调整规则直到答案是"能"。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 5 — COMMON PITFALLS
     ═══════════════════════════════════════════════════ -->

<h2 id="pitfalls">
  <span data-lang="en">Common Pitfalls</span>
  <span data-lang="zh">常见陷阱</span>
</h2>

<h3 id="degeneracy">
  <span data-lang="en">The Degeneracy Problem</span>
  <span data-lang="zh">退化问题</span>
</h3>

<p data-lang="en">
Some model specifications break ERGM. The classic example: if you use a raw "triangle count" term, the model can become unstable — it only considers networks that are almost empty or almost completely connected, ignoring everything in between. This is called <strong>degeneracy</strong>.
</p>

<p data-lang="zh">
有些模型设定会让 ERGM 崩溃。经典例子：如果你用原始的"三角形计数"项，模型会变得不稳定——它只考虑几乎没有连线或几乎全部连满的网络，忽略中间状态。这叫<strong>退化</strong>。
</p>

<p data-lang="en">
<strong>Why?</strong> Each new triangle makes more triangles easier to form (positive feedback loop). The model either collapses to empty or explodes to full.
</p>

<p data-lang="zh">
<strong>为什么？</strong>每多一个三角形都让形成更多三角形变得更容易（正反馈循环）。模型要么坍缩到空网络，要么爆炸到全连接。
</p>

<p data-lang="en">
<strong>Solution:</strong> Use <strong>geometrically weighted terms</strong> (GWESP, GWDegree). These add diminishing returns — the first shared friend matters a lot, the second less, the third even less. This prevents the runaway feedback.
</p>

<p data-lang="zh">
<strong>解决办法：</strong>使用<strong>几何加权项</strong>（GWESP、GWDegree）。它们引入递减效应——第一个共同朋友很重要，第二个没那么重要，第三个更不重要。这能防止失控的正反馈。
</p>

<h3 id="convergence">
  <span data-lang="en">Convergence Issues</span>
  <span data-lang="zh">收敛问题</span>
</h3>

<p data-lang="en">
Since ERGM uses MCMC (a simulation-based method), it can fail to converge — meaning the computer hasn't found stable parameter values. Signs of trouble: parameter estimates keep changing, or simulated networks look nothing like the real one.
</p>

<p data-lang="zh">
因为 ERGM 用的是 MCMC（基于模拟的方法），它可能不收敛——意思是计算机没找到稳定的参数值。出问题的迹象：参数估计一直在变，或者模拟出来的网络和真实网络完全不像。
</p>

<p data-lang="en">
<strong>Fixes:</strong> Increase the number of simulations (burn-in, sample size). Use geometrically weighted terms. Simplify the model — start small, then add terms one at a time.
</p>

<p data-lang="zh">
<strong>修复方法：</strong>增加模拟次数（burn-in、样本量）。使用几何加权项。简化模型——从小模型开始，逐个添加项。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 6 — READING OUTPUT
     ═══════════════════════════════════════════════════ -->

<h2 id="reading-output">
  <span data-lang="en">How to Read ERGM Output</span>
  <span data-lang="zh">怎么读 ERGM 输出</span>
</h2>

<p data-lang="en">
ERGM output looks like logistic regression output. For each term you get an estimate, standard error, and p-value:
</p>

<p data-lang="zh">
ERGM 输出看起来像逻辑回归输出。每个项都有估计值、标准误和 p 值：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Positive estimate:</strong> This pattern increases the probability of a tie. Example: θ<sub>reciprocity</sub> = +2.1 → ties tend to be reciprocal.</li>
  <li data-lang="zh"><strong>正的估计值：</strong>这个模式增加关系形成的概率。例：θ<sub>互惠</sub> = +2.1 → 关系倾向于互惠。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Negative estimate:</strong> This pattern decreases the probability. Example: θ<sub>edges</sub> = −3.5 → the network is sparse (most pairs are NOT connected).</li>
  <li data-lang="zh"><strong>负的估计值：</strong>这个模式降低概率。例：θ<sub>边</sub> = −3.5 → 网络是稀疏的（大多数人之间没有连线）。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>p &lt; 0.05:</strong> The effect is statistically significant.</li>
  <li data-lang="zh"><strong>p &lt; 0.05：</strong>效应在统计上显著。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Larger |estimate|:</strong> Stronger effect.</li>
  <li data-lang="zh"><strong>|估计值| 越大：</strong>效应越强。</li>
</ul>

<p data-lang="en">
<strong>Goodness-of-fit check:</strong> After fitting, simulate 100+ networks from the model and compare their statistics to the real network. If the real network's statistics fall within the range of simulated ones, the model fits well.
</p>

<p data-lang="zh">
<strong>拟合优度检查：</strong>拟合之后，从模型模拟 100+ 个网络，把它们的统计量和真实网络比较。如果真实网络的统计量在模拟范围之内，说明模型拟合得不错。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 7 — R CODE
     ═══════════════════════════════════════════════════ -->

<h2 id="r-code">
  <span data-lang="en">R Code Example</span>
  <span data-lang="zh">R 代码示例</span>
</h2>

<pre style="background: var(--ink); color: var(--cream); padding: 1.5rem; border-radius: 0.5rem; overflow-x: auto; font-size: 0.85em; line-height: 1.4;">
# Load package
library(ergm)

# Use built-in dataset
data(florentine)

# Fit a simple model:
#   edges       → controls overall density (like an intercept)
#   mutual      → do ties tend to be reciprocal?
#   gwesp(0.5)  → do friends-of-friends connect? (geometrically weighted)
model &lt;- ergm(flo_net ~ edges + mutual + gwesp(0.5, fixed = TRUE),
              control = control.ergm(seed = 123))

# View results — read just like logistic regression
summary(model)

# Check model fit: simulate networks and compare to real one
gof_result &lt;- gof(model)
plot(gof_result)
# Good fit = observed statistics (red line) within simulated range (boxplots)
</pre>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<!-- ═══════════════════════════════════════════════════
     LEVEL 8 — DEEP DIVE (full math for advanced readers)
     ═══════════════════════════════════════════════════ -->

<h2 id="deep-dive">
  <span data-lang="en">Deep Dive: Full Mathematical Details</span>
  <span data-lang="zh">深入：完整数学细节</span>
</h2>

<p data-lang="en">
<em>This section is for readers comfortable with probability theory and statistical modeling. If you understood the sections above, you already have enough to read ERGM papers. The math below explains the "why" behind each step.</em>
</p>

<p data-lang="zh">
<em>本节面向熟悉概率论和统计建模的读者。如果你看懂了上面的内容，已经足够读 ERGM 论文了。下面的数学解释每一步背后的"为什么"。</em>
</p>

<h3 id="formal-model">
  <span data-lang="en">Formal Probability Model</span>
  <span data-lang="zh">形式概率模型</span>
</h3>

<div class="math-note">
P(Y = y | θ) = exp(θ<sup>T</sup> g(y)) / κ(θ)
</div>

<p data-lang="en">
where <strong>g(y)</strong> is a vector of sufficient statistics, <strong>θ</strong> is the parameter vector, and <strong>κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*))</strong> sums over all 2<sup>n(n−1)/2</sup> possible networks (undirected) or 2<sup>n(n−1)</sup> (directed). This is an exponential family distribution.
</p>

<p data-lang="zh">
其中 <strong>g(y)</strong> 是充分统计量向量，<strong>θ</strong> 是参数向量，<strong>κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*))</strong> 对所有 2<sup>n(n−1)/2</sup>（无向）或 2<sup>n(n−1)</sup>（有向）个可能网络求和。这是指数族分布。
</p>

<h3 id="change-statistic">
  <span data-lang="en">Change Statistic and Conditional Log-Odds</span>
  <span data-lang="zh">变化统计量与条件对数几率</span>
</h3>

<p data-lang="en">
Conditioning on all other ties (Y<sub>−ij</sub>), the probability of tie (i, j) existing is:
</p>

<p data-lang="zh">
在给定所有其他关系（Y<sub>−ij</sub>）的条件下，关系 (i, j) 存在的概率为：
</p>

<div class="math-note">
logit P(Y<sub>ij</sub> = 1 | Y<sub>−ij</sub>) = θ<sup>T</sup> δg<sub>ij</sub>
</div>

<p data-lang="en">
where δg<sub>ij</sub> = g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>) is the change in network statistics when edge (i, j) is toggled. The normalizing constant κ cancels out, making this computationally tractable.
</p>

<p data-lang="zh">
其中 δg<sub>ij</sub> = g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>) 是翻转边 (i, j) 时网络统计量的变化。归一化常数 κ 被约掉了，使计算变得可行。
</p>

<h3 id="mcmc-mle">
  <span data-lang="en">MCMC-MLE Algorithm</span>
  <span data-lang="zh">MCMC-MLE 算法</span>
</h3>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Initialize</strong> with observed network y<sub>obs</sub> and initial θ<sup>(0)</sup>.</li>
  <li data-lang="zh"><strong>初始化：</strong>观察网络 y<sub>obs</sub> 和初始 θ<sup>(0)</sup>。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Simulate</strong> via Metropolis-Hastings: propose y' by toggling one edge; accept with probability α = min(1, exp(θ<sup>T</sup>[g(y') − g(y<sup>(t)</sup>)])).</li>
  <li data-lang="zh"><strong>模拟：</strong>Metropolis-Hastings 方法——翻转一条边提议 y'，以概率 α = min(1, exp(θ<sup>T</sup>[g(y') − g(y<sup>(t)</sup>)])) 接受。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>After burn-in</strong>, collect S thinned samples. Compute ḡ<sub>sim</sub> = (1/S) Σ g(y<sup>(t)</sup>).</li>
  <li data-lang="zh"><strong>灼烧期后</strong>，收集 S 个稀疏样本。计算 ḡ<sub>sim</sub> = (1/S) Σ g(y<sup>(t)</sup>)。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Update:</strong> θ<sup>(t+1)</sup> = θ<sup>(t)</sup> + step × [g(y<sub>obs</sub>) − ḡ<sub>sim</sub>]. This is stochastic gradient ascent on the log-likelihood.</li>
  <li data-lang="zh"><strong>更新：</strong>θ<sup>(t+1)</sup> = θ<sup>(t)</sup> + step × [g(y<sub>obs</sub>) − ḡ<sub>sim</sub>]。这是对数似然的随机梯度上升。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Converge</strong> when g(y<sub>obs</sub>) ≈ ḡ<sub>sim</sub>.</li>
  <li data-lang="zh"><strong>收敛：</strong>当 g(y<sub>obs</sub>) ≈ ḡ<sub>sim</sub> 时停止。</li>
</ol>

<h3 id="degeneracy-math">
  <span data-lang="en">Degeneracy: Mathematical Explanation</span>
  <span data-lang="zh">退化：数学解释</span>
</h3>

<p data-lang="en">
For a model with edges and triangles: P(Y = y) ∝ exp(θ<sub>E</sub>|E| + θ<sub>T</sub>|T|). When θ<sub>T</sub> &gt; 0, the change statistic for edge (i,j) includes +2L<sub>ij</sub> (twice the number of common neighbors). In dense regions, L<sub>ij</sub> is large → adding edges becomes self-reinforcing → the distribution becomes bimodal (near-empty or near-complete). <strong>Geometrically weighted terms</strong> (GWESP) fix this by applying exponential decay: each additional shared partner contributes λ times less (0 &lt; λ &lt; 1), capping the positive feedback.
</p>

<p data-lang="zh">
对于包含边和三角形的模型：P(Y = y) ∝ exp(θ<sub>E</sub>|E| + θ<sub>T</sub>|T|)。当 θ<sub>T</sub> &gt; 0 时，边 (i,j) 的变化统计量包含 +2L<sub>ij</sub>（公共邻居数的两倍）。在密集区域，L<sub>ij</sub> 很大 → 加边自我强化 → 分布变成双峰的（几乎空或几乎满）。<strong>几何加权项</strong>（GWESP）通过指数衰减来修复：每增加一个共享伙伴的贡献乘以 λ（0 &lt; λ &lt; 1），限制了正反馈。
</p>

<h3 id="convergence-diagnostics">
  <span data-lang="en">Convergence Diagnostics Checklist</span>
  <span data-lang="zh">收敛诊断清单</span>
</h3>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Trace plots:</strong> θ<sup>(t)</sup> should look like white noise around a mean. Drift = not converged.</li>
  <li data-lang="zh"><strong>轨迹图：</strong>θ<sup>(t)</sup> 应像围绕均值的白噪声。漂移 = 未收敛。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>GOF plots:</strong> Observed statistics should fall within simulated range.</li>
  <li data-lang="zh"><strong>GOF 图：</strong>观察统计量应在模拟范围之内。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Effective sample size (ESS):</strong> Should be ≥ 100 per parameter. ESS = S / (1 + 2Σρ<sub>k</sub>).</li>
  <li data-lang="zh"><strong>有效样本量（ESS）：</strong>每个参数应 ≥ 100。ESS = S / (1 + 2Σρ<sub>k</sub>)。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Gelman-Rubin PSRF:</strong> Run multiple chains; PSRF ≈ 1.0 indicates convergence. &gt; 1.05 = problem.</li>
  <li data-lang="zh"><strong>Gelman-Rubin PSRF：</strong>运行多条链；PSRF ≈ 1.0 表示收敛。&gt; 1.05 = 有问题。</li>
</ul>

<div class="insight-box" style="background: var(--warm); color: white; padding: 1rem; border-radius: 0.3rem; margin-top: 1rem;">
  <p data-lang="en"><strong>Summary:</strong> ERGM is logistic regression for networks. It predicts ties based on network patterns (reciprocity, clustering, homophily) and personal traits. The key challenge is estimation (MCMC) and model stability (degeneracy). Start simple, check convergence, and use geometrically weighted terms.</p>
  <p data-lang="zh"><strong>总结：</strong>ERGM 是给网络做的逻辑回归。它根据网络模式（互惠、聚类、同质性）和个人特征预测关系。关键挑战是估计（MCMC）和模型稳定性（退化）。从简单模型开始，检查收敛，使用几何加权项。</p>
</div>
