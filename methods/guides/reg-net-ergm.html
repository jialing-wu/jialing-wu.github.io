---
layout: methods-guide
title: "ERGM: From Intuition to Math"
title_zh: "ERGM：从直觉到数学"
parent_title: "Network Analysis"
parent_title_zh: "网络分析"
parent_url: "reg-network.html"
bilingual: true
mathjax: true
---

<!-- ═══════════════════════════════════════════════════
     LEVEL 1 — THE 30-SECOND VERSION
     ═══════════════════════════════════════════════════ -->

<h2 id="what-is-ergm">
  <span data-lang="en">What Is ERGM?</span>
  <span data-lang="zh">ERGM 是什么？</span>
</h2>

<p data-lang="en">
Imagine you have a map of "who asks whom for advice" in an office. You want to understand <strong>why</strong> these connections exist. Is it because people ask those who are similar to them? Because advice is reciprocal? Because some people are just popular?
</p>

<p data-lang="zh">
想象你有一张办公室里"谁找谁请教"的关系图。你想理解这些连线<strong>为什么</strong>存在。是因为人们找和自己相似的人？因为请教是互相的？还是因为某些人就是受欢迎？
</p>

<div class="method-when">
  <div data-lang="en">One-sentence definition</div>
  <div data-lang="zh">一句话定义</div>
  <p data-lang="en"><strong>ERGM = logistic regression for networks.</strong> Regular logistic regression predicts whether an event happens (yes/no) based on some factors. ERGM predicts whether a <em>tie between two people</em> exists (yes/no) based on network patterns and personal traits.</p>
  <p data-lang="zh"><strong>ERGM = 给网络做的逻辑回归。</strong>普通逻辑回归根据因素预测事件是否发生（是/否）。ERGM 根据网络模式和个人特征预测两人之间的<em>关系</em>是否存在（是/否）。</p>
</div>

<p data-lang="en">
<strong>Key difference from regular regression:</strong> in regular regression, each observation is independent. In a network, ties are <em>not</em> independent — whether Alice asks Bob depends on whether Alice also asks Carol, whether Bob asks Alice back, etc. ERGM is built to handle this.
</p>

<p data-lang="zh">
<strong>和普通回归的关键区别：</strong>普通回归里每个观测是独立的。但在网络里，关系<em>不是</em>独立的——Alice 是否请教 Bob，取决于她是否也请教了 Carol、Bob 是否反过来请教她等等。ERGM 就是专门处理这种相互依赖的。
</p>

<div class="method-analogy">
  <div data-lang="en">Analogy</div>
  <div data-lang="zh">类比</div>
  <p data-lang="en">Think of a network like a jigsaw puzzle. Regular regression looks at each piece individually. ERGM looks at how pieces fit <em>together</em> — it asks: "given the overall pattern, what makes this piece more likely to connect to that one?"</p>
  <p data-lang="zh">把网络想象成一幅拼图。普通回归一块一块地看。ERGM 看的是碎片怎么<em>组合在一起</em>——它问："考虑到整体模式，是什么让这块更可能和那块连在一起？"</p>
</div>

<!-- ═══════════════════════════════════════════════════
     LEVEL 2 — WHY NOT REGRESSION?
     ═══════════════════════════════════════════════════ -->

<hr class="section-divider">
<h2 id="why-not-regression">
  <span data-lang="en">Why Can't We Just Use Regular Regression?</span>
  <span data-lang="zh">为什么不能直接用普通回归？</span>
</h2>

<p data-lang="en">
The simplest network model is the <strong>random graph</strong>: every pair has the same probability <em>p</em> of being connected, independently. That is basically logistic regression with no predictors — just a coin flip for each pair.
</p>

<p data-lang="zh">
最简单的网络模型是<strong>随机图</strong>：每对人之间有相同的概率 <em>p</em> 产生连线，互相独立。这基本上就是没有预测变量的逻辑回归——对每对人掷硬币。
</p>

<p data-lang="en">
But real networks have patterns that random graphs can't produce:
</p>

<p data-lang="zh">
但真实网络有随机图产生不了的规律：
</p>

<h4 data-lang="en">Reciprocity</h4>
    <h4 data-lang="zh">互惠性</h4>
    <p data-lang="en">I ask you for advice, you're likely to ask me too. In a random graph, this would just be coincidence.</p>
    <p data-lang="zh">我找你请教，你很可能也来找我。在随机图里这只是巧合。</p>
    <h4 data-lang="en">Clustering</h4>
    <h4 data-lang="zh">聚类</h4>
    <p data-lang="en">If A knows B and B knows C, then A and C likely know each other. "Friends of friends." Random graphs don't produce this.</p>
    <p data-lang="zh">如果 A 认识 B，B 认识 C，那 A 和 C 也很可能认识。"朋友的朋友。" 随机图不会产生这种结构。</p>
<h4 data-lang="en">Popularity</h4>
    <h4 data-lang="zh">人气差异</h4>
    <p data-lang="en">Some people get asked by many others (hubs). Random graphs give everyone roughly equal connections.</p>
    <p data-lang="zh">有些人被很多人请教（中心节点）。随机图让每个人的连接数差不多。</p>
    <h4 data-lang="en">Homophily</h4>
    <h4 data-lang="zh">同质性</h4>
    <p data-lang="en">People connect with those similar to them (same age, same department). Random graphs ignore personal traits entirely.</p>
    <p data-lang="zh">人们和相似的人交往（同年龄、同部门）。随机图完全忽略个人特征。</p>

<div class="method-when">
  <div data-lang="en">Key point</div>
  <div data-lang="zh">要点</div>
  <p data-lang="en">ERGM lets the probability of each tie depend on these network patterns — not just individual traits, but the <em>structure</em> of the network itself.</p>
  <p data-lang="zh">ERGM 让每条关系的概率取决于这些网络模式——不仅是个人特征，还有网络<em>本身的结构</em>。</p>
</div>

<!-- ═══════════════════════════════════════════════════
     LEVEL 3 — THE CORE FORMULA
     ═══════════════════════════════════════════════════ -->

<hr class="section-divider">
<h2 id="ergm-formula">
  <span data-lang="en">The Core Formula — As a Scoring System</span>
  <span data-lang="zh">核心公式——当作评分系统来理解</span>
</h2>

<p data-lang="en">
Before we look at the formula, let's build the intuition. Think of ERGM as a <strong>scoring system</strong> for networks:
</p>

<p data-lang="zh">
看公式之前，先建立直觉。把 ERGM 想象成一个<strong>给网络打分的系统</strong>：
</p>

<h4 data-lang="en">Structural Patterns</h4>
    <h4 data-lang="zh">结构模式</h4>
    <p data-lang="en">Each pattern you care about (reciprocity, triangles, same-department ties) gets <strong>counted</strong>. These counts are called g — just "how many of this pattern exist."</p>
    <p data-lang="zh">你关心的每种模式（互惠、三角形、同部门关系）都被<strong>数出来</strong>。这些计数叫 g——就是"这种模式有多少个"。</p>
    <h4 data-lang="en">Weights (Importance)</h4>
    <h4 data-lang="zh">权重（重要性）</h4>
    <p data-lang="en">Each pattern gets a <strong>weight</strong> θ. Positive θ = "more of this pattern → higher score." Negative θ = "more of this → lower score." ERGM's job is to find these weights.</p>
    <p data-lang="zh">每种模式有一个<strong>权重</strong> θ。正 θ = "这种模式越多，分越高"。负 θ = "越多分越低"。ERGM 的任务就是找到这些权重。</p>

<p data-lang="en">
The total score = θ₁ × g₁ + θ₂ × g₂ + ... (weight × count for each pattern, then add up). A network with a <strong>higher score is more likely</strong> to be the one you actually observed.
</p>

<p data-lang="zh">
总分 = θ₁ × g₁ + θ₂ × g₂ + ...（每种模式的"权重 × 计数"加在一起）。<strong>总分越高的网络，越有可能</strong>就是你观察到的那个。
</p>

<p data-lang="en">
Now here's the formal version of that idea:
</p>

<p data-lang="zh">
下面是这个想法的数学表达：
</p>

<div style="text-align:center;font-size:16px;line-height:1.8;color:var(--ink-soft);font-family:var(--mono);letter-spacing:.02em;margin:24px 0;padding:20px;background:rgba(30,24,15,.03);border:1px solid var(--parchment);border-radius:6px;overflow-x:auto;">
P(Y = y) = exp(θ₁ × g₁(y) + θ₂ × g₂(y) + ... ) / κ
</div>


<h2 id="estimation">
  <span data-lang="en">How ERGM Finds the Parameters</span>
  <span data-lang="zh">ERGM 怎么找到参数</span>
</h2>

<p data-lang="en">
Remember κ — the normalizing constant we said you never compute? Here's why: it requires summing over <strong>every possible network</strong>. For just 10 people, there are 2<sup>45</sup> ≈ 35 trillion possible networks. Impossible to enumerate.
</p>

<p data-lang="zh">
还记得 κ——我们说永远不用算的那个归一化常数吗？原因是：它需要对<strong>所有可能的网络</strong>求和。光是 10 个人就有 2<sup>45</sup> ≈ 35 万亿种可能。根本没法穷举。
</p>

<p data-lang="en">
<strong>Solution: MCMC (Markov Chain Monte Carlo)</strong> — don't enumerate, <em>simulate</em>.
</p>

<p data-lang="zh">
<strong>解决办法：MCMC（马尔可夫链蒙特卡罗）</strong>——不穷举，<em>模拟</em>。
</p>

<div class="method-analogy">
  <div data-lang="en">Analogy: Reverse-engineering a recipe</div>
  <div data-lang="zh">类比：反推食谱</div>
  <p data-lang="en">You ate a delicious dish and want to find the recipe (θ). You don't try every possible combination of ingredients. Instead: make a guess → cook it → taste it → "too salty" → reduce salt → cook again → "not sweet enough" → add sugar → repeat until it tastes right. <strong>MCMC is this "taste and adjust" loop for networks.</strong></p>
  <p data-lang="zh">你吃了一道好菜，想找出食谱（θ）。你不需要试遍所有配料组合。而是：先猜 → 做出来 → 尝一口 → "太咸了" → 减盐 → 再做 → "不够甜" → 加糖 → 反复调整，直到味道对了。<strong>MCMC 就是网络版的"品尝和调整"循环。</strong></p>
</div>

<ol data-lang="en">
  <li><strong>Start:</strong> take the observed network and an initial guess for θ.</li>
  <li><strong>Simulate:</strong> randomly add or remove one tie at a time, creating simulated networks based on current θ.</li>
  <li><strong>Compare:</strong> do the simulated networks' statistics match the real one? Too few triangles → increase θ<sub>triangle</sub>. Too many → decrease it.</li>
  <li><strong>Repeat</strong> until simulated networks look like the real one. Then θ is your answer.</li>
</ol>
<ol data-lang="zh">
  <li><strong>起点：</strong>拿到观察到的网络，猜一组初始 θ。</li>
  <li><strong>模拟：</strong>根据当前 θ，每次随机加上或去掉一条关系，生成模拟网络。</li>
  <li><strong>对比：</strong>模拟网络的统计量和真实网络一致吗？三角形太少 → 增大 θ<sub>三角形</sub>。太多 → 减小。</li>
  <li><strong>重复，</strong>直到模拟网络看起来像真实网络。这时候的 θ 就是答案。</li>
</ol>

<div class="method-when">
  <div data-lang="en">In one sentence</div>
  <div data-lang="zh">一句话</div>
  <p data-lang="en">ERGM repeatedly asks: <em>"if these were the true rules, would we see something like the real network?"</em> It tweaks the rules until the answer is yes.</p>
  <p data-lang="zh">ERGM 反复问：<em>"如果这些是真实规则，我们能看到类似真实网络的东西吗？"</em>然后不断调整直到答案是"能"。</p>
</div>

<!-- ═══════════════════════════════════════════════════
     LEVEL 5 — COMMON PITFALLS
     ═══════════════════════════════════════════════════ -->

<hr class="section-divider">
<h2 id="pitfalls">
  <span data-lang="en">Common Pitfalls</span>
  <span data-lang="zh">常见陷阱</span>
</h2>

<h4 data-lang="en">Degeneracy</h4>
    <h4 data-lang="zh">退化问题</h4>
    <p data-lang="en">If you use raw "triangle count," the model can become unstable — it only considers nearly-empty or nearly-full networks. <strong>Why?</strong> Each new triangle makes more triangles easier (positive feedback loop).</p>
    <p data-lang="zh">如果你用原始的"三角形计数"项，模型会变得不稳定——只考虑几乎空或几乎满的网络。<strong>为什么？</strong>每多一个三角形都让更多三角形更容易形成（正反馈）。</p>
    <p data-lang="en" style="margin-top:8px"><strong>Fix:</strong> Use <strong>geometrically weighted terms</strong> (GWESP, GWDegree). They add diminishing returns — the 1st shared friend matters a lot, the 2nd less, the 3rd even less.</p>
    <p data-lang="zh" style="margin-top:8px"><strong>解决：</strong>使用<strong>几何加权项</strong>（GWESP、GWDegree）。引入递减效应——第一个共同朋友很重要，第二个没那么重要。</p>
    <h4 data-lang="en">Convergence Issues</h4>
    <h4 data-lang="zh">收敛问题</h4>
    <p data-lang="en">Since ERGM uses simulation (MCMC), it can fail to converge — the computer hasn't found stable parameter values. <strong>Signs:</strong> estimates keep changing, or simulated networks look nothing like the real one.</p>
    <p data-lang="zh">因为 ERGM 用模拟方法（MCMC），可能不收敛——计算机没找到稳定的参数值。<strong>迹象：</strong>估计值一直在变，或模拟网络和真实网络完全不像。</p>
    <p data-lang="en" style="margin-top:8px"><strong>Fixes:</strong> Increase simulations (burn-in, sample size). Use geometrically weighted terms. Start small, add terms one at a time.</p>
    <p data-lang="zh" style="margin-top:8px"><strong>修复：</strong>增加模拟次数。使用几何加权项。从小模型开始，逐个添加项。</p>

<!-- ═══════════════════════════════════════════════════
     LEVEL 6 — READING OUTPUT
     ═══════════════════════════════════════════════════ -->

<h2 id="reading-output">
  <span data-lang="en">How to Read ERGM Output</span>
  <span data-lang="zh">怎么读 ERGM 输出</span>
</h2>

<p data-lang="en">
ERGM output looks like logistic regression output. For each term you get an estimate, standard error, and p-value:
</p>

<p data-lang="zh">
ERGM 输出看起来像逻辑回归输出。每个项都有估计值、标准误和 p 值：
</p>


<h2 id="r-code">
  <span data-lang="en">R Code Example</span>
  <span data-lang="zh">R 代码示例</span>
</h2>

<pre style="background:var(--ink);color:#e8e0d4;padding:24px 28px;border-radius:6px;overflow-x:auto;font-size:13px;line-height:1.55;font-family:var(--mono);margin:20px 0;box-shadow:0 2px 8px rgba(30,24,15,.15)"><span class="comment"># Load package</span>
library(ergm)

<span class="comment"># Use built-in dataset</span>
data(florentine)

<span class="comment"># Fit a simple model:
#   edges       → controls overall density (like an intercept)
#   mutual      → do ties tend to be reciprocal?
#   gwesp(0.5)  → do friends-of-friends connect? (geometrically weighted)</span>
model &lt;- ergm(flo_net ~ edges + mutual + gwesp(0.5, fixed = TRUE),
              control = control.ergm(seed = 123))

<span class="comment"># View results — read just like logistic regression</span>
summary(model)

<span class="comment"># Check model fit: simulate networks and compare to real one</span>
gof_result &lt;- gof(model)
plot(gof_result)
<span class="comment"># Good fit = observed (red line) within simulated range (boxplots)</span></pre>

<!-- ═══════════════════════════════════════════════════
     LEVEL 8 — DEEP DIVE (collapsible)
     ═══════════════════════════════════════════════════ -->

<hr class="section-divider">
<h2 id="deep-dive">
  <span data-lang="en">Deep Dive: Full Mathematical Details</span>
  <span data-lang="zh">深入：完整数学细节</span>
</h2>

<div class="method-when">
  <div data-lang="en">Prerequisite</div>
  <div data-lang="zh">前置知识</div>
  <p data-lang="en">This section is for readers comfortable with probability theory and statistical modeling. If you understood the sections above, you already have enough to read ERGM papers.</p>
  <p data-lang="zh">本节面向熟悉概率论和统计建模的读者。如果你看懂了上面的内容，已经足够读 ERGM 论文了。</p>
</div>

<summary onclick="this.classList.toggle('open');this.nextElementSibling.classList.toggle('open')">
  <span class="arrow">▶</span>
  <span data-lang="en">Show full mathematical derivation</span>
  <span data-lang="zh">展开完整数学推导</span>
</summary>>

<h3 id="formal-model">
  <span data-lang="en">Formal Probability Model</span>
  <span data-lang="zh">形式概率模型</span>
</h3>

<div style="text-align:center;font-size:16px;line-height:1.8;color:var(--ink-soft);font-family:var(--mono);letter-spacing:.02em;margin:24px 0;padding:20px;background:rgba(30,24,15,.03);border:1px solid var(--parchment);border-radius:6px;overflow-x:auto;">
P(Y = y | θ) = exp(θ<sup>T</sup> g(y)) / κ(θ)
</div>

<p data-lang="en">
where <strong>g(y)</strong> is a vector of sufficient statistics, <strong>θ</strong> is the parameter vector, and <strong>κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*))</strong> sums over all 2<sup>n(n−1)/2</sup> possible networks (undirected) or 2<sup>n(n−1)</sup> (directed). This is an exponential family distribution.
</p>

<p data-lang="zh">
其中 <strong>g(y)</strong> 是充分统计量向量，<strong>θ</strong> 是参数向量，<strong>κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*))</strong> 对所有 2<sup>n(n−1)/2</sup>（无向）或 2<sup>n(n−1)</sup>（有向）个可能网络求和。这是指数族分布。
</p>

<h3 id="change-statistic">
  <span data-lang="en">Change Statistic and Conditional Log-Odds</span>
  <span data-lang="zh">变化统计量与条件对数几率</span>
</h3>

<p data-lang="en">
Conditioning on all other ties (Y<sub>−ij</sub>), the probability of tie (i, j) existing is:
</p>

<p data-lang="zh">
在给定所有其他关系（Y<sub>−ij</sub>）的条件下，关系 (i, j) 存在的概率为：
</p>

<div style="text-align:center;font-size:16px;line-height:1.8;color:var(--ink-soft);font-family:var(--mono);letter-spacing:.02em;margin:24px 0;padding:20px;background:rgba(30,24,15,.03);border:1px solid var(--parchment);border-radius:6px;overflow-x:auto;">
logit P(Y<sub>ij</sub> = 1 | Y<sub>−ij</sub>) = θ<sup>T</sup> δg<sub>ij</sub>
</div>

<p data-lang="en">
where δg<sub>ij</sub> = g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>) is the change in network statistics when edge (i, j) is toggled. The normalizing constant κ cancels out.
</p>

<p data-lang="zh">
其中 δg<sub>ij</sub> = g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>) 是翻转边 (i, j) 时网络统计量的变化。归一化常数 κ 被约掉了。
</p>

<h3 id="mcmc-mle">
  <span data-lang="en">MCMC-MLE Algorithm</span>
  <span data-lang="zh">MCMC-MLE 算法</span>
</h3>

<ol data-lang="en">
  <li><strong>Initialize</strong> with observed y<sub>obs</sub> and initial θ<sup>(0)</sup>.</li>
  <li><strong>Simulate</strong> via Metropolis-Hastings: propose y' by toggling one edge; accept with probability α = min(1, exp(θ<sup>T</sup>[g(y') − g(y<sup>(t)</sup>)])).</li>
  <li><strong>After burn-in</strong>, collect S thinned samples. Compute ḡ<sub>sim</sub> = (1/S) Σ g(y<sup>(t)</sup>).</li>
  <li><strong>Update:</strong> θ<sup>(t+1)</sup> = θ<sup>(t)</sup> + step × [g(y<sub>obs</sub>) − ḡ<sub>sim</sub>]. Stochastic gradient ascent on log-likelihood.</li>
  <li><strong>Converge</strong> when g(y<sub>obs</sub>) ≈ ḡ<sub>sim</sub>.</li>
</ol>
<ol data-lang="zh">
  <li><strong>初始化：</strong>观察网络 y<sub>obs</sub> 和初始 θ<sup>(0)</sup>。</li>
  <li><strong>模拟：</strong>Metropolis-Hastings——翻转一条边提议 y'，以概率 α = min(1, exp(θ<sup>T</sup>[g(y') − g(y<sup>(t)</sup>)])) 接受。</li>
  <li><strong>灼烧期后</strong>，收集 S 个稀疏样本。计算 ḡ<sub>sim</sub> = (1/S) Σ g(y<sup>(t)</sup>)。</li>
  <li><strong>更新：</strong>θ<sup>(t+1)</sup> = θ<sup>(t)</sup> + step × [g(y<sub>obs</sub>) − ḡ<sub>sim</sub>]。对数似然的随机梯度上升。</li>
  <li><strong>收敛：</strong>当 g(y<sub>obs</sub>) ≈ ḡ<sub>sim</sub> 时停止。</li>
</ol>

<h3 id="degeneracy-math">
  <span data-lang="en">Degeneracy: Mathematical Explanation</span>
  <span data-lang="zh">退化：数学解释</span>
</h3>

<p data-lang="en">
For a model with edges and triangles: P(Y = y) ∝ exp(θ<sub>E</sub>|E| + θ<sub>T</sub>|T|). When θ<sub>T</sub> > 0, the change statistic for edge (i,j) includes +2L<sub>ij</sub> (twice the number of common neighbors). In dense regions, L<sub>ij</sub> is large → adding edges is self-reinforcing → the distribution becomes bimodal. <strong>Geometrically weighted terms</strong> (GWESP) fix this via exponential decay: each additional shared partner contributes λ times less (0 < λ < 1).
</p>

<p data-lang="zh">
对于包含边和三角形的模型：P(Y = y) ∝ exp(θ<sub>E</sub>|E| + θ<sub>T</sub>|T|)。当 θ<sub>T</sub> > 0 时，变化统计量包含 +2L<sub>ij</sub>（公共邻居数的两倍）。在密集区域，L<sub>ij</sub> 很大 → 加边自我强化 → 分布变成双峰的。<strong>几何加权项</strong>（GWESP）通过指数衰减修复：每增加一个共享伙伴贡献乘以 λ（0 < λ < 1）。
</p>

<h3 id="convergence-diagnostics">
  <span data-lang="en">Convergence Diagnostics Checklist</span>
  <span data-lang="zh">收敛诊断清单</span>
</h3>


