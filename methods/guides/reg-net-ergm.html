---
layout: methods-guide
title: "ERGM: Mathematical Foundations"
title_zh: "ERGM：数学基础"
parent_title: "Network Analysis"
parent_title_zh: "网络分析"
parent_url: "reg-network.html"
bilingual: true
mathjax: true
---

<h2 id="ergm-motivation">
  <span data-lang="en">From Bernoulli Graphs to ERGM</span>
  <span data-lang="zh">从伯努利图到ERGM</span>
</h2>

<p data-lang="en">
Network models must explain why real networks look the way they do. The simplest approach is the <strong>random graph</strong>: each edge exists independently with probability <em>p</em>. This Bernoulli model is intuitive but fundamentally fails to capture realistic network properties.
</p>

<p data-lang="zh">
网络模型必须解释现实网络为什么具有特定的外观。最简单的方法是<strong>随机图</strong>：每条边独立以概率 <em>p</em> 存在。伯努利模型很直观，但根本无法捕捉现实网络的特性。
</p>

<div class="math-note">
P(Y = y) = p<sup>m</sup>(1 − p)<sup>n(n−1)/2 − m</sup>
</div>

<p data-lang="en">
where <em>y</em> is the realized network, <em>m</em> is the number of edges, and <em>n</em> is the number of nodes. This says: the probability of observing network <em>y</em> depends only on how many edges it has.
</p>

<p data-lang="zh">
其中 <em>y</em> 是实现的网络，<em>m</em> 是边数，<em>n</em> 是节点数。这表示：观察网络 <em>y</em> 的概率仅取决于它有多少条边。
</p>

<h3 id="bernoulli-problems">
  <span data-lang="en">Why Bernoulli Graphs Fail</span>
  <span data-lang="zh">伯努利图为什么失败</span>
</h3>

<p data-lang="en">
Real networks exhibit structure that Bernoulli graphs cannot produce:
</p>

<p data-lang="zh">
现实网络表现出伯努利图无法产生的结构：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Clustering (transitivity):</strong> In social networks, if A is friends with B and B is friends with C, then A and C are likely to be friends. Bernoulli graphs produce no clustering: triangles occur by pure chance. In contrast, real networks have clustering coefficients 0.3−0.5 compared to 0.001 for random graphs.</li>
  <li data-lang="zh"><strong>聚类（传递性）：</strong>在社交网络中，如果A是B的朋友，B是C的朋友，那么A和C也很可能是朋友。伯努利图不产生聚类：三角形纯粹是巧合。相比之下，现实网络的聚类系数为0.3−0.5，而随机图仅为0.001。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Reciprocity in directed networks:</strong> If A sends a message to B, B is likely to respond. Bernoulli graphs have no reciprocity: mutual edges occur with probability p². Real networks exhibit reciprocity rates 0.3−0.9.</li>
  <li data-lang="zh"><strong>有向网络中的互惠性：</strong>如果A向B发送消息，B很可能会回应。伯努利图没有互惠性：相互边以概率 p² 出现。现实网络的互惠率为0.3−0.9。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Degree heterogeneity:</strong> Real networks have hubs: a few nodes with very high degree and many nodes with low degree. Bernoulli graphs produce approximately Poisson degree distributions. Observed networks have power-law or heavy-tailed degree distributions.</li>
  <li data-lang="zh"><strong>度数异质性：</strong>现实网络有中心：少数节点度数很高，许多节点度数很低。伯努利图产生近似泊松度分布。观察到的网络具有幂律或重尾度分布。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Homophily (assortative mixing):</strong> People tend to connect with others like themselves (same race, age, education). Bernoulli graphs assign edges at random: no correlation between nodal attributes and tie formation.</li>
  <li data-lang="zh"><strong>同群偏好（同质混合）：</strong>人们倾向于与相似的人联系（相同种族、年龄、教育）。伯努利图随机分配边：节点属性与关系形成之间没有相关性。</li>
</ul>

<p data-lang="en">
<strong>The key insight:</strong> We need a probabilistic model where the probability of an edge depends not just on a global parameter but on <em>local network statistics</em>—counts of triangles, mutual pairs, degree distributions, and attribute matches. This is exactly what ERGM (Exponential Random Graph Model) provides.
</p>

<p data-lang="zh">
<strong>关键洞察：</strong>我们需要一个概率模型，其中边的概率不仅取决于全局参数，还取决于<em>局部网络统计量</em>——三角形、相互对、度分布和属性匹配的计数。这正是ERGM（指数随机图模型）提供的。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<h2 id="ergm-probability-model">
  <span data-lang="en">The ERGM Probability Model</span>
  <span data-lang="zh">ERGM概率模型</span>
</h2>

<p data-lang="en">
An Exponential Random Graph Model (ERGM) specifies the probability of observing a network <em>y</em> as:
</p>

<p data-lang="zh">
指数随机图模型（ERGM）将观察网络 <em>y</em> 的概率指定为：
</p>

<div class="math-note">
P(Y = y | θ) = exp(θ<sup>T</sup> g(y)) / κ(θ)
</div>

<p data-lang="en">
where:
</p>

<p data-lang="zh">
其中：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>g(y)</strong> is a <em>p</em>-dimensional vector of <strong>network statistics</strong> (sufficient statistics) computed from network <em>y</em>. Examples: number of edges, number of triangles, number of mutual dyads, homophily on each attribute.</li>
  <li data-lang="zh"><strong>g(y)</strong> 是从网络 <em>y</em> 计算得出的 <em>p</em> 维<strong>网络统计量</strong>（充分统计量）向量。示例：边数、三角形数、相互二元组数、每个属性上的同群偏好。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>θ</strong> is the parameter vector of length <em>p</em>. Each θ<sub>k</sub> represents the "weight" of statistic g<sub>k</sub>(y): positive values make networks with more of that statistic more likely; negative values make them less likely.</li>
  <li data-lang="zh"><strong>θ</strong> 是长度为 <em>p</em> 的参数向量。每个θ<sub>k</sub> 代表统计量 g<sub>k</sub>(y) 的"权重"：正值使具有更多该统计量的网络更可能；负值使其不太可能。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>κ(θ)</strong> is the <strong>normalizing constant</strong>: κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*)) where the sum is over all possible networks on <em>n</em> nodes. This ensures the probabilities sum to 1.</li>
  <li data-lang="zh"><strong>κ(θ)</strong> 是<strong>正规化常数</strong>：κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*))，其中求和遍历 <em>n</em> 个节点上的所有可能网络。这确保概率之和为 1。</li>
</ul>

<h3 id="sufficient-statistics">
  <span data-lang="en">Network Statistics (Sufficient Statistics)</span>
  <span data-lang="zh">网络统计量（充分统计量）</span>
</h3>

<p data-lang="en">
The vector g(y) contains network features that capture structural properties. Common choices:
</p>

<p data-lang="zh">
向量 g(y) 包含捕捉结构特性的网络特征。常见选择：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Edges:</strong> g<sub>edges</sub>(y) = number of edges in y. Negative θ<sub>edges</sub> makes networks sparse; positive makes them dense.</li>
  <li data-lang="zh"><strong>边：</strong>g<sub>edges</sub>(y) = y 中的边数。负θ<sub>edges</sub> 使网络稀疏；正θ使其密集。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Mutual dyads (directed networks):</strong> g<sub>mutual</sub>(y) = count of pairs (i,j) where Y<sub>ij</sub> = 1 AND Y<sub>ji</sub> = 1. Positive θ<sub>mutual</sub> encourages reciprocated relationships.</li>
  <li data-lang="zh"><strong>相互二元组（有向网络）：</strong>g<sub>mutual</sub>(y) = 对 (i,j) 的计数，其中 Y<sub>ij</sub> = 1 且 Y<sub>ji</sub> = 1。正θ<sub>mutual</sub> 鼓励相互关系。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Triangles:</strong> g<sub>triangle</sub>(y) = count of triangles (three nodes all connected). Positive θ<sub>triangle</sub> encourages clustering.</li>
  <li data-lang="zh"><strong>三角形：</strong>g<sub>triangle</sub>(y) = 三角形的计数（三个节点都连接）。正θ<sub>triangle</sub> 鼓励聚类。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Degree distribution:</strong> g<sub>degree-k</sub>(y) = count of nodes with exactly degree k. Allows flexible degree heterogeneity.</li>
  <li data-lang="zh"><strong>度分布：</strong>g<sub>degree-k</sub>(y) = 度数恰好为 k 的节点计数。允许灵活的度异质性。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Homophily (attribute matching):</strong> g<sub>homophily-X</sub>(y) = count of edges between nodes with the same value of attribute X. Positive θ encourages ties within demographic groups.</li>
  <li data-lang="zh"><strong>同群偏好（属性匹配）：</strong>g<sub>homophily-X</sub>(y) = 具有属性X相同值的节点之间的边数。正θ鼓励群体内的关系。</li>
</ul>

<h3 id="log-odds-derivation">
  <span data-lang="en">The Log-Odds (Change Statistic) Interpretation</span>
  <span data-lang="zh">对数几率（变化统计量）解释</span>
</h3>

<p data-lang="en">
A key feature of ERGMs is the <strong>change statistic</strong>: the probability that edge (i, j) exists, conditional on all other edges, depends only on how that edge changes the network statistics. This yields a logistic form.
</p>

<p data-lang="zh">
ERGM的一个关键特性是<strong>变化统计量</strong>：边 (i, j) 存在的概率，在所有其他边给定的条件下，仅取决于该边如何改变网络统计量。这产生了逻辑形式。
</p>

<p data-lang="en">
Starting from the ERGM definition:
</p>

<p data-lang="zh">
从ERGM定义开始：
</p>

<div class="math-note">
P(Y<sub>ij</sub> = 1 | Y<sub>−ij</sub> = y<sub>−ij</sub>) / P(Y<sub>ij</sub> = 0 | Y<sub>−ij</sub> = y<sub>−ij</sub>) = exp(θ<sup>T</sup> [g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>)])
</div>

<p data-lang="en">
Here, y<sub>+ij</sub> denotes the network with edge (i, j) added, and y<sub>−ij</sub> denotes the network with it removed. The difference δg<sub>ij</sub> = g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>) is the <strong>change statistic</strong>: how much each network statistic changes if we flip edge (i, j).
</p>

<p data-lang="zh">
此处，y<sub>+ij</sub> 表示添加了边 (i, j) 的网络，y<sub>−ij</sub> 表示删除了该边的网络。差异δg<sub>ij</sub> = g(y<sub>+ij</sub>) − g(y<sub>−ij</sub>) 是<strong>变化统计量</strong>：如果我们翻转边 (i, j)，每个网络统计量改变多少。
</p>

<p data-lang="en">
This simplifies to a logistic regression form:
</p>

<p data-lang="zh">
这简化为逻辑回归形式：
</p>

<div class="math-note">
log(odds) = θ<sup>T</sup> δg<sub>ij</sub>
</div>

<div class="math-note">
P(Y<sub>ij</sub> = 1 | rest) = exp(θ<sup>T</sup> δg<sub>ij</sub>) / (1 + exp(θ<sup>T</sup> δg<sub>ij</sub>)) = 1 / (1 + exp(−θ<sup>T</sup> δg<sub>ij</sub>))
</div>

<p data-lang="en">
<strong>Interpretation:</strong> The log-odds of edge (i, j) existing is a linear combination of change statistics. If adding that edge increases triangles and homophily, the log-odds term from those statistics is positive, making the edge more likely. If it increases isolated dyads (frowned upon in many models), the log-odds term is negative, making the edge less likely.
</p>

<p data-lang="zh">
<strong>解释：</strong>边 (i, j) 存在的对数几率是变化统计量的线性组合。如果添加该边增加了三角形和同群偏好，这些统计量的对数几率项为正，使边更可能。如果它增加了孤立二元组（许多模型中不受欢迎），对数几率项为负，使边不太可能。
</p>

<p data-lang="en">
This is <em>why ERGM is like logistic regression but with dependence</em>: in standard logistic regression, edge probabilities are independent. In ERGM, each edge probability depends on all others through the network statistics.
</p>

<p data-lang="zh">
这就是<em>为什么ERGM类似于逻辑回归但具有依赖性</em>的原因：在标准逻辑回归中，边概率是独立的。在ERGM中，每条边的概率通过网络统计量取决于所有其他边。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<h2 id="normalizing-constant">
  <span data-lang="en">The Intractable Normalizing Constant</span>
  <span data-lang="zh">难以处理的正规化常数</span>
</h2>

<p data-lang="en">
The ERGM probability is:
</p>

<p data-lang="zh">
ERGM概率是：
</p>

<div class="math-note">
P(Y = y | θ) = exp(θ<sup>T</sup> g(y)) / κ(θ)
</div>

<p data-lang="en">
where the normalizing constant is:
</p>

<p data-lang="zh">
其中正规化常数是：
</p>

<div class="math-note">
κ(θ) = Σ<sub>y*</sub> exp(θ<sup>T</sup> g(y*))
</div>

<p data-lang="en">
This is a sum over <strong>all possible networks</strong>. For an undirected network with <em>n</em> nodes, there are 2<sup>n(n−1)/2</sup> possible networks. For a small network with n=10, this is 2<sup>45</sup> ≈ 3.5 × 10<sup>13</sup> networks. For n=100, the number is astronomically large.
</p>

<p data-lang="zh">
这是对<strong>所有可能网络</strong>的求和。对于有 <em>n</em> 个节点的无向网络，有 2<sup>n(n−1)/2</sup> 个可能的网络。对于 n=10 的小网络，这是 2<sup>45</sup> ≈ 3.5 × 10<sup>13</sup> 个网络。对于 n=100，数字是天文数字。
</p>

<p data-lang="en">
<strong>We cannot enumerate all networks.</strong> Therefore, we cannot compute κ(θ) exactly. This is the central computational challenge of ERGM inference. We cannot use standard maximum likelihood estimation (which requires evaluating the likelihood P(Y = y<sub>obs</sub> | θ)).
</p>

<p data-lang="zh">
<strong>我们无法列举所有网络。</strong>因此，我们无法精确计算κ(θ)。这是ERGM推断的中心计算挑战。我们不能使用标准最大似然估计（这需要评估似然函数 P(Y = y<sub>obs</sub> | θ)）。
</p>

<p data-lang="en">
<strong>Solution:</strong> Use Markov Chain Monte Carlo (MCMC). Instead of computing κ(θ) directly, we simulate networks from the ERGM distribution and learn from the simulations. If we can sample networks from P(Y | θ), we can estimate the model parameters by comparing observed network statistics to simulated statistics.
</p>

<p data-lang="zh">
<strong>解决方案：</strong>使用马尔可夫链蒙特卡罗（MCMC）。与其直接计算κ(θ)，我们从ERGM分布模拟网络并从模拟中学习。如果我们能从 P(Y | θ) 采样网络，我们可以通过将观察到的网络统计量与模拟统计量进行比较来估计模型参数。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<h2 id="mcmc-estimation">
  <span data-lang="en">MCMC Estimation</span>
  <span data-lang="zh">MCMC估计</span>
</h2>

<p data-lang="en">
MCMC-MLE (Maximum Likelihood Estimation via MCMC) works as follows:
</p>

<p data-lang="zh">
MCMC-MLE（通过MCMC的最大似然估计）工作如下：
</p>

<h3 id="mcmc-steps">
  <span data-lang="en">The MCMC Algorithm for ERGM</span>
  <span data-lang="zh">ERGM的MCMC算法</span>
</h3>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Initialize:</strong> Start with the observed network y<sub>obs</sub> (or a random network). Set initial parameter guess θ<sup>(0)</sup>.</li>
  <li data-lang="zh"><strong>初始化：</strong>从观察到的网络 y<sub>obs</sub> 开始（或随机网络）。设置初始参数猜测θ<sup>(0)</sup>。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>At iteration t:</strong>
    <ul style="margin-left: 2rem;">
      <li>Take current state y<sup>(t)</sup> and current parameters θ<sup>(t)</sup>.</li>
      <li>Propose a new network y' by randomly toggling one edge in y<sup>(t)</sup> (Metropolis-Hastings step).</li>
      <li>Compute acceptance probability: α = min(1, P(y' | θ<sup>(t)</sup>) / P(y<sup>(t)</sup> | θ<sup>(t)</sup>)) = min(1, exp(θ<sup>(t)T</sup> [g(y') − g(y<sup>(t)</sup>)]))</li>
      <li>With probability α, accept y' and set y<sup>(t+1)</sup> = y'. Otherwise, keep y<sup>(t+1)</sup> = y<sup>(t)</sup>.</li>
    </ul>
  </li>
  <li data-lang="zh"><strong>在第t次迭代：</strong>
    <ul style="margin-left: 2rem;">
      <li>取当前状态 y<sup>(t)</sup> 和当前参数θ<sup>(t)</sup>。</li>
      <li>通过随机切换 y<sup>(t)</sup> 中的一条边来提议新网络 y'（Metropolis-Hastings步骤）。</li>
      <li>计算接受概率：α = min(1, P(y' | θ<sup>(t)</sup>) / P(y<sup>(t)</sup> | θ<sup>(t)</sup>)) = min(1, exp(θ<sup>(t)T</sup> [g(y') − g(y<sup>(t)</sup>)]))</li>
      <li>以概率α接受 y'，设置 y<sup>(t+1)</sup> = y'。否则，保持 y<sup>(t+1)</sup> = y<sup>(t)</sup>。</li>
    </ul>
  </li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Burn-in:</strong> Run the chain for B iterations without updating θ (e.g., B = 10,000). This allows the chain to "forget" its initialization and converge to the true distribution under θ<sup>(0)</sup>.</li>
  <li data-lang="zh"><strong>灼烧期：</strong>运行链B次迭代而不更新θ（例如 B = 10,000）。这允许链"遗忘"其初始化并收敛到θ<sup>(0)</sup>下的真实分布。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Sample:</strong> After burn-in, collect S samples y<sup>(B)</sup>, y<sup>(B+lag)</sup>, y<sup>(B+2×lag)</sup>, ..., y<sup>(B+S×lag)</sup> (thinning by lag to reduce autocorrelation; e.g., lag = 100). Compute statistics: ḡ<sub>sim</sub> = (1/S) Σ g(y<sup>(t)</sup>).</li>
  <li data-lang="zh"><strong>采样：</strong>灼烧后，收集S个样本 y<sup>(B)</sup>, y<sup>(B+lag)</sup>, y<sup>(B+2×lag)</sup>, ..., y<sup>(B+S×lag)</sup>（按滞后进行稀疏化以减少自相关；例如 lag = 100）。计算统计量：ḡ<sub>sim</sub> = (1/S) Σ g(y<sup>(t)</sup>)。</li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Update θ:</strong> Compare observed statistics g(y<sub>obs</sub>) to simulated statistics ḡ<sub>sim</sub>. If they match, θ is good. If they differ, adjust θ to make simulated networks more like the observed network.
    <div class="math-note">
    θ<sup>(t+1)</sup> = θ<sup>(t)</sup> + step_size × [g(y<sub>obs</sub>) − ḡ<sub>sim</sub>]
    </div>
    This is a stochastic gradient step: increase parameters whose statistics are too low in simulations, decrease those whose statistics are too high.
  </li>
  <li data-lang="zh"><strong>更新θ：</strong>比较观察到的统计量 g(y<sub>obs</sub>) 和模拟统计量ḡ<sub>sim</sub>。如果匹配，θ是好的。如果不同，调整θ使模拟网络更像观察到的网络。
    <div class="math-note">
    θ<sup>(t+1)</sup> = θ<sup>(t)</sup> + step_size × [g(y<sub>obs</sub>) − ḡ<sub>sim</sub>]
    </div>
    这是一个随机梯度步骤：增加在模拟中统计量太低的参数，减少统计量太高的参数。
  </li>
</ol>

<ol style="margin-left: 2rem;">
  <li data-lang="en"><strong>Repeat:</strong> Return to step 2 with updated θ<sup>(t+1)</sup>. Iterate until convergence: when g(y<sub>obs</sub>) ≈ ḡ<sub>sim</sub>.</li>
  <li data-lang="zh"><strong>重复：</strong>用更新的θ<sup>(t+1)</sup>回到步骤2。迭代直到收敛：当 g(y<sub>obs</sub>) ≈ ḡ<sub>sim</sub>。</li>
</ol>

<h3 id="convergence-diagnostics">
  <span data-lang="en">Convergence Diagnostics</span>
  <span data-lang="zh">收敛诊断</span>
</h3>

<p data-lang="en">
MCMC-MLE can fail silently if the chain doesn't converge. Key diagnostics:
</p>

<p data-lang="zh">
如果链不收敛，MCMC-MLE可能无声地失败。关键诊断：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Parameter trace plots:</strong> Plot θ<sup>(t)</sup> over iterations. Should look like white noise around a mean (stable). If it trends or drifts, the chain hasn't converged.</li>
  <li data-lang="zh"><strong>参数轨迹图：</strong>绘制各迭代的θ<sup>(t)</sup>。应该看起来像围绕平均值的白噪声（稳定）。如果趋势或漂移，链未收敛。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Simulated vs observed statistics:</strong> After fitting, plot g(y<sub>obs</sub>) alongside ḡ<sub>sim</sub>. For each statistic, the observed value should fall within the range of simulated values. Large discrepancies indicate model misfit.</li>
  <li data-lang="zh"><strong>模拟 vs 观察统计量：</strong>拟合后，将 g(y<sub>obs</sub>) 与ḡ<sub>sim</sub> 并排绘制。对于每个统计量，观察值应该在模拟值的范围内。大的差异表示模型不拟合。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Effective sample size:</strong> The MCMC samples are autocorrelated. Report ESS (effective sample size) = S / (1 + 2 Σ ρ<sub>k</sub>) where ρ<sub>k</sub> is the autocorrelation at lag k. ESS should be ≥ 100 per parameter.</li>
  <li data-lang="zh"><strong>有效样本量：</strong>MCMC样本是自相关的。报告ESS（有效样本量）= S / (1 + 2 Σ ρ<sub>k</sub>)，其中ρ<sub>k</sub> 是滞后 k 处的自相关。每个参数的ESS应该 ≥ 100。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Gelman-Rubin potential scale reduction (PSRF):</strong> Run multiple chains with different initializations. If chains converge to the same posterior, PSRF ≈ 1. Values > 1.05 suggest non-convergence.</li>
  <li data-lang="zh"><strong>Gelman-Rubin潜在尺度缩减（PSRF）：</strong>运行多条具有不同初始化的链。如果链收敛到相同的后验，PSRF ≈ 1。值 > 1.05 表示不收敛。</li>
</ul>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<h2 id="degeneracy-problem">
  <span data-lang="en">The Degeneracy Problem</span>
  <span data-lang="zh">退化问题</span>
</h2>

<p data-lang="en">
Some ERGM specifications lead to <strong>degenerate distributions</strong>: the model assigns nearly all probability mass to a few extreme networks (almost empty or almost complete) rather than a realistic distribution around the observed network. This makes MCMC exploration impossible and parameter estimation unreliable.
</p>

<p data-lang="zh">
某些ERGM规范导致<strong>退化分布</strong>：模型将几乎所有概率质量分配给几个极端网络（几乎为空或几乎完整），而不是观察网络周围的现实分布。这使得MCMC探索不可能且参数估计不可靠。
</p>

<h3 id="degeneracy-example">
  <span data-lang="en">Example: The Triangle Term Problem</span>
  <span data-lang="zh">示例：三角形项问题</span>
</h3>

<p data-lang="en">
Consider a simple model with edges and triangles:
</p>

<p data-lang="zh">
考虑一个包含边和三角形的简单模型：
</p>

<div class="math-note">
P(Y = y | θ<sub>E</sub>, θ<sub>T</sub>) = exp(θ<sub>E</sub> × |E| + θ<sub>T</sub> × |T|) / κ
</div>

<p data-lang="en">
where |E| is edge count and |T| is triangle count. If θ<sub>T</sub> > 0 (encouraging triangles), the model can reach a "tipping point": as the network adds edges, creating triangles becomes easier. Soon the model concentrates probability on either near-empty networks (where no triangles can form) or near-complete networks (where most possible triangles exist). The distribution becomes U-shaped: near-empty and near-complete networks are vastly more likely than observed sparse-but-clustered networks.
</p>

<p data-lang="zh">
其中 |E| 是边计数，|T| 是三角形计数。如果θ<sub>T</sub> > 0（鼓励三角形），模型可能达到"临界点"：当网络添加边时，创建三角形变得更容易。很快模型将概率集中在接近空网络（其中无法形成三角形）或接近完整网络（其中存在大多数可能的三角形）上。分布变成U形：近空和接近完整的网络比观察到的稀疏但聚集的网络更可能得多。
</p>

<p data-lang="en">
<strong>Why this happens mathematically:</strong> The change statistic for an edge (i, j) when you add it is: δg<sub>ij</sub> ≈ 1 (for edge) + 2L<sub>ij</sub> (triangles, where L<sub>ij</sub> is the count of common neighbors of i and j). In a dense network, L<sub>ij</sub> is large, making the log-odds huge and the edge almost certain. In a sparse network, L<sub>ij</sub> is small, making the log-odds negative and edges unlikely. This non-linear feedback destabilizes the model.
</p>

<p data-lang="zh">
<strong>数学上为什么会发生这种情况：</strong>当添加边 (i, j) 时的变化统计量大约为：δg<sub>ij</sub> ≈ 1（对于边）+ 2L<sub>ij</sub>（三角形，其中 L<sub>ij</sub> 是 i 和 j 的公共邻居计数）。在密集网络中，L<sub>ij</sub> 很大，使对数几率巨大，边几乎是确定的。在稀疏网络中，L<sub>ij</sub> 很小，使对数几率为负，边不太可能。这种非线性反馈使模型不稳定。
</p>

<h3 id="gwterms">
  <span data-lang="en">Solution: Geometrically Weighted Terms</span>
  <span data-lang="zh">解决方案：几何加权项</span>
</h3>

<p data-lang="en">
Instead of raw triangle counts, use <strong>geometrically weighted</strong> terms that grow sublinearly with local clustering. Common choices:
</p>

<p data-lang="zh">
与其使用原始三角形计数，使用<strong>几何加权</strong>项，该项随本地聚类而亚线性增长。常见选择：
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>GWESP (Geometrically Weighted Edgewise Shared Partner):</strong> Instead of counting all triangles, count "edgewise shared partners"—for each edge, how many common neighbors? Then weight them geometrically: the first shared partner contributes θ<sub>GWESP</sub>, the second contributes θ<sub>GWESP</sub> × λ, the third θ<sub>GWESP</sub> × λ², etc., where 0 < λ < 1. This attenuates the impact of many shared partners, preventing the tipping point.</li>
  <li data-lang="zh"><strong>GWESP（几何加权边尖端共享伙伴）：</strong>与其计算所有三角形，计算"边尖端共享伙伴"——对于每条边，有多少个公共邻居？然后进行几何加权：第一个共享伙伴贡献θ<sub>GWESP</sub>，第二个贡献θ<sub>GWESP</sub> × λ，第三个θ<sub>GWESP</sub> × λ²，等等，其中 0 < λ < 1。这减弱了许多共享伙伴的影响，防止临界点。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>GWDegree (Geometrically Weighted Degree):</strong> Instead of raw degree distribution terms g<sub>degree=k</sub>, use geometrically weighted degree: Σ deg(v) × λ^deg(v). This allows the model to control degree heterogeneity without promoting extreme hub-and-spoke structures.</li>
  <li data-lang="zh"><strong>GWDegree（几何加权度）：</strong>与其使用原始度分布项 g<sub>degree=k</sub>，使用几何加权度：Σ deg(v) × λ^deg(v)。这允许模型控制度异质性，而不会促进极端的轮辐结构。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Other GW terms:</strong> GWDSP (geometrically weighted dyadic shared partner), GWNDSP (non-directed version), etc. The key principle: weight higher-order effects with exponential decay (tuning parameter λ ∈ [0.3, 0.9]).</li>
  <li data-lang="zh"><strong>其他GW项：</strong>GWDSP（几何加权二元共享伙伴）、GWNDSP（无向版本）等。关键原则：用指数衰减对高阶效应进行加权（调整参数λ ∈ [0.3, 0.9]）。</li>
</ul>

<p data-lang="en">
<strong>Modern practice:</strong> Avoid raw triangle counts, degree distributions, and similar terms in ERGM specifications. Use geometrically weighted versions instead. This dramatically improves MCMC convergence and model stability.
</p>

<p data-lang="zh">
<strong>现代实践：</strong>在ERGM规范中避免原始三角形计数、度分布和类似的项。使用几何加权版本。这大大改善了MCMC收敛和模型稳定性。
</p>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<h2 id="r-implementation">
  <span data-lang="en">R Implementation with statnet</span>
  <span data-lang="zh">使用statnet的R实现</span>
</h2>

<p data-lang="en">
The <code>ergm</code> package (part of the <code>statnet</code> suite) is the standard for fitting ERGMs in R. Here's a complete working example.
</p>

<p data-lang="zh">
<code>ergm</code> 包（<code>statnet</code> 套件的一部分）是在 R 中拟合 ERGM 的标准。以下是一个完整的工作示例。
</p>

<pre style="background: var(--ink); color: var(--cream); padding: 1.5rem; border-radius: 0.5rem; overflow-x: auto; font-size: 0.85em; line-height: 1.4;">
# Install if needed: install.packages("ergm")
library(ergm)
library(tidyverse)

# Example: Florentine families business network
data(florentine)
flo_net &lt;- florentine

# Quick look at network
summary(flo_net)  # Number of nodes, edges, etc.

# Specify ERGM model
# edges: controls sparsity
# mutual: encourages reciprocity (directed networks)
# gwesp: geometrically weighted edgewise shared partners (clustering)
# nodecov: effect of node attribute (e.g., wealth)
# absdiff: homophily on continuous attributes

# Simple model with edges, reciprocity, and clustering
model_1 &lt;- ergm(flo_net ~ edges + mutual + gwesp(0.5, fixed = TRUE),
                 control = control.ergm(seed = 123,
                                        MCMC.samplesize = 5000,
                                        MCMC.interval = 50,
                                        MCMC.burnin = 5000))

# View results
summary(model_1)
# Output shows:
#   - Estimate: fitted parameter θ
#   - Std. Error: standard error from MCMC
#   - t-ratio: θ / SE
#   - p-value: two-sided test that θ = 0
# Interpretation: positive estimate means that statistic increases the network probability

# Check convergence diagnostics
# From summary output, look for:
#   - All p-values shown (good)
#   - t-ratios not extreme (|t| &lt; 5 is good)
#   - If summary shows "Warning: MCMC did not converge", try:
#       - Increase MCMC.samplesize (to 10000)
#       - Increase MCMC.burnin (to 10000)
#       - Increase MCMC.interval (to 100)

# Manual convergence check: trace plots
# (Note: mcmc.diagnostics is deprecated; use gof + graphical assessment instead)
print(model_1)

# Goodness-of-fit: Compare simulated and observed statistics
gof_model_1 &lt;- gof(model_1)
plot(gof_model_1)
# Should show observed statistics (red) within the range of simulated distributions (boxplots)

# Coefficient interpretation
coef(model_1)
# edges: negative means network is sparse (decreases probability of each edge)
# mutual: positive means directed ties tend to be reciprocated
# gwesp: positive means clustering is present (common neighbors increase tie probability)

# Simulate networks from fitted model
sim_nets &lt;- simulate(model_1, nsim = 100, seed = 456)
# Each simulated network should resemble observed network in key statistics

# Extract simulated network statistics
sim_stats &lt;- data.frame(
  edges = sapply(sim_nets, function(net) summary(net ~ edges)),
  mutual = sapply(sim_nets, function(net) summary(net ~ mutual)),
  gwesp = sapply(sim_nets, function(net) summary(net ~ gwesp(0.5, fixed = TRUE)))
)

# Compare to observed
obs_stats &lt;- c(
  edges = summary(flo_net ~ edges),
  mutual = summary(flo_net ~ mutual),
  gwesp = summary(flo_net ~ gwesp(0.5, fixed = TRUE))
)

print(obs_stats)
print(colMeans(sim_stats))
# Should be similar

# Model comparison with likelihood ratio test
# Fit a more complex model
model_2 &lt;- ergm(flo_net ~ edges + mutual + gwesp(0.5, fixed = TRUE)
                 + nodefactor("Faction"),
                 control = control.ergm(seed = 123))

# Likelihood ratio test (if both models fit successfully)
anova(model_1, model_2, test = "Chisq")
# Small p-value suggests model_2 is significantly better

# Extract and visualize
coef_df &lt;- data.frame(
  model = c("Model 1", "Model 1", "Model 1",
            "Model 2", "Model 2", "Model 2", "Model 2"),
  term = c("edges", "mutual", "gwesp",
           "edges", "mutual", "gwesp", "nodefactor"),
  estimate = c(coef(model_1), coef(model_2))
)

ggplot(coef_df, aes(x = term, y = estimate, fill = model)) +
  geom_col(position = "dodge") +
  geom_hline(yintercept = 0, color = "black", linetype = "dashed") +
  labs(title = "ERGM Coefficient Comparison",
       x = "Term", y = "Estimate",
       fill = "Model") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
</pre>

<h3 id="interpretation-guide">
  <span data-lang="en">Interpretation Guide</span>
  <span data-lang="zh">解释指南</span>
</h3>

<p data-lang="en">
<strong>Reading ERGM output:</strong>
</p>

<p data-lang="zh">
<strong>阅读ERGM输出：</strong>
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Positive coefficient:</strong> That network statistic increases the log-probability of the network. Example: θ<sub>mutual</sub> = 0.5 means adding a mutual (reciprocated) tie increases the network's probability—the network is more likely to have reciprocal relationships.</li>
  <li data-lang="zh"><strong>正系数：</strong>该网络统计量增加网络的对数概率。示例：θ<sub>mutual</sub> = 0.5 表示添加相互（互惠）关系增加网络的概率——网络更可能具有相互关系。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Negative coefficient:</strong> That statistic decreases the log-probability. Example: θ<sub>edges</sub> = −2 means more edges make the network less probable—the network is sparse.</li>
  <li data-lang="zh"><strong>负系数：</strong>该统计量降低对数概率。示例：θ<sub>edges</sub> = −2 表示更多边使网络不太可能——网络是稀疏的。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Magnitude matters:</strong> Larger absolute values mean stronger effects. θ = −10 is an extremely strong sparse preference; θ = −0.1 is weak.</li>
  <li data-lang="zh"><strong>大小很重要：</strong>较大的绝对值意味着更强的效果。θ = −10 是一个极强的稀疏偏好；θ = −0.1 是弱的。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Significance (p-value):</strong> If p < 0.05, the parameter is significantly different from 0. If p > 0.05, the effect is not statistically significant (could be noise).</li>
  <li data-lang="zh"><strong>显著性（p值）：</strong>如果 p < 0.05，参数与 0 显著不同。如果 p > 0.05，效果在统计上不显著（可能是噪声）。</li>
</ul>

<p data-lang="en">
<strong>Common pitfalls:</strong>
</p>

<p data-lang="zh">
<strong>常见陷阱：</strong>
</p>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>MCMC did not converge:</strong> Check trace plots (parameter estimates over iterations should be stable white noise). If they drift, try: increase burn-in, increase sample size, use geometrically weighted terms instead of raw terms, simplify the model (remove high-order terms).</li>
  <li data-lang="zh"><strong>MCMC未收敛：</strong>检查轨迹图（参数估计在迭代中应该是稳定的白噪声）。如果漂移，尝试：增加灼烧期、增加样本量、使用几何加权项而不是原始项、简化模型（删除高阶项）。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>MCMC explodes:</strong> The chain explores pathological networks with infinite statistics. This is the degeneracy problem (see above). Remove problematic terms (e.g., raw triangle counts) and use geometrically weighted alternatives.</li>
  <li data-lang="zh"><strong>MCMC爆炸：</strong>链探索具有无限统计量的病理网络。这是退化问题（见上文）。删除有问题的项（例如原始三角形计数）并使用几何加权替代品。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Model doesn't fit observed network:</strong> After fitting, simulate networks and compare statistics. If observed statistics lie outside the range of simulated statistics (in GOF plot), the model misses key features. Add terms capturing those features (e.g., if observed clustering is higher, add GWESP or other clustering terms).</li>
  <li data-lang="zh"><strong>模型不拟合观察网络：</strong>拟合后，模拟网络并比较统计量。如果观察统计量在模拟统计量范围之外（在GOF图中），模型缺少关键特征。添加捕捉这些特征的项（例如，如果观察到的聚类更高，添加GWESP或其他聚类项）。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Too many parameters:</strong> With many predictors, standard errors inflate and MCMC becomes inefficient. Start with a minimal model (edges + one or two structural terms + attributes). Incrementally add terms based on goodness-of-fit diagnostics.</li>
  <li data-lang="zh"><strong>参数过多：</strong>有许多预测因子，标准误差增加，MCMC变得低效。从最小模型开始（边 + 一个或两个结构项 + 属性）。基于拟合优度诊断增量添加项。</li>
</ul>

<hr style="margin: 2rem 0; border: none; border-top: 2px solid var(--gold);">

<h2 id="summary-key-points">
  <span data-lang="en">Summary: Key Takeaways</span>
  <span data-lang="zh">总结：关键要点</span>
</h2>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>ERGM is a probability model</strong> for networks: P(Y = y) ∝ exp(θ<sup>T</sup> g(y)). It assigns probability to each possible network based on its network statistics.</li>
  <li data-lang="zh"><strong>ERGM是网络的概率模型</strong>：P(Y = y) ∝ exp(θ<sup>T</sup> g(y))。它根据其网络统计量为每个可能的网络分配概率。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Network statistics</strong> (edges, clustering, reciprocity, homophily, etc.) are the predictors. Parameters θ quantify their effects on tie probability.</li>
  <li data-lang="zh"><strong>网络统计量</strong>（边、聚类、互惠、同群偏好等）是预测因子。参数θ量化它们对关系概率的影响。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>The normalizing constant κ(θ) is intractable</strong> (requires summing over all 2^(n choose 2) possible networks), so we use MCMC-MLE to estimate θ by comparing observed statistics to simulated statistics.</li>
  <li data-lang="zh"><strong>正规化常数κ(θ)难以处理</strong>（需要对所有 2^(n choose 2) 个可能网络求和），所以我们使用MCMC-MLE通过比较观察统计量和模拟统计量来估计θ。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Degeneracy is a major challenge:</strong> Some model specifications (e.g., raw triangle terms) produce U-shaped distributions concentrated on empty or complete networks. Use geometrically weighted terms (GWESP, GWDegree) instead.</li>
  <li data-lang="zh"><strong>退化是一个主要挑战：</strong>某些模型规范（例如原始三角形项）产生集中在空网络或完整网络上的U形分布。使用几何加权项（GWESP、GWDegree）。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>Always check convergence:</strong> Examine parameter trace plots, compare observed to simulated statistics (GOF), and verify that the model can generate networks resembling the observed data.</li>
  <li data-lang="zh"><strong>始终检查收敛：</strong>检查参数轨迹图、比较观察到的和模拟统计量（GOF），并验证模型可以生成类似观察数据的网络。</li>
</ul>

<ul style="margin-left: 2rem;">
  <li data-lang="en"><strong>ERGM is powerful but challenging:</strong> Unlike standard regression, network inference has fundamental computational barriers. Patience with MCMC diagnostics and iterative model refinement is essential.</li>
  <li data-lang="zh"><strong>ERGM很强大但很有挑战性：</strong>与标准回归不同，网络推断有基本的计算障碍。对MCMC诊断的耐心和迭代模型改进是必不可少的。</li>
</ul>

<div class="insight-box" style="background: var(--warm); color: white; padding: 1rem; border-radius: 0.3rem; margin-top: 1rem;">
  <p data-lang="en"><strong>Insight:</strong> ERGMs connect network science to statistical modeling. By specifying which network statistics matter (clustering, degree distribution, homophily), you embed your theoretical assumptions into a likelihood. The fitted parameters then quantify the strength of each mechanism. This makes ERGM invaluable for testing network theories and understanding why real networks take the shapes they do.</p>
  <p data-lang="zh"><strong>洞察：</strong>ERGM将网络科学与统计建模联系起来。通过指定哪些网络统计量很重要（聚类、度分布、同群偏好），你将理论假设嵌入似然函数中。然后拟合参数量化每个机制的强度。这使ERGM对于测试网络理论和理解为什么现实网络具有特定形状至关重要。</p>
</div>
