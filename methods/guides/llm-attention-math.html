<!DOCTYPE html>
<html lang="en" data-lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Head Attention: Complete Mathematical Derivation</title>
    <link rel="stylesheet" href="../style.css">
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400;1,500&family=EB+Garamond:ital,wght@0,400;0,500;1,400&family=Caveat:wght@400;500&family=IBM+Plex+Sans:wght@300;400;500&family=IBM+Plex+Mono:wght@400;500&family=Noto+Serif+SC:wght@300;400;500;600&family=Noto+Sans+SC:wght@300;400;500&display=swap" rel="stylesheet">
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'EB Garamond', serif;
            background: #f8f7f3;
            color: #2c2c2c;
            line-height: 1.8;
        }

        body.zh {
            font-family: 'Noto Serif SC', 'EB Garamond', serif;
        }

        .container {
            max-width: 780px;
            margin: 0 auto;
            padding: 40px 20px;
            background: #fafaf9;
        }

        .header {
            margin-bottom: 50px;
            border-bottom: 2px solid #d4af37;
            padding-bottom: 30px;
        }

        .header-tag {
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.75rem;
            letter-spacing: 0.15em;
            color: #d4af37;
            margin-bottom: 12px;
            font-weight: 500;
            text-transform: uppercase;
        }

        .header h1 {
            font-family: 'Cormorant Garamond', serif;
            font-size: 3em;
            font-weight: 300;
            line-height: 1.2;
            color: #1a1a1a;
            margin-bottom: 8px;
            letter-spacing: -0.02em;
        }

        .header .subtitle {
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.9rem;
            color: #666;
            font-weight: 300;
            letter-spacing: 0.05em;
        }

        .header-controls {
            margin-top: 20px;
            display: flex;
            gap: 20px;
            align-items: center;
        }

        .language-toggle {
            background: transparent;
            border: 1px solid #d4af37;
            color: #d4af37;
            padding: 8px 16px;
            border-radius: 3px;
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.85rem;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 500;
            letter-spacing: 0.05em;
        }

        .language-toggle:hover {
            background: #d4af37;
            color: #1a1a1a;
        }

        .back-link {
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.9rem;
            color: #666;
            text-decoration: none;
            transition: color 0.3s ease;
        }

        .back-link:hover {
            color: #d4af37;
        }

        .content {
            font-size: 1.05rem;
        }

        .section {
            margin-bottom: 45px;
        }

        .section h2 {
            font-family: 'Cormorant Garamond', serif;
            font-size: 2em;
            font-weight: 400;
            color: #1a1a1a;
            margin-bottom: 25px;
            margin-top: 45px;
            padding-bottom: 12px;
            border-bottom: 1px solid #e0e0e0;
            letter-spacing: -0.01em;
        }

        .section h3 {
            font-family: 'Cormorant Garamond', serif;
            font-size: 1.5em;
            font-weight: 400;
            color: #2c2c2c;
            margin-bottom: 18px;
            margin-top: 30px;
            letter-spacing: -0.01em;
        }

        .section h4 {
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.95rem;
            font-weight: 500;
            color: #444;
            margin-bottom: 12px;
            margin-top: 20px;
            text-transform: uppercase;
            letter-spacing: 0.08em;
        }

        p {
            margin-bottom: 18px;
            text-align: justify;
        }

        .math-box {
            background: #f0ebe2;
            border-left: 4px solid #d4af37;
            padding: 20px;
            margin: 25px 0;
            border-radius: 2px;
            overflow-x: auto;
        }

        .math-box.derivation {
            background: #fff9f0;
            border-left-color: #e8a76d;
        }

        .math-box.definition {
            background: #f5f5f0;
            border-left-color: #888;
        }

        .math-box.note {
            background: #fef9f3;
            border-left-color: #c9a054;
        }

        .equation {
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.95rem;
            margin: 15px 0;
            line-height: 1.6;
        }

        .text-zh {
            display: none;
        }

        .text-en {
            display: block;
        }

        body.zh .text-zh {
            display: block;
        }

        body.zh .text-en {
            display: none;
        }

        .lang-label {
            display: inline;
        }

        body.zh .lang-label {
            display: inline;
        }

        ul, ol {
            margin: 20px 0 20px 30px;
            line-height: 1.9;
        }

        li {
            margin-bottom: 12px;
        }

        .highlight {
            background-color: #fff8dc;
            padding: 2px 6px;
            border-radius: 2px;
            font-family: 'IBM Plex Mono', monospace;
            font-size: 0.95em;
        }

        .derivation-step {
            margin: 20px 0;
            padding-left: 20px;
            border-left: 2px solid #e8a76d;
        }

        .step-label {
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.85rem;
            color: #c9a054;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.08em;
            margin-bottom: 8px;
        }

        .toc {
            background: #fafaf5;
            border: 1px solid #e0e0e0;
            padding: 25px;
            margin-bottom: 40px;
            border-radius: 2px;
        }

        .toc h3 {
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.95rem;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.08em;
            color: #666;
            margin-bottom: 15px;
            margin-top: 0;
        }

        .toc ul {
            list-style: none;
            margin: 0;
            padding: 0;
        }

        .toc li {
            margin-bottom: 10px;
        }

        .toc a {
            color: #d4af37;
            text-decoration: none;
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.95rem;
            transition: color 0.3s ease;
        }

        .toc a:hover {
            color: #1a1a1a;
        }

        .footer {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 1px solid #e0e0e0;
            font-family: 'IBM Plex Sans', sans-serif;
            font-size: 0.85rem;
            color: #999;
            text-align: center;
        }

        @media (max-width: 600px) {
            .header h1 {
                font-size: 2em;
            }

            .section h2 {
                font-size: 1.5em;
            }

            .section h3 {
                font-size: 1.2em;
            }

            .container {
                padding: 25px 15px;
            }
        }

        /* MathJax overflow handling */
        .MathJax {
            overflow-x: auto;
        }

        .mjx-chtml {
            display: inline-block;
        }
    </style>
</head>
<body class="en">
    <div class="container">
        <div class="header">
            <div class="header-tag">
                <span class="text-en">ADVANCED Â· PROOF & DERIVATION</span>
                <span class="text-zh">è¿›é˜¶ Â· è¯æ˜ä¸æ¨å¯¼</span>
            </div>
            <h1>
                <span class="text-en">Multi-Head Attention:<br>Complete Mathematical Derivation</span>
                <span class="text-zh">å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼š<br>å®Œæ•´æ•°å­¦æ¨å¯¼</span>
            </h1>
            <p class="subtitle">
                <span class="text-en">Foundations of transformer architecture from first principles</span>
                <span class="text-zh">ä»ç¬¬ä¸€åŸç†å‡ºå‘çš„Transformeræ¶æ„åŸºç¡€</span>
            </p>

            <div class="header-controls">
                <button class="language-toggle" onclick="toggleLanguage()">
                    <span class="text-en">ä¸­æ–‡ (ZH)</span>
                    <span class="text-zh">English (EN)</span>
                </button>
                <a href="../llm.html" class="back-link">
                    <span class="text-en">â† Back to LLM Methods</span>
                    <span class="text-zh">â† è¿”å›LLMæ–¹æ³•</span>
                </a>
            </div>
        </div>

        <div class="toc">
            <h3>
                <span class="text-en">Contents</span>
                <span class="text-zh">ç›®å½•</span>
            </h3>
            <ul>
                <li><a href="#scaled-dot-product">
                    <span class="text-en">Scaled Dot-Product Attention</span>
                    <span class="text-zh">ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›</span>
                </a></li>
                <li><a href="#variance-analysis">
                    <span class="text-en">Variance Analysis: Why Scale by âˆšd_k</span>
                    <span class="text-zh">æ–¹å·®åˆ†æï¼šä¸ºä»€ä¹ˆè¦ç¼©æ”¾âˆšd_k</span>
                </a></li>
                <li><a href="#gradient-derivation">
                    <span class="text-en">Gradient Derivation</span>
                    <span class="text-zh">æ¢¯åº¦æ¨å¯¼</span>
                </a></li>
                <li><a href="#multihead">
                    <span class="text-en">Multi-Head Attention Architecture</span>
                    <span class="text-zh">å¤šå¤´æ³¨æ„åŠ›æ¶æ„</span>
                </a></li>
                <li><a href="#positional-encoding">
                    <span class="text-en">Positional Encoding Mathematics</span>
                    <span class="text-zh">ä½ç½®ç¼–ç æ•°å­¦</span>
                </a></li>
                <li><a href="#self-vs-cross">
                    <span class="text-en">Self-Attention vs Cross-Attention</span>
                    <span class="text-zh">è‡ªæ³¨æ„åŠ›ä¸äº¤å‰æ³¨æ„åŠ›</span>
                </a></li>
                <li><a href="#complexity">
                    <span class="text-en">Computational Complexity Analysis</span>
                    <span class="text-zh">è®¡ç®—å¤æ‚æ€§åˆ†æ</span>
                </a></li>
            </ul>
        </div>

        <div class="content">

            <!-- SECTION 1: SCALED DOT-PRODUCT ATTENTION -->
            <div class="section" id="scaled-dot-product">
                <h2>
                    <span class="text-en">1. Scaled Dot-Product Attention</span>
                    <span class="text-zh">1. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›</span>
                </h2>

                <p>
                    <span class="text-en">Attention is a mechanism that learns to selectively focus on relevant information. The scaled dot-product attention function computes a weighted sum of values based on the similarity between queries and keys.</span>
                    <span class="text-zh">æ³¨æ„åŠ›æ˜¯ä¸€ç§å­¦ä¹ æœ‰é€‰æ‹©åœ°å…³æ³¨ç›¸å…³ä¿¡æ¯çš„æœºåˆ¶ã€‚ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›å‡½æ•°æ ¹æ®æŸ¥è¯¢å’Œé”®ä¹‹é—´çš„ç›¸ä¼¼æ€§è®¡ç®—å€¼çš„åŠ æƒå’Œã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Definition</span>
                    <span class="text-zh">å®šä¹‰</span>
                </h3>

                <div class="math-box definition">
                    <div class="equation">$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</div>
                </div>

                <p>
                    <span class="text-en">Where:</span>
                    <span class="text-zh">å…¶ä¸­ï¼š</span>
                </p>
                <ul>
                    <li><span class="text-en"><span class="highlight">Q</span> âˆˆ â„^(nÃ—d_k): Query matrix</span>
                        <span class="text-zh"><span class="highlight">Q</span> âˆˆ â„^(nÃ—d_k): æŸ¥è¯¢çŸ©é˜µ</span></li>
                    <li><span class="text-en"><span class="highlight">K</span> âˆˆ â„^(mÃ—d_k): Key matrix</span>
                        <span class="text-zh"><span class="highlight">K</span> âˆˆ â„^(mÃ—d_k): é”®çŸ©é˜µ</span></li>
                    <li><span class="text-en"><span class="highlight">V</span> âˆˆ â„^(mÃ—d_v): Value matrix</span>
                        <span class="text-zh"><span class="highlight">V</span> âˆˆ â„^(mÃ—d_v): å€¼çŸ©é˜µ</span></li>
                    <li><span class="text-en"><span class="highlight">d_k</span>: Dimension of keys (typically d_model / num_heads)</span>
                        <span class="text-zh"><span class="highlight">d_k</span>: é”®çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸ºd_model / num_headsï¼‰</span></li>
                </ul>

                <h3>
                    <span class="text-en">Step-by-Step Computation</span>
                    <span class="text-zh">åˆ†æ­¥è®¡ç®—</span>
                </h3>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 1: Compute Attention Scores</span>
                        <span class="text-zh">æ­¥éª¤1ï¼šè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$S = QK^T \in \mathbb{R}^{n \times m}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">Each element S_{ij} = âŸ¨q_i, k_jâŸ© measures the similarity between query i and key j.</span>
                            <span class="text-zh">æ¯ä¸ªå…ƒç´ S_{ij} = âŸ¨q_i, k_jâŸ©æµ‹é‡æŸ¥è¯¢iå’Œé”®jä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚</span>
                        </p>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 2: Scale by âˆšd_k</span>
                        <span class="text-zh">æ­¥éª¤2ï¼šç¼©æ”¾âˆšd_k</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$\tilde{S} = \frac{S}{\sqrt{d_k}} = \frac{QK^T}{\sqrt{d_k}}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">This prevents attention scores from becoming too large, which would cause the softmax gradient to vanish.</span>
                            <span class="text-zh">è¿™é˜²æ­¢æ³¨æ„åŠ›åˆ†æ•°å˜å¾—å¤ªå¤§ï¼Œè¿™ä¼šå¯¼è‡´softmaxæ¢¯åº¦æ¶ˆå¤±ã€‚</span>
                        </p>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 3: Apply Softmax</span>
                        <span class="text-zh">æ­¥éª¤3ï¼šåº”ç”¨Softmax</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$A = \text{softmax}\left(\tilde{S}\right) \in \mathbb{R}^{n \times m}$$</div>
                        <div class="equation">$$A_{ij} = \frac{\exp(\tilde{S}_{ij})}{\sum_{k=1}^{m} \exp(\tilde{S}_{ik})}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">Each row of A represents a probability distribution over the m keys, summing to 1.</span>
                            <span class="text-zh">Açš„æ¯ä¸€è¡Œä»£è¡¨mä¸ªé”®ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒï¼Œæ±‚å’Œä¸º1ã€‚</span>
                        </p>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 4: Weighted Sum of Values</span>
                        <span class="text-zh">æ­¥éª¤4ï¼šå€¼çš„åŠ æƒå’Œ</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$O = AV \in \mathbb{R}^{n \times d_v}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">The output is a weighted combination of value vectors, where weights A_{ij} determine how much each value contributes to output i.</span>
                            <span class="text-zh">è¾“å‡ºæ˜¯å€¼å‘é‡çš„åŠ æƒç»„åˆï¼Œå…¶ä¸­æƒé‡A_{ij}ç¡®å®šæ¯ä¸ªå€¼å¯¹è¾“å‡ºiçš„è´¡çŒ®ç¨‹åº¦ã€‚</span>
                        </p>
                    </div>
                </div>
            </div>

            <!-- SECTION 2: VARIANCE ANALYSIS -->
            <div class="section" id="variance-analysis">
                <h2>
                    <span class="text-en">2. Variance Analysis: Why Scale by âˆšd_k?</span>
                    <span class="text-zh">2. æ–¹å·®åˆ†æï¼šä¸ºä»€ä¹ˆè¦ç¼©æ”¾âˆšd_kï¼Ÿ</span>
                </h2>

                <p>
                    <span class="text-en">The scaling factor is not arbitrary. It emerges from analyzing the variance of dot products between random vectors.</span>
                    <span class="text-zh">ç¼©æ”¾å› å­ä¸æ˜¯ä»»æ„çš„ã€‚å®ƒæ¥è‡ªäºéšæœºå‘é‡ä¹‹é—´ç‚¹ç§¯æ–¹å·®çš„åˆ†æã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Variance of Dot Products</span>
                    <span class="text-zh">ç‚¹ç§¯çš„æ–¹å·®</span>
                </h3>

                <p>
                    <span class="text-en">Consider two independent random vectors <span class="highlight">q</span> and <span class="highlight">k</span>, both with dimension d_k, where each element is drawn from a standard normal distribution ğ’©(0,1).</span>
                    <span class="text-zh">è€ƒè™‘ä¸¤ä¸ªç‹¬ç«‹çš„éšæœºå‘é‡<span class="highlight">q</span>å’Œ<span class="highlight">k</span>ï¼Œä¸¤è€…ç»´åº¦ä¸ºd_kï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒğ’©(0,1)ä¸­æŠ½å–ã€‚</span>
                </p>

                <div class="math-box derivation">
                    <div class="step-label">
                        <span class="text-en">Variance Calculation</span>
                        <span class="text-zh">æ–¹å·®è®¡ç®—</span>
                    </div>
                    <div class="equation">$$q, k \sim \mathcal{N}(0, 1)^{d_k}$$</div>
                    <div class="equation">$$\text{Var}(\langle q, k \rangle) = \mathbb{E}[\langle q, k \rangle^2] - (\mathbb{E}[\langle q, k \rangle])^2$$</div>
                </div>

                <p>
                    <span class="text-en">Since q and k are zero-mean, E[âŸ¨q,kâŸ©] = 0. The variance simplifies to:</span>
                    <span class="text-zh">ç”±äºqå’Œkæ˜¯é›¶å‡å€¼ï¼ŒE[âŸ¨q,kâŸ©] = 0ã€‚æ–¹å·®ç®€åŒ–ä¸ºï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$\text{Var}(\langle q, k \rangle) = \mathbb{E}[\langle q, k \rangle^2] = \mathbb{E}\left[\left(\sum_{i=1}^{d_k} q_i k_i\right)^2\right]$$</div>
                </div>

                <p>
                    <span class="text-en">Expanding the square and using independence:</span>
                    <span class="text-zh">å±•å¼€å¹³æ–¹å¹¶ä½¿ç”¨ç‹¬ç«‹æ€§ï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$= \mathbb{E}\left[\sum_{i=1}^{d_k} q_i^2 k_i^2 + \sum_{i \neq j} q_i q_j k_i k_j\right]$$</div>
                    <div class="equation">$$= \sum_{i=1}^{d_k} \mathbb{E}[q_i^2]\mathbb{E}[k_i^2] + \sum_{i \neq j} \mathbb{E}[q_i]\mathbb{E}[q_j]\mathbb{E}[k_i]\mathbb{E}[k_j]$$</div>
                </div>

                <p>
                    <span class="text-en">Since q_i, k_i ~ ğ’©(0,1), we have E[q_iÂ²] = E[k_iÂ²] = 1, and E[q_i] = E[k_i] = 0:</span>
                    <span class="text-zh">ç”±äºq_i, k_i ~ ğ’©(0,1)ï¼Œæˆ‘ä»¬æœ‰E[q_iÂ²] = E[k_iÂ²] = 1ï¼Œä¸”E[q_i] = E[k_i] = 0ï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$\text{Var}(\langle q, k \rangle) = d_k \cdot 1 \cdot 1 = d_k$$</div>
                </div>

                <h3>
                    <span class="text-en">Effect of Scaling</span>
                    <span class="text-zh">ç¼©æ”¾çš„æ•ˆæœ</span>
                </h3>

                <div class="math-box note">
                    <div class="equation">$$\text{Var}\left(\frac{\langle q, k \rangle}{\sqrt{d_k}}\right) = \frac{1}{d_k} \text{Var}(\langle q, k \rangle) = \frac{d_k}{d_k} = 1$$</div>
                </div>

                <p>
                    <span class="text-en">By scaling attention scores by 1/âˆšd_k, we ensure that regardless of the embedding dimension, attention logits maintain stable variance. This prevents extreme values that would cause softmax to saturate, leading to vanishing gradients during backpropagation.</span>
                    <span class="text-zh">é€šè¿‡æŒ‰1/âˆšd_kç¼©æ”¾æ³¨æ„åŠ›åˆ†æ•°ï¼Œæˆ‘ä»¬ç¡®ä¿æ— è®ºåµŒå…¥ç»´åº¦å¦‚ä½•ï¼Œæ³¨æ„åŠ›logitséƒ½èƒ½ä¿æŒç¨³å®šçš„æ–¹å·®ã€‚è¿™é˜²æ­¢äº†æç«¯å€¼å¯¼è‡´softmaxé¥±å’Œï¼Œä»è€Œåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚</span>
                </p>
            </div>

            <!-- SECTION 3: GRADIENT DERIVATION -->
            <div class="section" id="gradient-derivation">
                <h2>
                    <span class="text-en">3. Gradient Derivation</span>
                    <span class="text-zh">3. æ¢¯åº¦æ¨å¯¼</span>
                </h2>

                <p>
                    <span class="text-en">Computing gradients with respect to Q, K, and V is essential for training. We derive each using the chain rule.</span>
                    <span class="text-zh">è®¡ç®—å…³äºQã€Kå’ŒVçš„æ¢¯åº¦å¯¹äºè®­ç»ƒè‡³å…³é‡è¦ã€‚æˆ‘ä»¬ä½¿ç”¨é“¾å¼æ³•åˆ™æ¨å¯¼æ¯ä¸ªæ¢¯åº¦ã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Forward Pass Review</span>
                    <span class="text-zh">å‰å‘ä¼ æ’­å›é¡¾</span>
                </h3>

                <div class="math-box">
                    <div class="equation">$$S = QK^T$$</div>
                    <div class="equation">$$\tilde{S} = \frac{S}{\sqrt{d_k}}$$</div>
                    <div class="equation">$$A = \text{softmax}(\tilde{S})$$</div>
                    <div class="equation">$$O = AV$$</div>
                </div>

                <h3>
                    <span class="text-en">Gradient w.r.t. V</span>
                    <span class="text-zh">å…³äºVçš„æ¢¯åº¦</span>
                </h3>

                <div class="derivation-step">
                    <div class="math-box">
                        <div class="equation">$$\frac{\partial \mathcal{L}}{\partial V} = \frac{\partial \mathcal{L}}{\partial O} \cdot \frac{\partial O}{\partial V} = A^T \frac{\partial \mathcal{L}}{\partial O}$$</div>
                    </div>
                    <p style="margin-top: 15px;">
                        <span class="text-en">Since O = AV (matrix multiplication), âˆ‚O/âˆ‚V = A^T. This gradient is straightforward: the contribution to each value is weighted by its attention weights.</span>
                        <span class="text-zh">ç”±äºO = AVï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰ï¼Œâˆ‚O/âˆ‚V = A^Tã€‚è¿™ä¸ªæ¢¯åº¦å¾ˆç›´æ¥ï¼šæ¯ä¸ªå€¼çš„è´¡çŒ®ç”±å…¶æ³¨æ„åŠ›æƒé‡åŠ æƒã€‚</span>
                    </p>
                </div>

                <h3>
                    <span class="text-en">Gradient w.r.t. A (Attention Matrix)</span>
                    <span class="text-zh">å…³äºAï¼ˆæ³¨æ„åŠ›çŸ©é˜µï¼‰çš„æ¢¯åº¦</span>
                </h3>

                <div class="derivation-step">
                    <div class="math-box">
                        <div class="equation">$$\frac{\partial \mathcal{L}}{\partial A} = \frac{\partial \mathcal{L}}{\partial O} \cdot \frac{\partial O}{\partial A} = \frac{\partial \mathcal{L}}{\partial O} \cdot V^T$$</div>
                    </div>
                </div>

                <h3>
                    <span class="text-en">Gradient of Softmax</span>
                    <span class="text-zh">Softmaxçš„æ¢¯åº¦</span>
                </h3>

                <p>
                    <span class="text-en">The softmax derivative is more complex. For a softmax function:</span>
                    <span class="text-zh">softmaxå¯¼æ•°æ›´å¤æ‚ã€‚å¯¹äºsoftmaxå‡½æ•°ï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$\frac{\partial A_{ij}}{\partial \tilde{S}_{kl}} = \delta_{ik}(A_{ij}\delta_{jl} - A_{ij}A_{il})$$</div>
                </div>

                <p>
                    <span class="text-en">Where Î´ is the Kronecker delta. For elements in the same row (i=k), this simplifies to:</span>
                    <span class="text-zh">å…¶ä¸­Î´æ˜¯Kroneckerå¢é‡ã€‚å¯¹äºåŒä¸€è¡Œä¸­çš„å…ƒç´ ï¼ˆi=kï¼‰ï¼Œè¿™ç®€åŒ–ä¸ºï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$\frac{\partial}{\partial \tilde{S}_{ij}} = A_{ij}(\delta_{jj'} - A_{ij'})$$</div>
                </div>

                <h3>
                    <span class="text-en">Gradient w.r.t. Scaled Scores</span>
                    <span class="text-zh">å…³äºç¼©æ”¾åˆ†æ•°çš„æ¢¯åº¦</span>
                </h3>

                <div class="derivation-step">
                    <div class="math-box">
                        <div class="equation">$$\frac{\partial \mathcal{L}}{\partial \tilde{S}} = \frac{\partial \mathcal{L}}{\partial A} \odot A \left(I - A\right)$$</div>
                    </div>
                    <p style="margin-top: 15px;">
                        <span class="text-en">Where âŠ™ denotes element-wise multiplication (Hadamard product).</span>
                        <span class="text-zh">å…¶ä¸­âŠ™è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•ï¼ˆHadamardä¹˜ç§¯ï¼‰ã€‚</span>
                    </p>
                </div>

                <h3>
                    <span class="text-en">Gradient w.r.t. Q and K</span>
                    <span class="text-zh">å…³äºQå’ŒKçš„æ¢¯åº¦</span>
                </h3>

                <p>
                    <span class="text-en">Since <span class="highlight">S = QK^T</span> and <span class="highlight">âˆ‚S/âˆ‚Q = K</span>:</span>
                    <span class="text-zh">ç”±äº<span class="highlight">S = QK^T</span>ä¸”<span class="highlight">âˆ‚S/âˆ‚Q = K</span>ï¼š</span>
                </p>

                <div class="derivation-step">
                    <div class="math-box">
                        <div class="equation">$$\frac{\partial \mathcal{L}}{\partial \tilde{S}} = \frac{1}{\sqrt{d_k}} \frac{\partial \mathcal{L}}{\partial S}$$</div>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="math-box">
                        <div class="equation">$$\frac{\partial \mathcal{L}}{\partial Q} = \frac{1}{\sqrt{d_k}} \frac{\partial \mathcal{L}}{\partial S} K$$</div>
                        <div class="equation">$$\frac{\partial \mathcal{L}}{\partial K} = \frac{1}{\sqrt{d_k}} Q^T \frac{\partial \mathcal{L}}{\partial S}$$</div>
                    </div>
                </div>
            </div>

            <!-- SECTION 4: MULTI-HEAD ATTENTION -->
            <div class="section" id="multihead">
                <h2>
                    <span class="text-en">4. Multi-Head Attention Architecture</span>
                    <span class="text-zh">4. å¤šå¤´æ³¨æ„åŠ›æ¶æ„</span>
                </h2>

                <p>
                    <span class="text-en">Single-head attention may limit the model's ability to attend to information from different representation subspaces. Multi-head attention enables simultaneous attention to multiple subspaces.</span>
                    <span class="text-zh">å•å¤´æ³¨æ„åŠ›å¯èƒ½é™åˆ¶äº†æ¨¡å‹ä»ä¸åŒè¡¨ç¤ºå­ç©ºé—´å…³æ³¨ä¿¡æ¯çš„èƒ½åŠ›ã€‚å¤šå¤´æ³¨æ„åŠ›ä½¿åŒæ—¶å…³æ³¨å¤šä¸ªå­ç©ºé—´æˆä¸ºå¯èƒ½ã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Architecture Definition</span>
                    <span class="text-zh">æ¶æ„å®šä¹‰</span>
                </h3>

                <div class="math-box definition">
                    <div class="equation">$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$$</div>
                    <div class="equation">$$\text{where } \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$</div>
                </div>

                <p>
                    <span class="text-en">Where:</span>
                    <span class="text-zh">å…¶ä¸­ï¼š</span>
                </p>
                <ul>
                    <li><span class="text-en"><span class="highlight">h</span>: Number of attention heads (typically 8, 12, or 16)</span>
                        <span class="text-zh"><span class="highlight">h</span>: æ³¨æ„åŠ›å¤´æ•°ï¼ˆé€šå¸¸ä¸º8ã€12æˆ–16ï¼‰</span></li>
                    <li><span class="text-en"><span class="highlight">d_k = d_v = d_model / h</span>: Dimension of each head</span>
                        <span class="text-zh"><span class="highlight">d_k = d_v = d_model / h</span>: æ¯ä¸ªå¤´çš„ç»´åº¦</span></li>
                    <li><span class="text-en"><span class="highlight">W_i^Q, W_i^K, W_i^V</span> âˆˆ â„^(d_model Ã— d_k): Projection matrices for head i</span>
                        <span class="text-zh"><span class="highlight">W_i^Q, W_i^K, W_i^V</span> âˆˆ â„^(d_model Ã— d_k): å¤´içš„æŠ•å½±çŸ©é˜µ</span></li>
                    <li><span class="text-en"><span class="highlight">W^O</span> âˆˆ â„^(hÂ·d_v Ã— d_model): Output projection</span>
                        <span class="text-zh"><span class="highlight">W^O</span> âˆˆ â„^(hÂ·d_v Ã— d_model): è¾“å‡ºæŠ•å½±</span></li>
                </ul>

                <h3>
                    <span class="text-en">Dimension Analysis</span>
                    <span class="text-zh">ç»´åº¦åˆ†æ</span>
                </h3>

                <div class="math-box note">
                    <p style="margin-top: 0;">
                        <span class="text-en">For a transformer with d_model = 512 and h = 8:</span>
                        <span class="text-zh">å¯¹äºd_model = 512ä¸”h = 8çš„transformerï¼š</span>
                    </p>
                    <div class="equation">$$d_k = d_v = \frac{512}{8} = 64$$</div>
                    <ul style="margin-bottom: 0;">
                        <li><span class="text-en">Each W_i^Q, W_i^K, W_i^V has shape 512 Ã— 64</span>
                            <span class="text-zh">æ¯ä¸ªW_i^Q, W_i^K, W_i^Vçš„å½¢çŠ¶ä¸º512 Ã— 64</span></li>
                        <li><span class="text-en">Each head output has shape (batch_size, seq_len, 64)</span>
                            <span class="text-zh">æ¯ä¸ªå¤´çš„è¾“å‡ºå½¢çŠ¶ä¸º(batch_size, seq_len, 64)</span></li>
                        <li><span class="text-en">Concatenated output has shape (batch_size, seq_len, 512)</span>
                            <span class="text-zh">è¿æ¥è¾“å‡ºçš„å½¢çŠ¶ä¸º(batch_size, seq_len, 512)</span></li>
                    </ul>
                </div>

                <h3>
                    <span class="text-en">Computational Flow</span>
                    <span class="text-zh">è®¡ç®—æµç¨‹</span>
                </h3>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 1: Projection</span>
                        <span class="text-zh">æ­¥éª¤1ï¼šæŠ•å½±</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$Q_i = QW_i^Q, \quad K_i = KW_i^K, \quad V_i = VW_i^V$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">Project input to each head's subspace. Each projection matrix learns different aspects of the input.</span>
                            <span class="text-zh">å°†è¾“å…¥æŠ•å½±åˆ°æ¯ä¸ªå¤´çš„å­ç©ºé—´ã€‚æ¯ä¸ªæŠ•å½±çŸ©é˜µå­¦ä¹ è¾“å…¥çš„ä¸åŒæ–¹é¢ã€‚</span>
                        </p>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 2: Apply Attention</span>
                        <span class="text-zh">æ­¥éª¤2ï¼šåº”ç”¨æ³¨æ„åŠ›</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$\text{head}_i = \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right)V_i$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">Each head computes attention independently within its subspace.</span>
                            <span class="text-zh">æ¯ä¸ªå¤´åœ¨å…¶å­ç©ºé—´å†…ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ã€‚</span>
                        </p>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 3: Concatenate</span>
                        <span class="text-zh">æ­¥éª¤3ï¼šè¿æ¥</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$\text{Concat} = [\text{head}_1, \text{head}_2, \ldots, \text{head}_h] \in \mathbb{R}^{n \times (h \cdot d_v)}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">Concatenate outputs from all heads along the feature dimension.</span>
                            <span class="text-zh">æ²¿ç‰¹å¾ç»´è¿æ¥æ‰€æœ‰å¤´çš„è¾“å‡ºã€‚</span>
                        </p>
                    </div>
                </div>

                <div class="derivation-step">
                    <div class="step-label">
                        <span class="text-en">Step 4: Output Projection</span>
                        <span class="text-zh">æ­¥éª¤4ï¼šè¾“å‡ºæŠ•å½±</span>
                    </div>
                    <div class="math-box">
                        <div class="equation">$$\text{MultiHead}(Q,K,V) = \text{Concat} \cdot W^O \in \mathbb{R}^{n \times d_{model}}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">A learned linear transformation combines information from all heads back to the original dimension.</span>
                            <span class="text-zh">å­¦ä¹ çš„çº¿æ€§å˜æ¢å°†æ¥è‡ªæ‰€æœ‰å¤´çš„ä¿¡æ¯ç»„åˆå›åŸå§‹ç»´åº¦ã€‚</span>
                        </p>
                    </div>
                </div>

                <h3>
                    <span class="text-en">Why Multi-Head Matters</span>
                    <span class="text-zh">ä¸ºä»€ä¹ˆå¤šå¤´å¾ˆé‡è¦</span>
                </h3>

                <p>
                    <span class="text-en">Multi-head attention provides:</span>
                    <span class="text-zh">å¤šå¤´æ³¨æ„åŠ›æä¾›ï¼š</span>
                </p>
                <ul>
                    <li><span class="text-en"><span class="highlight">Representation Diversity</span>: Different heads can learn different attention patterns (e.g., syntactic vs. semantic)</span>
                        <span class="text-zh"><span class="highlight">è¡¨ç¤ºå¤šæ ·æ€§</span>ï¼šä¸åŒçš„å¤´å¯ä»¥å­¦ä¹ ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œå¥æ³•ä¸è¯­ä¹‰ï¼‰</span></li>
                    <li><span class="text-en"><span class="highlight">Parallel Computation</span>: Heads compute in parallel, enabling efficient computation</span>
                        <span class="text-zh"><span class="highlight">å¹¶è¡Œè®¡ç®—</span>ï¼šå¤´å¹¶è¡Œè®¡ç®—ï¼Œå¯ç”¨é«˜æ•ˆè®¡ç®—</span></li>
                    <li><span class="text-en"><span class="highlight">Reduced Complexity per Head</span>: Each head operates on reduced dimension (d_k rather than d_model), lowering per-head computational cost</span>
                        <span class="text-zh"><span class="highlight">å‡å°‘æ¯ä¸ªå¤´çš„å¤æ‚æ€§</span>ï¼šæ¯ä¸ªå¤´åœ¨é™ä½çš„ç»´åº¦ä¸Šè¿è¡Œï¼ˆd_kè€Œä¸æ˜¯d_modelï¼‰ï¼Œé™ä½æ¯ä¸ªå¤´çš„è®¡ç®—æˆæœ¬</span></li>
                </ul>
            </div>

            <!-- SECTION 5: POSITIONAL ENCODING -->
            <div class="section" id="positional-encoding">
                <h2>
                    <span class="text-en">5. Positional Encoding Mathematics</span>
                    <span class="text-zh">5. ä½ç½®ç¼–ç æ•°å­¦</span>
                </h2>

                <p>
                    <span class="text-en">Transformers are permutation-invariant: they treat input sequences identically regardless of token order. Positional encodings inject order information into the model.</span>
                    <span class="text-zh">Transformeræ˜¯æ’åˆ—ä¸å˜çš„ï¼šæ— è®ºä»¤ç‰Œé¡ºåºå¦‚ä½•ï¼Œå®ƒä»¬éƒ½ä»¥ç›¸åŒæ–¹å¼å¤„ç†è¾“å…¥åºåˆ—ã€‚ä½ç½®ç¼–ç å°†é¡ºåºä¿¡æ¯æ³¨å…¥æ¨¡å‹ä¸­ã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Sinusoidal Positional Encoding</span>
                    <span class="text-zh">æ­£å¼¦ä½ç½®ç¼–ç </span>
                </h3>

                <div class="math-box definition">
                    <div class="equation">$$PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$</div>
                    <div class="equation">$$PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$$</div>
                    <p style="margin-bottom: 0; margin-top: 15px;">
                        <span class="text-en">where pos is the position in the sequence and i is the dimension index.</span>
                        <span class="text-zh">å…¶ä¸­posæ˜¯åºåˆ—ä¸­çš„ä½ç½®ï¼Œiæ˜¯ç»´åº¦ç´¢å¼•ã€‚</span>
                    </p>
                </div>

                <h3>
                    <span class="text-en">Mathematical Properties</span>
                    <span class="text-zh">æ•°å­¦æ€§è´¨</span>
                </h3>

                <h4>
                    <span class="text-en">1. Linear Attention Distance</span>
                    <span class="text-zh">1. çº¿æ€§æ³¨æ„åŠ›è·ç¦»</span>
                </h4>

                <p>
                    <span class="text-en">A key property is that PE(pos+k) can be expressed as a linear function of PE(pos):</span>
                    <span class="text-zh">ä¸€ä¸ªå…³é”®æ€§è´¨æ˜¯PE(pos+k)å¯ä»¥è¡¨ç¤ºä¸ºPE(pos)çš„çº¿æ€§å‡½æ•°ï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$PE(pos+k, i) = PE(pos, i)\cos(k\omega_i) + PE(pos', i)\sin(k\omega_i)$$</div>
                    <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                        <span class="text-en">where Ï‰_i = 1/10000^(2i/d_model) and pos' is the complementary dimension. This allows the model to learn relative positions efficiently.</span>
                        <span class="text-zh">å…¶ä¸­Ï‰_i = 1/10000^(2i/d_model)ï¼Œpos'æ˜¯è¡¥å……ç»´åº¦ã€‚è¿™å…è®¸æ¨¡å‹æœ‰æ•ˆåœ°å­¦ä¹ ç›¸å¯¹ä½ç½®ã€‚</span>
                    </p>
                </div>

                <h4>
                    <span class="text-en">2. Periodicity Analysis</span>
                    <span class="text-zh">2. å‘¨æœŸæ€§åˆ†æ</span>
                </h4>

                <p>
                    <span class="text-en">Each sinusoidal component has a period that increases with i:</span>
                    <span class="text-zh">æ¯ä¸ªæ­£å¼¦æˆåˆ†çš„å‘¨æœŸéšiå¢åŠ ï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$T_i = 2\pi \cdot 10000^{2i/d_{model}}$$</div>
                </div>

                <p>
                    <span class="text-en">For i = 0, T_0 â‰ˆ 6.28 (cycle every ~6 positions). For i = d_model/2, T_(d_model/2) â‰ˆ 2Ï€Â·10000 (very long period).</span>
                    <span class="text-zh">å¯¹äºi = 0ï¼ŒT_0 â‰ˆ 6.28ï¼ˆæ¯~6ä¸ªä½ç½®å¾ªç¯ä¸€æ¬¡ï¼‰ã€‚å¯¹äºi = d_model/2ï¼ŒT_(d_model/2) â‰ˆ 2Ï€Â·10000ï¼ˆéå¸¸é•¿çš„å‘¨æœŸï¼‰ã€‚</span>
                </p>

                <h4>
                    <span class="text-en">3. Uniqueness</span>
                    <span class="text-zh">3. å”¯ä¸€æ€§</span>
                </h4>

                <p>
                    <span class="text-en">Different positions have unique encoding vectors (within practical sequence lengths), allowing the model to distinguish positions:</span>
                    <span class="text-zh">ä¸åŒçš„ä½ç½®å…·æœ‰å”¯ä¸€çš„ç¼–ç å‘é‡ï¼ˆåœ¨å®é™…åºåˆ—é•¿åº¦å†…ï¼‰ï¼Œå…è®¸æ¨¡å‹åŒºåˆ†ä½ç½®ï¼š</span>
                </p>

                <div class="math-box note">
                    <p style="margin-top: 0;">
                        <span class="text-en">For a d_model = 512 dimension and sequence length â‰¤ 5000, PE vectors are nearly orthogonal, ensuring position distinction.</span>
                        <span class="text-zh">å¯¹äºd_model = 512ç»´åº¦å’Œåºåˆ—é•¿åº¦â‰¤ 5000ï¼ŒPEå‘é‡å‡ ä¹æ­£äº¤ï¼Œç¡®ä¿ä½ç½®åŒºåˆ†ã€‚</span>
                    </p>
                </div>

                <h3>
                    <span class="text-en">Rotary Position Embedding (RoPE)</span>
                    <span class="text-zh">æ—‹è½¬ä½ç½®åµŒå…¥ï¼ˆRoPEï¼‰</span>
                </h3>

                <p>
                    <span class="text-en">RoPE, introduced in recent models (e.g., GPT-Neo-X, LLaMA), encodes position by rotating query and key vectors in their 2D subspaces.</span>
                    <span class="text-zh">RoPEåœ¨æœ€è¿‘çš„æ¨¡å‹ï¼ˆä¾‹å¦‚GPT-Neo-Xã€LLaMAï¼‰ä¸­å¼•å…¥ï¼Œé€šè¿‡åœ¨å…¶2Då­ç©ºé—´ä¸­æ—‹è½¬æŸ¥è¯¢å’Œé”®å‘é‡æ¥ç¼–ç ä½ç½®ã€‚</span>
                </p>

                <div class="math-box definition">
                    <p style="margin-top: 0; margin-bottom: 10px;">
                        <span class="text-en"><span class="highlight">RoPE Rotation Matrix</span> for dimension pair (2i, 2i+1):</span>
                        <span class="text-zh"><span class="highlight">RoPEæ—‹è½¬çŸ©é˜µ</span>ï¼ˆç»´åº¦å¯¹(2i, 2i+1)ï¼‰ï¼š</span>
                    </p>
                    <div class="equation">$$\mathcal{R}_m(\theta_i) = \begin{pmatrix} \cos(m\theta_i) & -\sin(m\theta_i) \\ \sin(m\theta_i) & \cos(m\theta_i) \end{pmatrix}$$</div>
                    <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                        <span class="text-en">where Î¸_i = 10000^(-2i/d_model) and m is the position index.</span>
                        <span class="text-zh">å…¶ä¸­Î¸_i = 10000^(-2i/d_model)ï¼Œmæ˜¯ä½ç½®ç´¢å¼•ã€‚</span>
                    </p>
                </div>

                <h4>
                    <span class="text-en">RoPE Application</span>
                    <span class="text-zh">RoPEåº”ç”¨</span>
                </h4>

                <p>
                    <span class="text-en">For query and key vectors, RoPE applies rotations:</span>
                    <span class="text-zh">å¯¹äºæŸ¥è¯¢å’Œé”®å‘é‡ï¼ŒRoPEåº”ç”¨æ—‹è½¬ï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$q'_m = \mathcal{R}_m(\theta) q_m$$</div>
                    <div class="equation">$$k'_n = \mathcal{R}_n(\theta) k_n$$</div>
                </div>

                <p>
                    <span class="text-en">The attention score then becomes:</span>
                    <span class="text-zh">æ³¨æ„åŠ›åˆ†æ•°éšåå˜æˆï¼š</span>
                </p>

                <div class="math-box">
                    <div class="equation">$$q'_m \cdot k'_n = q_m \cdot \mathcal{R}_{n-m}(\theta) k_n$$</div>
                </div>

                <p>
                    <span class="text-en">Critically, the dot product depends only on the <span class="highlight">relative position</span> (n - m), not absolute positions. This enables:
                    <ul style="margin-top: 10px;">
                        <li><span class="text-en">Generalization to longer sequences during inference</span>
                            <span class="text-zh">åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—</span></li>
                        <li><span class="text-en">Natural handling of relative distance rather than absolute position</span>
                            <span class="text-zh">è‡ªç„¶å¤„ç†ç›¸å¯¹è·ç¦»è€Œä¸æ˜¯ç»å¯¹ä½ç½®</span></li>
                    </ul></span>
                    <span class="text-zh">å…³é”®çš„æ˜¯ï¼Œç‚¹ç§¯ä»…å–å†³äº<span class="highlight">ç›¸å¯¹ä½ç½®</span>ï¼ˆn - mï¼‰ï¼Œè€Œä¸æ˜¯ç»å¯¹ä½ç½®ã€‚è¿™å¯ç”¨ï¼š
                    <ul style="margin-top: 10px;">
                        <li>åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ³›åŒ–åˆ°æ›´é•¿çš„åºåˆ—</li>
                        <li>è‡ªç„¶å¤„ç†ç›¸å¯¹è·ç¦»è€Œä¸æ˜¯ç»å¯¹ä½ç½®</li>
                    </ul></span>
                </p>

                <h4>
                    <span class="text-en">RoPE vs. Sinusoidal PE</span>
                    <span class="text-zh">RoPEä¸æ­£å¼¦PEå¯¹æ¯”</span>
                </h4>

                <div class="math-box note">
                    <ul style="margin-top: 0; margin-bottom: 0;">
                        <li><span class="text-en"><span class="highlight">Sinusoidal</span>: Absolute position encoding, added to embeddings</span>
                            <span class="text-zh"><span class="highlight">æ­£å¼¦</span>ï¼šç»å¯¹ä½ç½®ç¼–ç ï¼Œæ·»åŠ åˆ°åµŒå…¥ä¸­</span></li>
                        <li><span class="text-en"><span class="highlight">RoPE</span>: Relative position information, applied via rotation during attention computation</span>
                            <span class="text-zh"><span class="highlight">RoPE</span>ï¼šç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œåœ¨æ³¨æ„åŠ›è®¡ç®—æœŸé—´é€šè¿‡æ—‹è½¬åº”ç”¨</span></li>
                        <li><span class="text-en"><span class="highlight">RoPE Advantage</span>: Better extrapolation to longer sequences, more explicit relative position encoding</span>
                            <span class="text-zh"><span class="highlight">RoPEä¼˜åŠ¿</span>ï¼šæ›´å¥½åœ°å¤–æ¨åˆ°æ›´é•¿çš„åºåˆ—ï¼Œæ›´æ˜ç¡®çš„ç›¸å¯¹ä½ç½®ç¼–ç </span></li>
                    </ul>
                </div>
            </div>

            <!-- SECTION 6: SELF VS CROSS ATTENTION -->
            <div class="section" id="self-vs-cross">
                <h2>
                    <span class="text-en">6. Self-Attention vs. Cross-Attention</span>
                    <span class="text-zh">6. è‡ªæ³¨æ„åŠ›ä¸äº¤å‰æ³¨æ„åŠ›</span>
                </h2>

                <p>
                    <span class="text-en">While the core attention mechanism is identical, self-attention and cross-attention differ in how Q, K, V are sourced, enabling different information flows.</span>
                    <span class="text-zh">è™½ç„¶æ ¸å¿ƒæ³¨æ„åŠ›æœºåˆ¶ç›¸åŒï¼Œä½†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›åœ¨Qã€Kã€Vçš„æºå¤´ä¸Šæœ‰æ‰€ä¸åŒï¼Œèƒ½å¤Ÿå®ç°ä¸åŒçš„ä¿¡æ¯æµã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Self-Attention</span>
                    <span class="text-zh">è‡ªæ³¨æ„åŠ›</span>
                </h3>

                <div class="math-box definition">
                    <p style="margin-top: 0; margin-bottom: 10px;">
                        <span class="text-en"><span class="highlight">Query, Key, and Value come from the same input</span>:</span>
                        <span class="text-zh"><span class="highlight">æŸ¥è¯¢ã€é”®å’Œå€¼æ¥è‡ªåŒä¸€è¾“å…¥</span>ï¼š</span>
                    </p>
                    <div class="equation">$$Q = H W^Q, \quad K = H W^K, \quad V = H W^V$$</div>
                    <div class="equation">$$\text{Attention}(H) = \text{softmax}\left(\frac{(HW^Q)(HW^K)^T}{\sqrt{d_k}}\right)HW^V$$</div>
                    <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                        <span class="text-en">where H âˆˆ â„^(nÃ—d_model) is the hidden state.</span>
                        <span class="text-zh">å…¶ä¸­H âˆˆ â„^(nÃ—d_model)æ˜¯éšè—çŠ¶æ€ã€‚</span>
                    </p>
                </div>

                <h4>
                    <span class="text-en">Information Flow</span>
                    <span class="text-zh">ä¿¡æ¯æµ</span>
                </h4>

                <p>
                    <span class="text-en">In self-attention, each position can attend to all other positions in the same sequence, enabling:</span>
                    <span class="text-zh">åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼Œæ¯ä¸ªä½ç½®å¯ä»¥å…³æ³¨åŒä¸€åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»–ä½ç½®ï¼Œä»è€Œå®ç°ï¼š</span>
                </p>
                <ul>
                    <li><span class="text-en">Contextual understanding within a single sequence</span>
                        <span class="text-zh">å•ä¸ªåºåˆ—å†…çš„ä¸Šä¸‹æ–‡ç†è§£</span></li>
                    <li><span class="text-en">Bidirectional context (in non-causal settings)</span>
                        <span class="text-zh">åŒå‘ä¸Šä¸‹æ–‡ï¼ˆåœ¨éå› æœè®¾ç½®ä¸­ï¼‰</span></li>
                    <li><span class="text-en">Direct inter-token relationships</span>
                        <span class="text-zh">ç›´æ¥çš„ä»¤ç‰Œé—´å…³ç³»</span></li>
                </ul>

                <h3>
                    <span class="text-en">Cross-Attention</span>
                    <span class="text-zh">äº¤å‰æ³¨æ„åŠ›</span>
                </h3>

                <div class="math-box definition">
                    <p style="margin-top: 0; margin-bottom: 10px;">
                        <span class="text-en"><span class="highlight">Query comes from one sequence, Key and Value from another</span>:</span>
                        <span class="text-zh"><span class="highlight">æŸ¥è¯¢æ¥è‡ªä¸€ä¸ªåºåˆ—ï¼Œé”®å’Œå€¼æ¥è‡ªå¦ä¸€ä¸ª</span>ï¼š</span>
                    </p>
                    <div class="equation">$$Q = H_{target} W^Q$$</div>
                    <div class="equation">$$K = H_{source} W^K, \quad V = H_{source} W^V$$</div>
                    <div class="equation">$$\text{CrossAttention}(H_{target}, H_{source}) = \text{softmax}\left(\frac{(H_{target}W^Q)(H_{source}W^K)^T}{\sqrt{d_k}}\right)H_{source}W^V$$</div>
                </div>

                <h4>
                    <span class="text-en">Information Flow</span>
                    <span class="text-zh">ä¿¡æ¯æµ</span>
                </h4>

                <p>
                    <span class="text-en">Cross-attention enables information transfer between two different sequences:</span>
                    <span class="text-zh">äº¤å‰æ³¨æ„åŠ›èƒ½å¤Ÿåœ¨ä¸¤ä¸ªä¸åŒçš„åºåˆ—ä¹‹é—´è¿›è¡Œä¿¡æ¯ä¼ è¾“ï¼š</span>
                </p>
                <ul>
                    <li><span class="text-en">Encoder-decoder architectures: Decoder attends to encoder outputs</span>
                        <span class="text-zh">ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼šè§£ç å™¨å…³æ³¨ç¼–ç å™¨è¾“å‡º</span></li>
                    <li><span class="text-en">Image-text models: Text tokens attend to image patches</span>
                        <span class="text-zh">å›¾åƒæ–‡æœ¬æ¨¡å‹ï¼šæ–‡æœ¬ä»¤ç‰Œå…³æ³¨å›¾åƒè¡¥ä¸</span></li>
                    <li><span class="text-en">Retrieval-augmented generation: Query attends to retrieved documents</span>
                        <span class="text-zh">æ£€ç´¢å¢å¼ºç”Ÿæˆï¼šæŸ¥è¯¢å…³æ³¨æ£€ç´¢åˆ°çš„æ–‡æ¡£</span></li>
                </ul>

                <h3>
                    <span class="text-en">Mathematical Distinction</span>
                    <span class="text-zh">æ•°å­¦åŒºåˆ†</span>
                </h3>

                <div class="math-box note">
                    <div style="margin-top: 0;">
                        <p style="margin-bottom: 8px;">
                            <span class="text-en"><span class="highlight">Self-Attention</span>: Symmetric dependency</span>
                            <span class="text-zh"><span class="highlight">è‡ªæ³¨æ„åŠ›</span>ï¼šå¯¹ç§°ä¾èµ–</span>
                        </p>
                        <div class="equation">$$\text{Attention}(H,H,H)$$</div>
                    </div>
                    <div style="margin-top: 15px;">
                        <p style="margin-bottom: 8px;">
                            <span class="text-en"><span class="highlight">Cross-Attention</span>: Asymmetric dependency</span>
                            <span class="text-zh"><span class="highlight">äº¤å‰æ³¨æ„åŠ›</span>ï¼šéå¯¹ç§°ä¾èµ–</span>
                        </p>
                        <div class="equation">$$\text{Attention}(H_1, H_2, H_2) \neq \text{Attention}(H_2, H_1, H_1)$$</div>
                    </div>
                </div>

                <p>
                    <span class="text-en">This asymmetry allows the model to explicitly control which sequence provides context and which sequence is being updated.</span>
                    <span class="text-zh">è¿™ç§ä¸å¯¹ç§°å…è®¸æ¨¡å‹æ˜ç¡®æ§åˆ¶å“ªä¸ªåºåˆ—æä¾›ä¸Šä¸‹æ–‡ï¼Œå“ªä¸ªåºåˆ—æ­£åœ¨è¢«æ›´æ–°ã€‚</span>
                </p>
            </div>

            <!-- SECTION 7: COMPLEXITY ANALYSIS -->
            <div class="section" id="complexity">
                <h2>
                    <span class="text-en">7. Computational Complexity Analysis</span>
                    <span class="text-zh">7. è®¡ç®—å¤æ‚æ€§åˆ†æ</span>
                </h2>

                <p>
                    <span class="text-en">Understanding the computational cost of attention is critical for designing efficient models, especially for long sequences.</span>
                    <span class="text-zh">ç†è§£æ³¨æ„åŠ›çš„è®¡ç®—æˆæœ¬å¯¹äºè®¾è®¡é«˜æ•ˆæ¨¡å‹è‡³å…³é‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºé•¿åºåˆ—ã€‚</span>
                </p>

                <h3>
                    <span class="text-en">Standard Attention Complexity</span>
                    <span class="text-zh">æ ‡å‡†æ³¨æ„åŠ›å¤æ‚æ€§</span>
                </h3>

                <div class="math-box definition">
                    <p style="margin-top: 0; margin-bottom: 10px;">
                        <span class="text-en"><span class="highlight">Time Complexity: O(nÂ²d_k)</span></span>
                        <span class="text-zh"><span class="highlight">æ—¶é—´å¤æ‚æ€§: O(nÂ²d_k)</span></span>
                    </p>
                    <div class="equation">$$QK^T: \quad O(n \times d_k \times n) = O(n^2 d_k)$$</div>
                    <div class="equation">$$\text{softmax}(QK^T) \cdot V: \quad O(n \times n \times d_v) = O(n^2 d_v)$$</div>
                    <div class="equation">$$\text{Total}: \quad O(n^2 d_k) + O(n^2 d_v) = O(n^2 d_{model})$$</div>
                </div>

                <h4>
                    <span class="text-en">Memory Complexity</span>
                    <span class="text-zh">å†…å­˜å¤æ‚æ€§</span>
                </h4>

                <div class="math-box">
                    <p style="margin-top: 0; margin-bottom: 10px;">
                        <span class="text-en"><span class="highlight">Space Complexity: O(nÂ² + nd_model)</span></span>
                        <span class="text-zh"><span class="highlight">ç©ºé—´å¤æ‚æ€§: O(nÂ² + nd_model)</span></span>
                    </p>
                    <ul style="margin-bottom: 0;">
                        <li><span class="text-en">Attention matrix A: O(nÂ²)</span>
                            <span class="text-zh">æ³¨æ„åŠ›çŸ©é˜µA: O(nÂ²)</span></li>
                        <li><span class="text-en">Input/Output: O(nd_model)</span>
                            <span class="text-zh">è¾“å…¥/è¾“å‡º: O(nd_model)</span></li>
                        <li><span class="text-en">Intermediate: O(nÂ²) dominates for long sequences</span>
                            <span class="text-zh">ä¸­é—´ç»“æœ: O(nÂ²)å¯¹äºé•¿åºåˆ—å ä¸»å¯¼</span></li>
                    </ul>
                </div>

                <h3>
                    <span class="text-en">Why O(nÂ²) is Problematic</span>
                    <span class="text-zh">ä¸ºä»€ä¹ˆO(nÂ²)æ˜¯é—®é¢˜</span>
                </h3>

                <p>
                    <span class="text-en">For sequence length n = 4096 with d_model = 512:</span>
                    <span class="text-zh">å¯¹äºåºåˆ—é•¿åº¦n = 4096ï¼Œd_model = 512ï¼š</span>
                </p>

                <div class="math-box note">
                    <ul style="margin-top: 0; margin-bottom: 0;">
                        <li><span class="text-en">Attention matrix: 4096Â² Ã— 2 bytes (float16) = 64 MB per head</span>
                            <span class="text-zh">æ³¨æ„åŠ›çŸ©é˜µ: 4096Â² Ã— 2å­—èŠ‚ (float16) = æ¯ä¸ªå¤´64 MB</span></li>
                        <li><span class="text-en">For 12 heads: 768 MB just for attention weights</span>
                            <span class="text-zh">å¯¹äº12ä¸ªå¤´: ä»…æ³¨æ„åŠ›æƒé‡å°±éœ€è¦768 MB</span></li>
                        <li><span class="text-en">With batch size 16: ~12 GB memory for attention alone</span>
                            <span class="text-zh">æ‰¹å¤§å°ä¸º16: ä»…æ³¨æ„åŠ›å°±éœ€è¦~12 GBå†…å­˜</span></li>
                    </ul>
                </div>

                <h3>
                    <span class="text-en">Approaches to Reduce Complexity</span>
                    <span class="text-zh">é™ä½å¤æ‚æ€§çš„æ–¹æ³•</span>
                </h3>

                <h4>
                    <span class="text-en">1. Sparse Attention</span>
                    <span class="text-zh">1. ç¨€ç–æ³¨æ„åŠ›</span>
                </h4>

                <div class="derivation-step">
                    <p>
                        <span class="text-en">Restrict attention to local windows instead of all pairs:</span>
                        <span class="text-zh">å°†æ³¨æ„åŠ›é™åˆ¶åœ¨å±€éƒ¨çª—å£è€Œä¸æ˜¯æ‰€æœ‰å¯¹ï¼š</span>
                    </p>
                    <div class="math-box">
                        <div class="equation">$$\text{Complexity}: \quad O(n \cdot w \cdot d_k)$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">where w is the local window size (e.g., 64 or 128). For w â‰ª n, this reduces complexity to linear.</span>
                            <span class="text-zh">å…¶ä¸­wæ˜¯å±€éƒ¨çª—å£å¤§å°ï¼ˆä¾‹å¦‚64æˆ–128ï¼‰ã€‚å¯¹äºw â‰ª nï¼Œè¿™å°†å¤æ‚æ€§é™ä½åˆ°çº¿æ€§ã€‚</span>
                        </p>
                    </div>
                    <p style="margin-top: 10px;">
                        <span class="text-en"><span class="highlight">Examples</span>: Longformer, BigBird</span>
                        <span class="text-zh"><span class="highlight">ç¤ºä¾‹</span>: Longformerã€BigBird</span>
                    </p>
                </div>

                <h4>
                    <span class="text-en">2. Low-Rank Approximation</span>
                    <span class="text-zh">2. ä½ç§©è¿‘ä¼¼</span>
                </h4>

                <div class="derivation-step">
                    <p>
                        <span class="text-en">Approximate attention matrix using low-rank decomposition:</span>
                        <span class="text-zh">ä½¿ç”¨ä½ç§©åˆ†è§£è¿‘ä¼¼æ³¨æ„åŠ›çŸ©é˜µï¼š</span>
                    </p>
                    <div class="math-box">
                        <div class="equation">$$A \approx U V^T$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">where U, V âˆˆ â„^(nÃ—r) with r â‰ª n.</span>
                            <span class="text-zh">å…¶ä¸­U, V âˆˆ â„^(nÃ—r)ï¼Œr â‰ª nã€‚</span>
                        </p>
                    </div>
                    <p style="margin-top: 10px;">
                        <span class="text-en"><span class="highlight">Complexity</span>: O(nÂ·rÂ·d_k) where r is the rank</span>
                        <span class="text-zh"><span class="highlight">å¤æ‚æ€§</span>: O(nÂ·rÂ·d_k)ï¼Œå…¶ä¸­ræ˜¯ç§©</span>
                    </p>
                </div>

                <h4>
                    <span class="text-en">3. Linear Attention / Kernel Methods</span>
                    <span class="text-zh">3. çº¿æ€§æ³¨æ„åŠ›/æ ¸æ–¹æ³•</span>
                </h4>

                <div class="derivation-step">
                    <p>
                        <span class="text-en">Use feature maps to compute attention implicitly:</span>
                        <span class="text-zh">ä½¿ç”¨ç‰¹å¾æ˜ å°„éšå¼è®¡ç®—æ³¨æ„åŠ›ï¼š</span>
                    </p>
                    <div class="math-box">
                        <div class="equation">$$\text{Attention}(Q,K,V) \approx \frac{(\phi(Q)\phi(K)^T)V}{\phi(Q)(\phi(K)^T \mathbf{1})}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">where Ï† is a feature mapping (e.g., ELU, kernel function).</span>
                            <span class="text-zh">å…¶ä¸­Ï†æ˜¯ç‰¹å¾æ˜ å°„ï¼ˆä¾‹å¦‚ELUã€æ ¸å‡½æ•°ï¼‰ã€‚</span>
                        </p>
                    </div>
                    <p style="margin-top: 10px;">
                        <span class="text-en"><span class="highlight">Complexity</span>: O(nÂ·d_kÂ·d_v)</span>
                        <span class="text-zh"><span class="highlight">å¤æ‚æ€§</span>: O(nÂ·d_kÂ·d_v)</span>
                    </p>
                    <p>
                        <span class="text-en"><span class="highlight">Examples</span>: Performer, Linear Transformer</span>
                        <span class="text-zh"><span class="highlight">ç¤ºä¾‹</span>: Performerã€Linear Transformer</span>
                    </p>
                </div>

                <h4>
                    <span class="text-en">4. Flash Attention (Algorithmic Optimization)</span>
                    <span class="text-zh">4. Flash Attentionï¼ˆç®—æ³•ä¼˜åŒ–ï¼‰</span>
                </h4>

                <div class="derivation-step">
                    <p>
                        <span class="text-en">Keep attention computation in GPU fast memory (SRAM) instead of global memory:</span>
                        <span class="text-zh">å°†æ³¨æ„åŠ›è®¡ç®—ä¿ç•™åœ¨GPUå¿«é€Ÿå†…å­˜ï¼ˆSRAMï¼‰ä¸­ï¼Œè€Œä¸æ˜¯å…¨å±€å†…å­˜ä¸­ï¼š</span>
                    </p>
                    <div class="math-box">
                        <div class="equation">$$\text{FLOPS}: \quad O(n^2 d_k) \quad \text{(unchanged)}$$</div>
                        <div class="equation">$$\text{Memory Access}: \quad \text{2-5x reduction}$$</div>
                        <p style="margin-bottom: 0; margin-top: 10px; font-size: 0.95rem;">
                            <span class="text-en">By optimizing I/O patterns, Flash Attention achieves actual 2-3x speedup on modern GPUs without changing theoretical complexity.</span>
                            <span class="text-zh">é€šè¿‡ä¼˜åŒ–I/Oæ¨¡å¼ï¼ŒFlash Attentionåœ¨ç°ä»£GPUä¸Šå®ç°2-3å€çš„å®é™…åŠ é€Ÿï¼Œè€Œä¸æ”¹å˜ç†è®ºå¤æ‚æ€§ã€‚</span>
                        </p>
                    </div>
                </div>

                <h3>
                    <span class="text-en">Complexity Comparison Table</span>
                    <span class="text-zh">å¤æ‚æ€§æ¯”è¾ƒè¡¨</span>
                </h3>

                <div class="math-box note" style="overflow-x: auto;">
                    <table style="width: 100%; border-collapse: collapse; font-size: 0.9rem;">
                        <tr style="border-bottom: 1px solid #ccc;">
                            <td style="padding: 8px; text-align: left;"><span class="text-en">Method</span><span class="text-zh">æ–¹æ³•</span></td>
                            <td style="padding: 8px; text-align: center;"><span class="text-en">Time</span><span class="text-zh">æ—¶é—´</span></td>
                            <td style="padding: 8px; text-align: center;"><span class="text-en">Space</span><span class="text-zh">ç©ºé—´</span></td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 8px;"><span class="text-en">Standard</span><span class="text-zh">æ ‡å‡†</span></td>
                            <td style="padding: 8px; text-align: center;">O(nÂ²d)</td>
                            <td style="padding: 8px; text-align: center;">O(nÂ²)</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 8px;"><span class="text-en">Sparse (local)</span><span class="text-zh">ç¨€ç–ï¼ˆå±€éƒ¨ï¼‰</span></td>
                            <td style="padding: 8px; text-align: center;">O(nÂ·wÂ·d)</td>
                            <td style="padding: 8px; text-align: center;">O(nÂ·w)</td>
                        </tr>
                        <tr style="border-bottom: 1px solid #ddd;">
                            <td style="padding: 8px;"><span class="text-en">Linear</span><span class="text-zh">çº¿æ€§</span></td>
                            <td style="padding: 8px; text-align: center;">O(nÂ·dÂ²)</td>
                            <td style="padding: 8px; text-align: center;">O(nÂ·d)</td>
                        </tr>
                        <tr>
                            <td style="padding: 8px;"><span class="text-en">Flash (I/O)</span><span class="text-zh">Flashï¼ˆI/Oï¼‰</span></td>
                            <td style="padding: 8px; text-align: center;">O(nÂ²d)*</td>
                            <td style="padding: 8px; text-align: center;">O(nÂ² + d)*</td>
                        </tr>
                    </table>
                    <p style="margin: 10px 0 0 0; font-size: 0.85rem;">
                        <span class="text-en">* Same theoretical complexity, but 2-3x faster in practice due to optimized I/O.</span>
                        <span class="text-zh">* ç›¸åŒçš„ç†è®ºå¤æ‚æ€§ï¼Œä½†ç”±äºä¼˜åŒ–çš„I/Oåœ¨å®è·µä¸­å¿«2-3å€ã€‚</span>
                    </p>
                </div>
            </div>

            <!-- FOOTER -->
            <div class="footer">
                <p>
                    <span class="text-en">Advanced mathematics guide for transformer architectures. For implementation details, consult the Attention Is All You Need paper and subsequent research.</span>
                    <span class="text-zh">Transformeræ¶æ„çš„é«˜çº§æ•°å­¦æŒ‡å—ã€‚æœ‰å…³å®ç°ç»†èŠ‚ï¼Œè¯·å‚è€ƒã€ŠAttention Is All You Needã€‹è®ºæ–‡å’Œéšåçš„ç ”ç©¶ã€‚</span>
                </p>
            </div>

        </div>
    </div>

    <script>
        function toggleLanguage() {
            const html = document.documentElement;
            const currentLang = html.getAttribute('data-lang');
            const body = document.body;

            if (currentLang === 'en') {
                html.setAttribute('data-lang', 'zh');
                body.classList.remove('en');
                body.classList.add('zh');
            } else {
                html.setAttribute('data-lang', 'en');
                body.classList.remove('zh');
                body.classList.add('en');
            }
        }
    </script>
</body>
</html>
