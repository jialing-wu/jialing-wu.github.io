<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Logistic Regression from First Principles | Machine Learning</title>

    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400;1,500&family=EB+Garamond:ital,wght@0,400;0,500;1,400&family=Caveat:wght@400;500&family=IBM+Plex+Sans:wght@300;400;500&family=IBM+Plex+Mono:wght@400;500&family=Noto+Serif+SC:wght@300;400;500;600&family=Noto+Sans+SC:wght@300;400;500&display=swap" rel="stylesheet">

    <!-- Base Styles -->
    <link rel="stylesheet" href="../style.css">

    <!-- Guide Styles -->
    <link rel="stylesheet" href="guide-style.css">

    <!-- MathJax for Mathematics -->
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        /* Override: text paragraphs inside math-box should not inherit monospace */
        .math-box p, .math-box .step-text {
            font-family: var(--body, "EB Garamond", serif);
            font-size: 15px;
            line-height: 1.7;
            color: var(--ink-faded);
            margin-bottom: 10px;
        }
        /* Step label style */
        .step-label {
            font-family: var(--sans);
            font-size: 11px;
            font-weight: 700;
            letter-spacing: 0.1em;
            text-transform: uppercase;
            color: var(--gold);
            margin-bottom: 4px;
        }
        /* Divider between steps within a derivation block */
        .step-block {
            margin-bottom: 18px;
            padding-bottom: 14px;
            border-bottom: 1px solid rgba(194,153,61,.2);
        }
        .step-block:last-child { border-bottom: none; margin-bottom: 0; padding-bottom: 0; }
    </style>
</head>

<body class="en">
    <div class="guide-layout">
        <!-- Topbar -->
        <div class="guide-topbar">
            <div class="guide-breadcrumb">
                <a href="../machine-learning.html" data-lang="en">Machine Learning</a>
                <a href="../machine-learning.html" data-lang="zh">æœºå™¨å­¦ä¹ </a>
                <span class="sep">/</span>
                <span data-lang="en">Logistic Regression from First Principles</span>
                <span data-lang="zh">ä»ç¬¬ä¸€æ€§åŸç†ç†è§£é€»è¾‘å›å½’</span>
            </div>
            <div class="guide-lang-toggle">
                <button class="guide-lang-btn active" data-lang="zh">ä¸­æ–‡</button>
                <button class="guide-lang-btn" data-lang="en">EN</button>
            </div>
        </div>

        <!-- Content -->
        <div class="guide-content-wrapper">
            <div class="guide-content">
                <!-- Back link -->
                <a href="../machine-learning.html#ml-classification" class="guide-back" data-lang="en">â† Back to Machine Learning: Classification</a>
                <a href="../machine-learning.html#ml-classification" class="guide-back" data-lang="zh">â† è¿”å›æœºå™¨å­¦ä¹ ï¼šåˆ†ç±»</a>

                <!-- Page header -->
                <div class="guide-header">
                    <div class="guide-tag">
                        <span data-lang="en">DEEP DIVE Â· MODULE 1b Â· CLASSIFICATION</span>
                        <span data-lang="zh">æ·±å…¥æŒ‡å— Â· æ¨¡å— 1b Â· åˆ†ç±»</span>
                    </div>
                    <h1>
                        <span data-lang="en">Logistic Regression from First Principles</span>
                        <span data-lang="zh">ä»ç¬¬ä¸€æ€§åŸç†ç†è§£é€»è¾‘å›å½’</span>
                    </h1>
                    <p>
                        <span data-lang="en">Why OLS fails for binary outcomes, how the sigmoid squashes any number into a probability, and what cross-entropy loss is really measuring</span>
                        <span data-lang="zh">ä¸ºä»€ä¹ˆ OLS æ— æ³•å¤„ç†äºŒå…ƒç»“æœã€sigmoid å‡½æ•°å¦‚ä½•å°†ä»»æ„æ•°å€¼å‹ç¼©ä¸ºæ¦‚ç‡ï¼Œä»¥åŠäº¤å‰ç†µæŸå¤±çš„çœŸæ­£å«ä¹‰</span>
                    </p>
                </div>

                <!-- ========== SECTION 1 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">1. Why Not Just Use OLS for Binary Outcomes?</span>
                        <span data-lang="zh">1. ä¸ºä»€ä¹ˆä¸ç›´æ¥å¯¹äºŒå…ƒç»“æœç”¨ OLSï¼Ÿ</span>
                    </h2>

                    <p data-lang="en">
                        Imagine you are predicting whether a country experienced democratic backsliding in a given year â€” the outcome is either 1 (yes) or 0 (no). A natural instinct is to run OLS: regress the binary outcome on GDP, inequality, and political history. This is called the <span class="highlight-key">Linear Probability Model (LPM)</span>, and it can work tolerably in some settings. But it has two fundamental problems.
                    </p>
                    <p data-lang="zh">
                        å‡è®¾ä½ è¦é¢„æµ‹æŸå›½æŸå¹´æ˜¯å¦å‘ç”Ÿäº†æ°‘ä¸»å€’é€€â€”â€”ç»“æœé 1ï¼ˆæ˜¯ï¼‰å³ 0ï¼ˆå¦ï¼‰ã€‚ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯ç”¨ OLSï¼šå°†äºŒå…ƒç»“æœå¯¹ GDPã€ä¸å¹³ç­‰ç¨‹åº¦å’Œæ”¿æ²»å†å²å›å½’ã€‚è¿™å«åš<span class="highlight-key">çº¿æ€§æ¦‚ç‡æ¨¡å‹ï¼ˆLPMï¼‰</span>ï¼Œåœ¨æŸäº›åœºæ™¯ä¸‹å‹‰å¼ºå¯ç”¨ã€‚ä½†å®ƒæœ‰ä¸¤ä¸ªæ ¹æœ¬æ€§é—®é¢˜ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Problem 1 â€” Predictions outside [0, 1]</span>
                        <span data-lang="zh">é—®é¢˜ 1 â€” é¢„æµ‹å€¼è¶…å‡º [0, 1]</span>
                    </h3>
                    <p data-lang="en">
                        OLS has no guardrails. For very rich or very stable countries, OLS might predict a backsliding probability of âˆ’0.3; for very fragile states it might predict 1.4. Probabilities below zero or above one are mathematically meaningless. You cannot interpret "âˆ’30% chance of backsliding."
                    </p>
                    <p data-lang="zh">
                        OLS æ²¡æœ‰ä»»ä½•çº¦æŸã€‚å¯¹äºéå¸¸å¯Œè£•æˆ–ç¨³å®šçš„å›½å®¶ï¼ŒOLS å¯èƒ½é¢„æµ‹æ°‘ä¸»å€’é€€çš„æ¦‚ç‡ä¸º âˆ’0.3ï¼›å¯¹äºæåº¦è„†å¼±çš„å›½å®¶åˆ™å¯èƒ½é¢„æµ‹ 1.4ã€‚å°äºé›¶æˆ–å¤§äºä¸€çš„æ¦‚ç‡åœ¨æ•°å­¦ä¸Šæ¯«æ— æ„ä¹‰ã€‚ä½ æ— æ³•è§£é‡Š"å‘ç”Ÿå€’é€€çš„æ¦‚ç‡ä¸º âˆ’30%"ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Problem 2 â€” Heteroskedasticity is baked in</span>
                        <span data-lang="zh">é—®é¢˜ 2 â€” å¼‚æ–¹å·®æ€§å¤©ç„¶å­˜åœ¨</span>
                    </h3>
                    <p data-lang="en">
                        For a binary outcome y, the variance of the error is p(1âˆ’p), where p = P(y=1). Since p varies across observations, the error variance is never constant â€” OLS standard errors are wrong by construction. Inference (t-tests, p-values, confidence intervals) is unreliable.
                    </p>
                    <p data-lang="zh">
                        å¯¹äºäºŒå…ƒç»“æœ yï¼Œè¯¯å·®çš„æ–¹å·®ä¸º p(1âˆ’p)ï¼Œå…¶ä¸­ p = P(y=1)ã€‚ç”±äº p éšè§‚æµ‹å€¼å˜åŒ–ï¼Œè¯¯å·®æ–¹å·®æ°¸è¿œä¸æ˜¯å¸¸æ•°â€”â€”OLS æ ‡å‡†è¯¯ä»ä¸€å¼€å§‹å°±æ˜¯é”™çš„ã€‚æ¨æ–­ï¼ˆt æ£€éªŒã€p å€¼ã€ç½®ä¿¡åŒºé—´ï¼‰ä¹Ÿéšä¹‹å¤±æ•ˆã€‚
                    </p>

                    <div class="definition-box">
                        <span data-lang="en"><strong>The fix:</strong> Instead of modelling E[y] = XÎ¸ directly, model the log-odds log(p/1âˆ’p) = XÎ¸. This keeps predictions bounded in (0,1) and correctly handles the binary distribution. That is logistic regression.</span>
                        <span data-lang="zh"><strong>è§£å†³æ–¹æ¡ˆï¼š</strong>ä¸ç›´æ¥å»ºæ¨¡ E[y] = XÎ¸ï¼Œè€Œæ˜¯å»ºæ¨¡ log ä¼˜åŠ¿æ¯” log(p/1âˆ’p) = XÎ¸ã€‚è¿™ä½¿é¢„æµ‹å€¼è¢«çº¦æŸåœ¨ (0,1) ä¹‹é—´ï¼Œå¹¶æ­£ç¡®å¤„ç†äºŒå…ƒåˆ†å¸ƒã€‚è¿™å°±æ˜¯é€»è¾‘å›å½’ã€‚</span>
                    </div>
                </div>

                <!-- ========== SECTION 2 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">2. The Sigmoid Function: Squashing Any Number into a Probability</span>
                        <span data-lang="zh">2. Sigmoid å‡½æ•°ï¼šå°†ä»»æ„æ•°å€¼å‹ç¼©ä¸ºæ¦‚ç‡</span>
                    </h2>

                    <p data-lang="en">
                        The core trick in logistic regression is the <span class="highlight-key">sigmoid function</span> Ïƒ(z), also called the logistic function. It takes any real number z â€” positive or negative, large or small â€” and maps it to the interval (0, 1).
                    </p>
                    <p data-lang="zh">
                        é€»è¾‘å›å½’çš„æ ¸å¿ƒæŠ€å·§æ˜¯ <span class="highlight-key">sigmoid å‡½æ•°</span> Ïƒ(z)ï¼Œä¹Ÿç§° logistic å‡½æ•°ã€‚å®ƒå°†ä»»æ„å®æ•° zï¼ˆæ— è®ºæ­£è´Ÿå¤§å°ï¼‰æ˜ å°„åˆ°åŒºé—´ (0, 1)ã€‚
                    </p>

                    <div class="math-box">
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$
                    </div>

                    <p data-lang="en">
                        Think of z = XÎ¸ as a raw "backsliding score" computed from country characteristics. A very negative z (stable, wealthy, long-democratic country) â†’ Ïƒ(z) â‰ˆ 0. A very positive z (fragile, unequal, short history of democracy) â†’ Ïƒ(z) â‰ˆ 1. z = 0 â†’ Ïƒ(z) = 0.5, the 50/50 tipping point.
                    </p>
                    <p data-lang="zh">
                        æŠŠ z = XÎ¸ æƒ³è±¡æˆæ ¹æ®å›½å®¶ç‰¹å¾è®¡ç®—å‡ºçš„åŸå§‹"å€’é€€å¾—åˆ†"ã€‚z æåº¦ä¸ºè´Ÿï¼ˆç¨³å®šã€å¯Œè£•ã€é•¿æœŸæ°‘ä¸»åŒ–å›½å®¶ï¼‰â†’ Ïƒ(z) â‰ˆ 0ã€‚z æåº¦ä¸ºæ­£ï¼ˆè„†å¼±ã€ä¸å¹³ç­‰ã€æ°‘ä¸»å†å²çŸ­çš„å›½å®¶ï¼‰â†’ Ïƒ(z) â‰ˆ 1ã€‚z = 0 â†’ Ïƒ(z) = 0.5ï¼Œå³ 50/50 çš„ä¸´ç•Œç‚¹ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Where does the sigmoid come from?</span>
                        <span data-lang="zh">Sigmoid å‡½æ•°ä»ä½•è€Œæ¥ï¼Ÿ</span>
                    </h3>
                    <p data-lang="en">
                        Start with the <span class="highlight-key">odds</span> of an event: odds = p / (1âˆ’p). If p = 0.75, odds = 3 (three-to-one in favour). The log-odds â€” called the <span class="highlight-key">logit</span> â€” is:
                    </p>
                    <p data-lang="zh">
                        ä»äº‹ä»¶çš„<span class="highlight-key">ä¼˜åŠ¿æ¯”ï¼ˆoddsï¼‰</span>å‡ºå‘ï¼šodds = p / (1âˆ’p)ã€‚è‹¥ p = 0.75ï¼Œåˆ™ odds = 3ï¼ˆä¸‰æ¯”ä¸€ï¼‰ã€‚å¯¹æ•°ä¼˜åŠ¿æ¯”â€”â€”ç§°ä¸º <span class="highlight-key">logit</span>â€”â€”ä¸ºï¼š
                    </p>

                    <div class="math-box">
$$\text{logit}(p) = \log\frac{p}{1-p} = z = x^\top\theta$$
                    </div>

                    <p data-lang="en">
                        Solving this equation for p gives the sigmoid: p = 1/(1+eâ»á¶»). So logistic regression models the log-odds as a linear function of features, which is equivalent to modelling the probability through a sigmoid.
                    </p>
                    <p data-lang="zh">
                        å¯¹ p æ±‚è§£è¿™ä¸ªæ–¹ç¨‹ï¼Œå°±å¾—åˆ° sigmoidï¼šp = 1/(1+eâ»á¶»)ã€‚å› æ­¤ï¼Œé€»è¾‘å›å½’å°†å¯¹æ•°ä¼˜åŠ¿æ¯”å»ºæ¨¡ä¸ºç‰¹å¾çš„çº¿æ€§å‡½æ•°ï¼Œç­‰ä»·äºé€šè¿‡ sigmoid å‡½æ•°å»ºæ¨¡æ¦‚ç‡ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">A useful derivative</span>
                        <span data-lang="zh">ä¸€ä¸ªæœ‰ç”¨çš„å¯¼æ•°æ€§è´¨</span>
                    </h3>
                    <p data-lang="en">
                        The sigmoid has an elegant self-referential derivative that simplifies calculus enormously:
                    </p>
                    <p data-lang="zh">
                        Sigmoid å‡½æ•°çš„å¯¼æ•°æœ‰ä¸€ä¸ªä¼˜ç¾çš„è‡ªå¼•ç”¨æ€§è´¨ï¼Œæå¤§ç®€åŒ–äº†å¾®ç§¯åˆ†æ¨å¯¼ï¼š
                    </p>

                    <div class="math-box">
$$\sigma'(z) = \sigma(z)\bigl(1 - \sigma(z)\bigr)$$
                    </div>

                    <p data-lang="en">
                        In plain English: the rate of change of Ïƒ(z) depends on where you are on the curve â€” steep in the middle (near z=0), flat at the extremes. This matters when we compute gradients for optimization.
                    </p>
                    <p data-lang="zh">
                        ç›´ç™½åœ°è¯´ï¼šÏƒ(z) çš„å˜åŒ–é€Ÿç‡å–å†³äºä½ åœ¨æ›²çº¿ä¸Šçš„ä½ç½®â€”â€”åœ¨ä¸­é—´ï¼ˆzâ‰ˆ0ï¼‰é™„è¿‘å˜åŒ–æœ€é™¡ï¼Œåœ¨ä¸¤ç«¯è¶‹äºå¹³ç¼“ã€‚è¿™åœ¨è®¡ç®—ä¼˜åŒ–æ‰€éœ€çš„æ¢¯åº¦æ—¶éå¸¸é‡è¦ã€‚
                    </p>
                </div>

                <!-- ========== SECTION 3 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">3. The Cost Function: Cross-Entropy Loss</span>
                        <span data-lang="zh">3. æˆæœ¬å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤±</span>
                    </h2>

                    <p data-lang="en">
                        In linear regression, we minimized squared error. Can we do the same for logistic regression? No â€” and here is why.
                    </p>
                    <p data-lang="zh">
                        åœ¨çº¿æ€§å›å½’ä¸­ï¼Œæˆ‘ä»¬æœ€å°åŒ–å¹³æ–¹è¯¯å·®ã€‚é€»è¾‘å›å½’èƒ½åšåŒæ ·çš„äº‹å—ï¼Ÿä¸èƒ½â€”â€”åŸå› å¦‚ä¸‹ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Why squared error fails for classification</span>
                        <span data-lang="zh">ä¸ºä»€ä¹ˆå¹³æ–¹è¯¯å·®ä¸é€‚ç”¨äºåˆ†ç±»</span>
                    </h3>
                    <p data-lang="en">
                        The composition of a sigmoid with a squared error creates a <span class="highlight-key">non-convex</span> cost function â€” full of local minima. Gradient descent would get stuck and never reliably find the global optimum. We need a different loss that is convex over Î¸.
                    </p>
                    <p data-lang="zh">
                        å°† sigmoid ä¸å¹³æ–¹è¯¯å·®ç»„åˆä¼šäº§ç”Ÿä¸€ä¸ª<span class="highlight-key">éå‡¸</span>æˆæœ¬å‡½æ•°â€”â€”å……æ»¡å±€éƒ¨æœ€å°å€¼ã€‚æ¢¯åº¦ä¸‹é™ä¼šé™·å…¥å±€éƒ¨æœ€å°å€¼ï¼Œæ— æ³•å¯é åœ°æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¯¹ Î¸ æ¥è¯´æ˜¯å‡¸å‡½æ•°çš„æŸå¤±ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Deriving the cost from maximum likelihood</span>
                        <span data-lang="zh">ä»æœ€å¤§ä¼¼ç„¶æ¨å¯¼æˆæœ¬å‡½æ•°</span>
                    </h3>
                    <p data-lang="en">
                        For a binary outcome, each observation follows a Bernoulli distribution: P(yáµ¢ | xáµ¢, Î¸) = páµ¢^yáµ¢ Â· (1âˆ’páµ¢)^(1âˆ’yáµ¢), where páµ¢ = Ïƒ(xáµ¢áµ€Î¸). The likelihood of the entire dataset (assuming independent observations) is the product of these terms. Taking the log and flipping the sign gives the <span class="highlight-key">negative log-likelihood</span>, which we minimize:
                    </p>
                    <p data-lang="zh">
                        å¯¹äºäºŒå…ƒç»“æœï¼Œæ¯ä¸ªè§‚æµ‹æœä»ä¼¯åŠªåˆ©åˆ†å¸ƒï¼šP(yáµ¢ | xáµ¢, Î¸) = páµ¢^yáµ¢ Â· (1âˆ’páµ¢)^(1âˆ’yáµ¢)ï¼Œå…¶ä¸­ páµ¢ = Ïƒ(xáµ¢áµ€Î¸)ã€‚æ•´ä¸ªæ•°æ®é›†çš„ä¼¼ç„¶ï¼ˆå‡è®¾è§‚æµ‹ç‹¬ç«‹ï¼‰æ˜¯è¿™äº›é¡¹çš„ä¹˜ç§¯ã€‚å–å¯¹æ•°å¹¶ç¿»è½¬ç¬¦å·ï¼Œå¾—åˆ°<span class="highlight-key">è´Ÿå¯¹æ•°ä¼¼ç„¶</span>ï¼Œå³æˆ‘ä»¬è¦æœ€å°åŒ–çš„ï¼š
                    </p>

                    <div class="math-box">
$$J(\theta) = -\frac{1}{n}\sum_{i=1}^{n}\Bigl[y_i\log\hat{p}_i + (1-y_i)\log(1-\hat{p}_i)\Bigr]$$
                    </div>

                    <p data-lang="en">
                        This is the <span class="highlight-key">binary cross-entropy loss</span>. It has an elegant interpretation: when yáµ¢ = 1 (backsliding occurred), only the first term matters â€” we are penalized by âˆ’log(pÌ‚áµ¢). If the model predicts pÌ‚áµ¢ = 0.99, the penalty is tiny (âˆ’log(0.99) â‰ˆ 0.01). If the model predicts pÌ‚áµ¢ = 0.01 for a country that actually backslid, the penalty explodes (âˆ’log(0.01) â‰ˆ 4.6). Cross-entropy punishes confident wrong predictions harshly.
                    </p>
                    <p data-lang="zh">
                        è¿™å°±æ˜¯<span class="highlight-key">äºŒå…ƒäº¤å‰ç†µæŸå¤±</span>ã€‚å®ƒæœ‰ä¸€ä¸ªä¼˜ç¾çš„è§£é‡Šï¼šå½“ yáµ¢ = 1ï¼ˆå‘ç”Ÿäº†å€’é€€ï¼‰æ—¶ï¼Œåªæœ‰ç¬¬ä¸€é¡¹æœ‰æ•ˆâ€”â€”æƒ©ç½šä¸º âˆ’log(pÌ‚áµ¢)ã€‚å¦‚æœæ¨¡å‹é¢„æµ‹ pÌ‚áµ¢ = 0.99ï¼Œæƒ©ç½šå¾ˆå°ï¼ˆâˆ’log(0.99) â‰ˆ 0.01ï¼‰ã€‚å¦‚æœæ¨¡å‹å¯¹ä¸€ä¸ªå®é™…å‘ç”Ÿå€’é€€çš„å›½å®¶é¢„æµ‹ pÌ‚áµ¢ = 0.01ï¼Œæƒ©ç½šåˆ™çˆ†ç‚¸å¼å¢é•¿ï¼ˆâˆ’log(0.01) â‰ˆ 4.6ï¼‰ã€‚äº¤å‰ç†µå¯¹è‡ªä¿¡ä½†é”™è¯¯çš„é¢„æµ‹ç»™äºˆä¸¥å‰æƒ©ç½šã€‚
                    </p>

                    <div class="definition-box">
                        <span data-lang="en"><strong>Probabilistic connection:</strong> Minimizing cross-entropy is exactly equivalent to finding the Maximum Likelihood Estimate (MLE) of Î¸. There is no "arbitrary" choice of loss function here â€” cross-entropy is what the probability model itself demands. It is also a convex function in Î¸, guaranteeing that gradient descent finds the global minimum.</span>
                        <span data-lang="zh"><strong>æ¦‚ç‡è”ç³»ï¼š</strong>æœ€å°åŒ–äº¤å‰ç†µä¸æ±‚ Î¸ çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å®Œå…¨ç­‰ä»·ã€‚è¿™é‡Œæ²¡æœ‰"ä»»æ„"çš„æŸå¤±å‡½æ•°é€‰æ‹©â€”â€”äº¤å‰ç†µæ­£æ˜¯æ¦‚ç‡æ¨¡å‹æœ¬èº«æ‰€è¦æ±‚çš„ã€‚å®ƒå¯¹ Î¸ ä¹Ÿæ˜¯å‡¸å‡½æ•°ï¼Œä¿è¯æ¢¯åº¦ä¸‹é™èƒ½æ‰¾åˆ°å…¨å±€æœ€å°å€¼ã€‚</span>
                    </div>
                </div>

                <!-- ========== SECTION 4 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">4. The Gradient and the Update Rule</span>
                        <span data-lang="zh">4. æ¢¯åº¦ä¸æ›´æ–°è§„åˆ™</span>
                    </h2>

                    <p data-lang="en">
                        Unlike linear regression, there is no closed-form solution for logistic regression. We must minimize J(Î¸) using gradient descent. The gradient has a surprisingly clean form.
                    </p>
                    <p data-lang="zh">
                        ä¸çº¿æ€§å›å½’ä¸åŒï¼Œé€»è¾‘å›å½’æ²¡æœ‰é—­å¼è§£ã€‚æˆ‘ä»¬å¿…é¡»ç”¨æ¢¯åº¦ä¸‹é™æ¥æœ€å°åŒ– J(Î¸)ã€‚å…¶æ¢¯åº¦æœ‰ä¸€ä¸ªå‡ºäººæ„æ–™çš„ç®€æ´å½¢å¼ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Deriving the gradient</span>
                        <span data-lang="zh">æ¨å¯¼æ¢¯åº¦</span>
                    </h3>
                    <p data-lang="en">
                        Apply the chain rule: âˆ‚J/âˆ‚Î¸ = âˆ‚J/âˆ‚pÌ‚ Â· âˆ‚pÌ‚/âˆ‚z Â· âˆ‚z/âˆ‚Î¸. Using Ïƒ'(z) = Ïƒ(z)(1âˆ’Ïƒ(z)) and pÌ‚ = Ïƒ(z):
                    </p>
                    <p data-lang="zh">
                        åº”ç”¨é“¾å¼æ³•åˆ™ï¼šâˆ‚J/âˆ‚Î¸ = âˆ‚J/âˆ‚pÌ‚ Â· âˆ‚pÌ‚/âˆ‚z Â· âˆ‚z/âˆ‚Î¸ã€‚åˆ©ç”¨ Ïƒ'(z) = Ïƒ(z)(1âˆ’Ïƒ(z)) ä»¥åŠ pÌ‚ = Ïƒ(z)ï¼š
                    </p>

                    <div class="math-box">
$$\nabla_\theta J(\theta) = \frac{1}{n} X^\top (\hat{p} - y)$$
                    </div>

                    <p data-lang="en">
                        The gradient is exactly the same form as in OLS: (1/n) Xáµ€(Å· âˆ’ y), with Å· replaced by pÌ‚ = Ïƒ(XÎ¸). The sigmoid's self-referential derivative cancels out neatly through the chain rule. This clean result is not a coincidence â€” it follows from using the natural loss function (cross-entropy) paired with its natural link function (logit/sigmoid).
                    </p>
                    <p data-lang="zh">
                        æ¢¯åº¦ä¸ OLS ä¸­çš„å½¢å¼å®Œå…¨ç›¸åŒï¼š(1/n) Xáµ€(Å· âˆ’ y)ï¼Œåªæ˜¯å°† Å· æ¢ä¸º pÌ‚ = Ïƒ(XÎ¸)ã€‚Sigmoid çš„è‡ªå¼•ç”¨å¯¼æ•°åœ¨é“¾å¼æ³•åˆ™ä¸­å·§å¦™æŠµæ¶ˆã€‚è¿™ä¸€ç®€æ´ç»“æœå¹¶éå·§åˆâ€”â€”å®ƒæ¥è‡ªäºå°†è‡ªç„¶æŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰ä¸è‡ªç„¶è¿æ¥å‡½æ•°ï¼ˆlogit/sigmoidï¼‰é…å¯¹ä½¿ç”¨ã€‚
                    </p>

                    <p data-lang="en">The gradient descent update rule is therefore:</p>
                    <p data-lang="zh">å› æ­¤ï¼Œæ¢¯åº¦ä¸‹é™æ›´æ–°è§„åˆ™ä¸ºï¼š</p>

                    <div class="math-box">
$$\theta \leftarrow \theta - \frac{\alpha}{n} X^\top\bigl(\sigma(X\theta) - y\bigr)$$
                    </div>

                    <p data-lang="en">
                        Each step computes the predicted probabilities Ïƒ(XÎ¸), subtracts the true labels y, and nudges Î¸ in the direction that reduces the cross-entropy. With a well-chosen learning rate Î±, this converges to the MLE solution.
                    </p>
                    <p data-lang="zh">
                        æ¯ä¸€æ­¥è®¡ç®—é¢„æµ‹æ¦‚ç‡ Ïƒ(XÎ¸)ï¼Œå‡å»çœŸå®æ ‡ç­¾ yï¼Œç„¶åå°† Î¸ å‘å‡å°‘äº¤å‰ç†µçš„æ–¹å‘ç§»åŠ¨ä¸€å°æ­¥ã€‚é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡ Î±ï¼Œè¿™å°†æ”¶æ•›åˆ° MLE è§£ã€‚
                    </p>
                </div>

                <!-- ========== SECTION 5 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">5. Interpreting Coefficients as Odds Ratios</span>
                        <span data-lang="zh">5. å°†ç³»æ•°è§£é‡Šä¸ºä¼˜åŠ¿æ¯”</span>
                    </h2>

                    <p data-lang="en">
                        In linear regression, Î¸â±¼ means "a one-unit increase in xâ±¼ changes y by Î¸â±¼, holding all else constant." In logistic regression, the interpretation is different â€” and more nuanced.
                    </p>
                    <p data-lang="zh">
                        åœ¨çº¿æ€§å›å½’ä¸­ï¼ŒÎ¸â±¼ çš„å«ä¹‰æ˜¯"æ§åˆ¶å…¶ä»–å˜é‡åï¼Œxâ±¼ æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œy å˜åŒ– Î¸â±¼"ã€‚åœ¨é€»è¾‘å›å½’ä¸­ï¼Œè§£é‡Šæ–¹å¼ä¸åŒâ€”â€”ä¹Ÿæ›´åŠ å¾®å¦™ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Coefficient â†’ log-odds â†’ odds ratio</span>
                        <span data-lang="zh">ç³»æ•° â†’ å¯¹æ•°ä¼˜åŠ¿ â†’ ä¼˜åŠ¿æ¯”</span>
                    </h3>
                    <p data-lang="en">
                        Because the model is log(p/1âˆ’p) = XÎ¸, each coefficient Î¸â±¼ is the change in log-odds per unit increase in xâ±¼. Exponentiating gives the <span class="highlight-key">odds ratio</span>:
                    </p>
                    <p data-lang="zh">
                        å› ä¸ºæ¨¡å‹ä¸º log(p/1âˆ’p) = XÎ¸ï¼Œæ¯ä¸ªç³»æ•° Î¸â±¼ æ˜¯ xâ±¼ æ¯å¢åŠ ä¸€ä¸ªå•ä½å¯¹åº”çš„å¯¹æ•°ä¼˜åŠ¿å˜åŒ–ã€‚å–æŒ‡æ•°å¾—åˆ°<span class="highlight-key">ä¼˜åŠ¿æ¯”</span>ï¼š
                    </p>

                    <div class="math-box">
$$\text{Odds Ratio}_j = e^{\theta_j}$$
                    </div>

                    <p data-lang="en">
                        An odds ratio of 1.5 means the odds of backsliding multiply by 1.5 for each one-unit increase in xâ±¼. If xâ±¼ is the Gini index (0â€“1 scale), Î¸â±¼ = 0.4 means OR = e^0.4 â‰ˆ 1.49: a one-point increase in inequality is associated with 49% higher odds of democratic backsliding, holding all other predictors constant.
                    </p>
                    <p data-lang="zh">
                        ä¼˜åŠ¿æ¯”ä¸º 1.5 æ„å‘³ç€ xâ±¼ æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œå‘ç”Ÿå€’é€€çš„ä¼˜åŠ¿å°±ä¹˜ä»¥ 1.5ã€‚å¦‚æœ xâ±¼ æ˜¯åŸºå°¼ç³»æ•°ï¼ˆ0â€“1 åˆ»åº¦ï¼‰ï¼ŒÎ¸â±¼ = 0.4 åˆ™ä¼˜åŠ¿æ¯” = e^0.4 â‰ˆ 1.49ï¼šæ§åˆ¶å…¶ä»–é¢„æµ‹å˜é‡åï¼Œä¸å¹³ç­‰ç¨‹åº¦æ¯å¢åŠ ä¸€ä¸ªå•ä½ï¼Œæ°‘ä¸»å€’é€€çš„ä¼˜åŠ¿æé«˜ 49%ã€‚
                    </p>

                    <h3>
                        <span data-lang="en">Three reference points for odds ratios</span>
                        <span data-lang="zh">ä¼˜åŠ¿æ¯”çš„ä¸‰ä¸ªå‚è€ƒç‚¹</span>
                    </h3>

                    <table class="guide-table">
                        <thead>
                            <tr>
                                <th>
                                    <span data-lang="en">Odds Ratio eá¶¿</span>
                                    <span data-lang="zh">ä¼˜åŠ¿æ¯” eá¶¿</span>
                                </th>
                                <th>
                                    <span data-lang="en">What it means</span>
                                    <span data-lang="zh">å«ä¹‰</span>
                                </th>
                                <th>
                                    <span data-lang="en">Example (backsliding)</span>
                                    <span data-lang="zh">ç¤ºä¾‹ï¼ˆæ°‘ä¸»å€’é€€ï¼‰</span>
                                </th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td><strong>eá¶¿ &lt; 1</strong></td>
                                <td>
                                    <span data-lang="en">Protective factor: higher xâ±¼ â†’ lower odds of the event</span>
                                    <span data-lang="zh">ä¿æŠ¤å› ç´ ï¼šxâ±¼ è¶Šå¤§ â†’ äº‹ä»¶å‘ç”Ÿçš„ä¼˜åŠ¿è¶Šä½</span>
                                </td>
                                <td>
                                    <span data-lang="en">e.g. eá¶¿ = 0.6 for log GDP â€” wealthier countries less likely to backslide</span>
                                    <span data-lang="zh">ä¾‹å¦‚ log GDP çš„ eá¶¿ = 0.6 â€”â€”è¶Šå¯Œè£•çš„å›½å®¶è¶Šä¸å¯èƒ½å‘ç”Ÿå€’é€€</span>
                                </td>
                            </tr>
                            <tr>
                                <td><strong>eá¶¿ = 1</strong></td>
                                <td>
                                    <span data-lang="en">No effect: xâ±¼ is unrelated to the odds (Î¸â±¼ = 0)</span>
                                    <span data-lang="zh">æ— å½±å“ï¼šxâ±¼ ä¸ä¼˜åŠ¿æ— å…³ï¼ˆÎ¸â±¼ = 0ï¼‰</span>
                                </td>
                                <td>
                                    <span data-lang="en">e.g. geographic area â€” country size has no effect on backsliding odds</span>
                                    <span data-lang="zh">ä¾‹å¦‚åœ°ç†é¢ç§¯â€”â€”å›½å®¶å¤§å°å¯¹å€’é€€ä¼˜åŠ¿æ— å½±å“</span>
                                </td>
                            </tr>
                            <tr>
                                <td><strong>eá¶¿ &gt; 1</strong></td>
                                <td>
                                    <span data-lang="en">Risk factor: higher xâ±¼ â†’ higher odds of the event</span>
                                    <span data-lang="zh">é£é™©å› ç´ ï¼šxâ±¼ è¶Šå¤§ â†’ äº‹ä»¶å‘ç”Ÿçš„ä¼˜åŠ¿è¶Šé«˜</span>
                                </td>
                                <td>
                                    <span data-lang="en">e.g. eá¶¿ = 1.8 for Gini â€” more inequality raises backsliding odds by 80%</span>
                                    <span data-lang="zh">ä¾‹å¦‚åŸºå°¼ç³»æ•°çš„ eá¶¿ = 1.8 â€”â€”æ›´é«˜çš„ä¸å¹³ç­‰ä½¿å€’é€€ä¼˜åŠ¿æé«˜ 80%</span>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <div class="definition-box">
                        <span data-lang="en"><strong>Caution on marginal effects:</strong> The effect of xâ±¼ on the <em>probability</em> p (not just the odds) depends on where you are on the sigmoid curve. Near p=0.5, a unit increase in xâ±¼ has a large effect on probability; near p=0 or p=1 it has almost none (the sigmoid is flat there). For the average marginal effect on probability, compute âˆ‚p/âˆ‚xâ±¼ = Î¸â±¼Â·pÌ‚(1âˆ’pÌ‚) evaluated at each observation and then average. Most statistical software (Stata: <code>margins</code>, R: <code>marginaleffects</code> package) does this automatically.</span>
                        <span data-lang="zh"><strong>è¾¹é™…æ•ˆåº”æ³¨æ„äº‹é¡¹ï¼š</strong>xâ±¼ å¯¹<em>æ¦‚ç‡</em> pï¼ˆè€Œéä¼˜åŠ¿æ¯”ï¼‰çš„å½±å“å–å†³äºä½ åœ¨ sigmoid æ›²çº¿ä¸Šçš„ä½ç½®ã€‚åœ¨ pâ‰ˆ0.5 é™„è¿‘ï¼Œxâ±¼ å¢åŠ ä¸€ä¸ªå•ä½å¯¹æ¦‚ç‡å½±å“å¾ˆå¤§ï¼›åœ¨ pâ‰ˆ0 æˆ– pâ‰ˆ1 é™„è¿‘å‡ ä¹æ²¡æœ‰å½±å“ï¼ˆsigmoid åœ¨é‚£é‡Œæ˜¯å¹³çš„ï¼‰ã€‚å¦‚éœ€å¯¹æ¦‚ç‡çš„å¹³å‡è¾¹é™…æ•ˆåº”ï¼Œè®¡ç®— âˆ‚p/âˆ‚xâ±¼ = Î¸â±¼Â·pÌ‚(1âˆ’pÌ‚) å¹¶å¯¹æ‰€æœ‰è§‚æµ‹å–å¹³å‡ã€‚å¤§å¤šæ•°ç»Ÿè®¡è½¯ä»¶ï¼ˆStataï¼š<code>margins</code>ï¼ŒRï¼š<code>marginaleffects</code> åŒ…ï¼‰å¯è‡ªåŠ¨å®Œæˆã€‚</span>
                    </div>
                </div>

                <!-- ========== SECTION 6 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">6. Multi-Class Extension: Softmax Regression</span>
                        <span data-lang="zh">6. å¤šåˆ†ç±»æ‰©å±•ï¼šSoftmax å›å½’</span>
                    </h2>

                    <p data-lang="en">
                        Logistic regression handles two classes (backsliding / no backsliding). What if there are K > 2 classes â€” say, five regime types: liberal democracy, electoral democracy, competitive authoritarianism, electoral authoritarianism, and closed autocracy?
                    </p>
                    <p data-lang="zh">
                        é€»è¾‘å›å½’å¤„ç†ä¸¤ä¸ªç±»åˆ«ï¼ˆå€’é€€/ä¸å€’é€€ï¼‰ã€‚å¦‚æœæœ‰ K > 2 ä¸ªç±»åˆ«â€”â€”æ¯”å¦‚äº”ç§æ”¿ä½“ç±»å‹ï¼šè‡ªç”±æ°‘ä¸»ã€é€‰ä¸¾æ°‘ä¸»ã€ç«äº‰æ€§å¨æƒä¸»ä¹‰ã€é€‰ä¸¾å¨æƒä¸»ä¹‰å’Œå°é—­ä¸“åˆ¶â€”â€”è¯¥æ€ä¹ˆåŠï¼Ÿ
                    </p>

                    <h3>
                        <span data-lang="en">The softmax function</span>
                        <span data-lang="zh">Softmax å‡½æ•°</span>
                    </h3>
                    <p data-lang="en">
                        We maintain K separate weight vectors Î¸â‚, Î¸â‚‚, â€¦, Î¸_K (one per class). For each observation, we compute K raw scores z_k = xáµ€Î¸_k, then convert them to probabilities using softmax:
                    </p>
                    <p data-lang="zh">
                        æˆ‘ä»¬ä¸ºæ¯ä¸ªç±»åˆ«ç»´æŠ¤ä¸€ä¸ªæƒé‡å‘é‡ Î¸â‚, Î¸â‚‚, â€¦, Î¸_Kã€‚å¯¹æ¯ä¸ªè§‚æµ‹ï¼Œè®¡ç®— K ä¸ªåŸå§‹å¾—åˆ† z_k = xáµ€Î¸_kï¼Œç„¶åç”¨ softmax å°†å…¶è½¬æ¢ä¸ºæ¦‚ç‡ï¼š
                    </p>

                    <div class="math-box">
$$P(y = k \mid x, \Theta) = \frac{e^{z_k}}{\sum_{j=1}^{K} e^{z_j}} = \frac{e^{x^\top\theta_k}}{\sum_{j=1}^{K} e^{x^\top\theta_j}}$$
                    </div>

                    <p data-lang="en">
                        Why exponentiate? Two reasons: (1) e^z is always positive, guaranteeing non-negative probabilities; (2) e^z amplifies differences between raw scores, making the highest-scoring class stand out more sharply. Dividing by the sum normalizes everything to sum to 1.
                    </p>
                    <p data-lang="zh">
                        ä¸ºä»€ä¹ˆå–æŒ‡æ•°ï¼Ÿä¸¤ä¸ªåŸå› ï¼š(1) e^z æ€»æ˜¯æ­£æ•°ï¼Œä¿è¯æ¦‚ç‡éè´Ÿï¼›(2) e^z æ”¾å¤§äº†åŸå§‹å¾—åˆ†ä¹‹é—´çš„å·®å¼‚ï¼Œä½¿å¾—åˆ†æœ€é«˜çš„ç±»åˆ«æ›´åŠ çªå‡ºã€‚é™¤ä»¥æ€»å’Œä½¿æ‰€æœ‰æ¦‚ç‡åŠ èµ·æ¥ç­‰äº 1ã€‚
                    </p>

                    <p data-lang="en">
                        Notice that when K=2, softmax reduces to the standard sigmoid. The loss function generalizes to categorical cross-entropy:
                    </p>
                    <p data-lang="zh">
                        æ³¨æ„å½“ K=2 æ—¶ï¼Œsoftmax é€€åŒ–ä¸ºæ ‡å‡† sigmoidã€‚æŸå¤±å‡½æ•°æ¨å¹¿ä¸ºåˆ†ç±»äº¤å‰ç†µï¼š
                    </p>

                    <div class="math-box">
$$J(\Theta) = -\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K} \mathbf{1}[y_i = k]\log P(y_i = k \mid x_i, \Theta)$$
                    </div>

                    <p data-lang="en">
                        Here ğŸ[yáµ¢ = k] is an indicator: it equals 1 only when observation i truly belongs to class k. At each step, the model is penalized only for getting the true class probability wrong.
                    </p>
                    <p data-lang="zh">
                        å…¶ä¸­ ğŸ[yáµ¢ = k] æ˜¯æŒ‡ç¤ºå‡½æ•°ï¼šä»…å½“è§‚æµ‹ i çœŸæ­£å±äºç±»åˆ« k æ—¶ç­‰äº 1ã€‚åœ¨æ¯ä¸€æ­¥ä¸­ï¼Œæ¨¡å‹ä»…å› æŠŠçœŸå®ç±»åˆ«çš„æ¦‚ç‡é¢„æµ‹é”™è¯¯è€Œå—åˆ°æƒ©ç½šã€‚
                    </p>
                </div>

                <!-- ========== SECTION 7 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">7. OLS vs. Logistic Regression: When to Use Which</span>
                        <span data-lang="zh">7. OLS ä¸é€»è¾‘å›å½’ï¼šä½•æ—¶ç”¨å“ªä¸ª</span>
                    </h2>

                    <table class="guide-table">
                        <thead>
                            <tr>
                                <th>
                                    <span data-lang="en">Criterion</span>
                                    <span data-lang="zh">æ¯”è¾ƒç»´åº¦</span>
                                </th>
                                <th>
                                    <span data-lang="en">OLS / Linear Regression</span>
                                    <span data-lang="zh">OLS / çº¿æ€§å›å½’</span>
                                </th>
                                <th>
                                    <span data-lang="en">Logistic Regression</span>
                                    <span data-lang="zh">é€»è¾‘å›å½’</span>
                                </th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>
                                    <span data-lang="en">Outcome type</span>
                                    <span data-lang="zh">ç»“æœç±»å‹</span>
                                </td>
                                <td>
                                    <span data-lang="en">Continuous (democracy score, GDP, life expectancy)</span>
                                    <span data-lang="zh">è¿ç»­å‹ï¼ˆæ°‘ä¸»ç¨‹åº¦å¾—åˆ†ã€GDPã€é¢„æœŸå¯¿å‘½ï¼‰</span>
                                </td>
                                <td>
                                    <span data-lang="en">Binary (backsliding yes/no) or multi-class (regime type)</span>
                                    <span data-lang="zh">äºŒå…ƒï¼ˆå€’é€€ä¸å¦ï¼‰æˆ–å¤šç±»ï¼ˆæ”¿ä½“ç±»å‹ï¼‰</span>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <span data-lang="en">Prediction range</span>
                                    <span data-lang="zh">é¢„æµ‹å€¼èŒƒå›´</span>
                                </td>
                                <td>
                                    <span data-lang="en">Unbounded (âˆ’âˆ, +âˆ) â€” can predict impossible values</span>
                                    <span data-lang="zh">æ— ç•Œï¼ˆâˆ’âˆ, +âˆï¼‰â€”â€”å¯èƒ½é¢„æµ‹å‡ºä¸å¯èƒ½çš„å€¼</span>
                                </td>
                                <td>
                                    <span data-lang="en">Bounded (0, 1) â€” always a valid probability</span>
                                    <span data-lang="zh">æœ‰ç•Œ (0, 1)â€”â€”å§‹ç»ˆæ˜¯æœ‰æ•ˆæ¦‚ç‡</span>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <span data-lang="en">Loss function</span>
                                    <span data-lang="zh">æŸå¤±å‡½æ•°</span>
                                </td>
                                <td>
                                    <span data-lang="en">Mean Squared Error (MSE)</span>
                                    <span data-lang="zh">å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰</span>
                                </td>
                                <td>
                                    <span data-lang="en">Cross-entropy (negative log-likelihood)</span>
                                    <span data-lang="zh">äº¤å‰ç†µï¼ˆè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼‰</span>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <span data-lang="en">Closed-form solution?</span>
                                    <span data-lang="zh">æœ‰é—­å¼è§£ï¼Ÿ</span>
                                </td>
                                <td>
                                    <span data-lang="en">Yes â€” Normal Equation Î¸* = (XâŠ¤X)â»Â¹XâŠ¤y</span>
                                    <span data-lang="zh">æœ‰â€”â€”æ³•çº¿æ–¹ç¨‹ Î¸* = (XâŠ¤X)â»Â¹XâŠ¤y</span>
                                </td>
                                <td>
                                    <span data-lang="en">No â€” requires iterative gradient descent or Newton-Raphson</span>
                                    <span data="zh">æ— â€”â€”éœ€è¦æ¢¯åº¦ä¸‹é™æˆ– Newton-Raphson è¿­ä»£</span>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <span data-lang="en">Coefficient interpretation</span>
                                    <span data-lang="zh">ç³»æ•°è§£é‡Š</span>
                                </td>
                                <td>
                                    <span data-lang="en">Direct: Î¸â±¼ = change in y per unit xâ±¼</span>
                                    <span data-lang="zh">ç›´æ¥ï¼šÎ¸â±¼ = xâ±¼ æ¯å˜åŒ–ä¸€å•ä½ï¼Œy çš„å˜åŒ–é‡</span>
                                </td>
                                <td>
                                    <span data-lang="en">Indirect: eá¶¿Ê² = odds ratio; marginal effect on p varies</span>
                                    <span data-lang="zh">é—´æ¥ï¼šeá¶¿Ê² = ä¼˜åŠ¿æ¯”ï¼›å¯¹æ¦‚ç‡ p çš„è¾¹é™…æ•ˆåº”éšä½ç½®å˜åŒ–</span>
                                </td>
                            </tr>
                            <tr>
                                <td>
                                    <span data-lang="en">Error variance assumption</span>
                                    <span data-lang="zh">è¯¯å·®æ–¹å·®å‡è®¾</span>
                                </td>
                                <td>
                                    <span data-lang="en">Homoskedastic errors required for valid SEs</span>
                                    <span data-lang="zh">éœ€è¦åŒæ–¹å·®è¯¯å·®ä»¥è·å¾—æœ‰æ•ˆæ ‡å‡†è¯¯</span>
                                </td>
                                <td>
                                    <span data-lang="en">Bernoulli variance p(1âˆ’p) is built into the model â€” no separate assumption needed</span>
                                    <span data-lang="zh">ä¼¯åŠªåˆ©æ–¹å·® p(1âˆ’p) å†…ç½®äºæ¨¡å‹ä¸­â€”â€”æ— éœ€å•ç‹¬å‡è®¾</span>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <!-- ========== SECTION 8 ========== -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">8. Key Takeaways</span>
                        <span data-lang="zh">8. æ ¸å¿ƒè¦ç‚¹</span>
                    </h2>

                    <p data-lang="en">
                        Logistic regression is the principled solution to the binary classification problem. Every design choice â€” the sigmoid, the log-odds link, the cross-entropy loss â€” follows logically from the requirement that predictions be valid probabilities derived from maximum likelihood.
                    </p>
                    <p data-lang="zh">
                        é€»è¾‘å›å½’æ˜¯äºŒå…ƒåˆ†ç±»é—®é¢˜çš„æœ‰åŸåˆ™çš„è§£å†³æ–¹æ¡ˆã€‚æ¯ä¸€ä¸ªè®¾è®¡é€‰æ‹©â€”â€”sigmoidã€å¯¹æ•°ä¼˜åŠ¿è¿æ¥å‡½æ•°ã€äº¤å‰ç†µæŸå¤±â€”â€”éƒ½ä»"é¢„æµ‹å€¼å¿…é¡»æ˜¯æœ€å¤§ä¼¼ç„¶æ¨å¯¼å‡ºçš„æœ‰æ•ˆæ¦‚ç‡"è¿™ä¸€è¦æ±‚ä¸­é€»è¾‘åœ°æ¨å¯¼è€Œæ¥ã€‚
                    </p>

                    <div class="definition-box">
                        <span data-lang="en">
                            Binary outcome y âˆˆ {0,1} â†’ can't use OLS (unbounded predictions, heteroskedastic errors) â†’ model log-odds = XÎ¸ instead â†’ solving for p gives sigmoid pÌ‚ = Ïƒ(XÎ¸) â†’ maximize likelihood â‰¡ minimize cross-entropy loss J(Î¸) â†’ no closed form, so use gradient descent with update Î¸ â† Î¸ âˆ’ (Î±/n)Xáµ€(pÌ‚ âˆ’ y) â†’ coefficients Î¸â±¼ interpreted as log-odds effects; eá¶¿Ê² is the odds ratio â†’ extend to K classes via softmax.
                        </span>
                        <span data-lang="zh">
                            äºŒå…ƒç»“æœ y âˆˆ {0,1} â†’ ä¸èƒ½ç”¨ OLSï¼ˆé¢„æµ‹å€¼æ— ç•Œï¼Œè¯¯å·®å¼‚æ–¹å·®ï¼‰â†’ è½¬è€Œå»ºæ¨¡å¯¹æ•°ä¼˜åŠ¿æ¯” = XÎ¸ â†’ è§£å‡º p å¾—åˆ° sigmoidï¼špÌ‚ = Ïƒ(XÎ¸) â†’ æœ€å¤§åŒ–ä¼¼ç„¶ â‰¡ æœ€å°åŒ–äº¤å‰ç†µæŸå¤± J(Î¸) â†’ æ— é—­å¼è§£ï¼Œç”¨æ¢¯åº¦ä¸‹é™æ›´æ–° Î¸ â† Î¸ âˆ’ (Î±/n)Xáµ€(pÌ‚ âˆ’ y) â†’ ç³»æ•° Î¸â±¼ è§£é‡Šä¸ºå¯¹æ•°ä¼˜åŠ¿æ•ˆåº”ï¼›eá¶¿Ê² ä¸ºä¼˜åŠ¿æ¯” â†’ é€šè¿‡ softmax æ¨å¹¿åˆ° K ä¸ªç±»åˆ«ã€‚
                        </span>
                    </div>
                </div>

                <!-- References -->
                <div class="guide-section">
                    <h2>
                        <span data-lang="en">References</span>
                        <span data-lang="zh">å‚è€ƒæ–‡çŒ®</span>
                    </h2>
                    <p class="footnote">
                        Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer. [Ch. 4]<br>
                        Murphy, K. P. (2022). <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press. [Ch. 10]<br>
                        Ng, A. (2000). <em>CS229 Lecture Notes: Logistic Regression and Generalized Linear Models</em>. Stanford University.<br>
                        King, G., &amp; Zeng, L. (2001). Logistic Regression in Rare Events Data. <em>Political Analysis</em>, 9(2), 137â€“163.<br>
                        Long, J. S. (1997). <em>Regression Models for Categorical and Limited Dependent Variables</em>. SAGE Publications.
                    </p>
                </div>

                <!-- Back link at bottom -->
                <div style="margin-top: 48px; padding-top: 24px; border-top: 1px solid var(--parchment); text-align: center;">
                    <a href="../machine-learning.html#ml-classification" class="guide-back" data-lang="en">â† Back to Machine Learning: Classification</a>
                    <a href="../machine-learning.html#ml-classification" class="guide-back" data-lang="zh">â† è¿”å›æœºå™¨å­¦ä¹ ï¼šåˆ†ç±»</a>
                </div>
            </div>
        </div>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const langBtns = document.querySelectorAll('.guide-lang-btn');
        const enElements = document.querySelectorAll('[data-lang="en"]');
        const zhElements = document.querySelectorAll('[data-lang="zh"]');
        const savedLang = localStorage.getItem('preferred-lang') || 'en';
        setLanguage(savedLang);
        langBtns.forEach(btn => {
            btn.addEventListener('click', function() {
                const lang = this.getAttribute('data-lang');
                localStorage.setItem('preferred-lang', lang);
                setLanguage(lang);
            });
        });
        function setLanguage(lang) {
            langBtns.forEach(btn => btn.classList.remove('active'));
            const activeBtn = document.querySelector('.guide-lang-btn[data-lang="' + lang + '"]');
            if (activeBtn) activeBtn.classList.add('active');
            enElements.forEach(el => { el.style.display = lang === 'en' ? '' : 'none'; });
            zhElements.forEach(el => { el.style.display = lang === 'zh' ? '' : 'none'; });
            document.body.className = lang;
        }
    });
    </script>

</body>
</html>
