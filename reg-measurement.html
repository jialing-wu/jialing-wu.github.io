---
layout: methods-course
title: "Measurement & Scaling"
breadcrumb: "Statistics"
bilingual: true
prev:
  url: reg-causal.html
  title: "Causal Inference"
next:
  url: reg-timeseries.html
  title: "Time Series & Panel Data"
---

<style>
.problem-index{margin:0 0 8px;padding:16px 20px;border:1px solid var(--parchment);border-radius:4px;background:var(--warm)}
.problem-index-title{font-family:var(--sans);font-size:10px;font-weight:700;letter-spacing:.12em;text-transform:uppercase;color:var(--gold);margin-bottom:12px}
.problem-index a{display:block;font-size:14.5px;line-height:2;color:var(--ink-faded);text-decoration:none;transition:color .2s}
.problem-index a:hover{color:var(--red)}
.problem-index a .pi-arrow{font-family:var(--sans);font-size:11px;color:var(--gold);margin-left:6px}
</style>

<!-- HEADER -->
<div class="method-header">
  <h1>Measurement, Scaling &amp; Dimensional Analysis</h1>
  <div class="method-meta">Statistics &middot; Intermediate 06</div>
</div>

<!-- INTRO CARDS -->
<div class="intro-cards">
  <div class="intro-card">
    <div class="card-label" data-lang="en">What Is This?</div>
    <div class="card-label" data-lang="zh">这一页讲什么？</div>
    <div data-lang="en"><p>You can't analyze what you can't measure. But in social science, the most important concepts — ideology, trust, wellbeing, social capital — are not directly observable. Measurement theory gives you principled ways to build reliable, valid indicators.</p></div>
    <div data-lang="zh"><p>你无法分析你无法测量的东西。但在社会科学中，最重要的概念——意识形态、信任、幸福感、社会资本——都无法直接观察。测量理论给你提供有原则的方式来构建可靠、有效的指标。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Prerequisites</div>
    <div class="card-label" data-lang="zh">前置知识</div>
    <div data-lang="en"><p>Regression Analysis. Familiarity with correlation helpful.</p></div>
    <div data-lang="zh"><p>回归分析。了解相关性会有帮助。</p></div>
  </div>
  <div class="intro-card">
    <div class="card-label" data-lang="en">Software &amp; Tools</div>
    <div class="card-label" data-lang="zh">软件工具</div>
    <div data-lang="en"><p>R (psych, lavaan, ltm) or Stata (alpha, factor, sem).</p></div>
    <div data-lang="zh"><p>R（psych、lavaan、ltm）或 Stata（alpha、factor、sem）。</p></div>
  </div>
</div>

<!-- PROBLEM INDEX -->
<div class="problem-index">
  <div class="problem-index-title" data-lang="en">What problem are you facing?</div>
  <div class="problem-index-title" data-lang="zh">你遇到了什么问题？</div>
  <a href="#ms-s1" data-lang="en">How do I measure an abstract concept I can't directly observe? <span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#ms-s1" data-lang="zh">我怎么测量一个无法直接观察的抽象概念？<span class="pi-arrow">&rarr; &sect;1</span></a>
  <a href="#ms-s2" data-lang="en">I have multiple survey items — how do I combine them into one index? <span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#ms-s2" data-lang="zh">我有多个调查题项——怎么把它们合并成一个指标？<span class="pi-arrow">&rarr; &sect;2</span></a>
  <a href="#ms-s3" data-lang="en">How do I know if my measure is reliable and valid? <span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#ms-s3" data-lang="zh">我怎么知道我的测量是否可靠且有效？<span class="pi-arrow">&rarr; &sect;3</span></a>
  <a href="#ms-s4" data-lang="en">How do I build a new scale from scratch? <span class="pi-arrow">&rarr; &sect;4</span></a>
  <a href="#ms-s4" data-lang="zh">我怎么从零构建一个新量表？<span class="pi-arrow">&rarr; &sect;4</span></a>
  <a href="#ms-s5" data-lang="en">My items differ in difficulty or extremity — should I use IRT? <span class="pi-arrow">&rarr; &sect;5</span></a>
  <a href="#ms-s5" data-lang="zh">我的题项难度不同——我应该用 IRT 吗？<span class="pi-arrow">&rarr; &sect;5</span></a>
</div>

<hr class="section-divider">

<!-- SECTION 1 -->
<div class="section" id="ms-s1">
  <h2 data-lang="en">How Do I Measure an Abstract Concept I Can't Directly Observe?</h2>
  <h2 data-lang="zh">我怎么测量一个无法直接观察的抽象概念？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You want to study political ideology, social trust, or democratic quality. You can't directly observe ideology — you can't open a person's head and measure it. Instead, you ask survey questions: "Do you think government should be more involved in healthcare?" (1=No to 5=Yes). Multiple questions across respondents create variation you can analyze. But each question captures only one facet of ideology. Measurement theory tells you how to validate that your questions actually measure ideology (not something else) and how to combine questions into a coherent scale.</p>
    <p data-lang="zh"><strong>问题：</strong>你想研究政治意识形态、社会信任或民主质量。你无法直接观察意识形态——你无法打开一个人的头并测量它。相反，你问调查问题：「你认为政府应该更多地参与医疗保健吗？」（1=否到 5=是）。跨受访者的多个问题创造了你可以分析的变化。但每个问题仅捕获意识形态的一个方面。测量理论告诉你如何验证你的问题是否实际测量意识形态（不是其他东西）以及如何将问题组合成一个一致的量表。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Conceptualization vs. Operationalization</h3>
    <h3 data-lang="zh">概念化与操作化</h3>
    <p class="method-desc" data-lang="en">Conceptualization: define what you mean by "ideology." Is it positions on economic redistribution, social progressivism, foreign interventionism, or all three? Political scientists disagree, so you must be explicit. Operationalization: once you've defined ideology, choose observable indicators (survey items) that proxy it. Ideology cannot be directly measured, so you use questions about healthcare policy, tax progressivity, immigration as stand-ins. The gap between concept and operationalization is always present. Validity means your operationalization captures the concept; invalidity means you're measuring something else (e.g., education rather than ideology). Multiple operationalizations of the same concept often yield different conclusions. Freedom House measures democracy via political rights + civil liberties; Varieties of Democracy uses hundreds of indicators. Countries might rank differently depending on which operationalization you use.</p>
    <p class="method-desc" data-lang="zh">概念化：定义你对「意识形态」的含义。它是关于经济再分配、社会进步主义、外交干预主义的立场，还是所有这些？政治科学家不同意，所以你必须明确。操作化：一旦你定义了意识形态，选择可观察的指标（调查项目）来代理它。意识形态无法直接测量，所以你使用关于医疗保健政策、税收渐进性、移民的问题作为替代物。概念和操作化之间的差距始终存在。有效性意味着你的操作化捕获了概念；无效性意味着你在测量其他东西（例如，教育而非意识形态）。同一概念的多个操作化通常产生不同的结论。Freedom House 通过政治权利 + 公民自由测量民主；Varieties of Democracy 使用数百个指标。国家可能会根据你使用的操作化而排名不同。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> You can't directly measure "fitness," so you use proxy measures: VO2 max (aerobic capacity), resting heart rate, BMI, strength tests. But which combination captures fitness best? A powerlifter has high strength but low VO2 max. A marathon runner has high VO2 max but low strength. No single indicator is perfect. Similarly, different operationalizations of ideology (single-left-right question vs. multi-item scale) capture different facets.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>你无法直接测量「健身」，所以你使用代理措施：VO2 max（有氧能力）、静息心率、BMI、力量测试。但哪个组合最好地捕获健身？举重运动员有很高的力量但 VO2 max 很低。马拉松运动员 VO2 max 很高但力量很低。没有单一指标是完美的。同样，不同的意识形态操作化（单一左右问题对多项量表）捕获不同的方面。</div>
    <div class="method-when" data-lang="en"><strong>When to conceptualize carefully:</strong> Always! Before any measurement, define your concept explicitly. Discuss what it includes and excludes. <strong>When multiple operationalizations exist:</strong> Acknowledge the ambiguity. Run analyses with multiple operationalizations to see if conclusions are robust. If conclusions change, your results are sensitive to how you measured the construct (a major limitation).</div>
    <div class="method-when" data-lang="zh"><strong>何时仔细概念化：</strong>总是！在任何测量之前，明确定义你的概念。讨论它包括和排除了什么。<strong>当多个操作化存在：</strong>承认歧义。用多个操作化运行分析以查看结论是否强健。如果结论改变，你的结果对你如何测量构造敏感（一个主要限制）。</p>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Measuring "democratic backsliding." One operationalization: decline in Freedom House scores (binary rights/liberties indicators). Another: V-Dem's Liberal Democracy Index (electoral, deliberative, civil, participatory components). Hungary under Orbán scores worse on V-Dem (captures institutional decline) than Freedom House (which emphasizes binary freedoms). Which operationalization is correct? Depends on your concept of democracy — both have merit, but they highlight different aspects of democratic decay.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>测量「民主倒退」。一个操作化：Freedom House 分数的下降（二元权利/自由指标）。另一个：V-Dem 的自由民主指数（选举、审议、公民、参与成分）。Orbán 统治下的匈牙利在 V-Dem（捕获制度衰退）上的得分比 Freedom House（强调二元自由）差。哪个操作化是正确的？取决于你的民主概念——两者都有价值，但它们突出了民主衰退的不同方面。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Types of Validity</h3>
    <h3 data-lang="zh">有效性的类型</h3>
    <p class="method-desc" data-lang="en">Face validity: does the measure look like it's measuring the concept? An ideology scale asking about healthcare, taxation, and foreign policy has face validity (seems reasonable). Content validity: does the measure comprehensively capture the concept? A single-item ideology question lacks content validity — ideology is multidimensional. Criterion validity: does the measure predict related outcomes? A political knowledge scale should predict actual news consumption and voting behavior (convergent validity). It should not predict height or shoe size (discriminant validity). Construct validity: the umbrella category encompassing all of the above. Your measurement is valid if your operationalization actually measures what you claim it measures.</p>
    <p class="method-desc" data-lang="zh">表面效度：测度看起来像它在测量概念吗？询问医疗保健、税收和外交政策的意识形态量表有表面效度（看起来合理）。内容效度：测度是否全面捕获概念？单项意识形态问题缺乏内容效度——意识形态是多维的。标准效度：测度是否预测相关的结果？政治知识量表应预测实际新闻消费和投票行为（收敛效度）。它不应预测身高或鞋码（判别效度）。构造效度：包括上述所有的总体类别。如果你的操作化实际测量了你声称的东西，你的测量是有效的。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> A bathroom scale has criterion validity if it predicts your actual weight (measured by a hospital scale). It has discriminant validity if it doesn't predict your height (unrelated concept). It has face validity if it looks like a weight-measuring device. A scale that's rusty and has "height" written on it might be broken, lacking all three.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>如果浴室秤能预测你的实际体重（由医院秤测量），它就有标准效度。如果它不能预测你的身高（无关概念），它就有判别效度。如果它看起来像一个测重设备，它有表面效度。生锈且上面写有「身高」的秤可能坏了，缺乏所有三个。</div>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 2 -->
<div class="section" id="ms-s2">
  <h2 data-lang="en">I Have Multiple Survey Items — How Do I Combine Them Into One Index?</h2>
  <h2 data-lang="zh">我有多个调查题项——怎么把它们合并成一个指标？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You have five survey items on political trust: "Trust in parliament," "Trust in courts," "Trust in police," "Trust in elections," "Trust in president." Respondents answered 1–5 on each. You want one measure of trust. Do you average them? That assumes equal weighting and equal spacing (1 to 2 is the same jump as 4 to 5). Better: use Exploratory Factor Analysis (EFA), which lets the data reveal the structure. EFA finds latent factors (underlying dimensions) that explain correlations among your items. It weights items based on how strongly they correlate with the underlying trust factor, and it tests whether they're unidimensional or multidimensional.</p>
    <p data-lang="zh"><strong>问题：</strong>你有五个关于政治信任的调查项目：「对议会的信任」、「对法院的信任」、「对警察的信任」、「对选举的信任」、「对总统的信任」。受访者在每个项目上回答 1-5。你想要一个信任的衡量指标。你平均它们吗？那假设了等权重和等间距（1 到 2 与 4 到 5 的跳跃相同）。更好：使用探索性因子分析（EFA），它让数据显示结构。EFA 找到潜在因子（基础维度），解释你的项目之间的相关性。它根据项目与底层信任因子的相关强度进行加权，并测试它们是否是单维或多维的。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Additive Indices and Their Limitations</h3>
    <h3 data-lang="zh">加法指数及其局限</h3>
    <p class="method-desc" data-lang="en">The simplest approach: sum or average your items. This assumes (1) all items equally important (not just their correlations), (2) intervals are equal (difference between 1 and 2 = difference between 4 and 5), (3) all items measure one underlying construct (unidimensional). Often these are too strong. An additive index is transparent and easy to interpret, but it may miss the underlying structure. If you have 5 trust items and 2 civic efficacy items mixed together, a simple average treats them equally despite measuring different constructs. Use additive indices only when you're confident in equal weighting and unidimensionality. Otherwise, use EFA to discover the actual structure.</p>
    <p class="method-desc" data-lang="zh">最简单的方法：对你的项目求和或平均。这假设（1）所有项目同等重要（不仅仅是它们的相关性），（2）间隔相等（1 和 2 之间的差 = 4 和 5 之间的差），（3）所有项目测量一个底层构造（单维）。通常这些太强了。加法指数是透明的且易于解释的，但它可能会错过底层结构。如果你有 5 个信任项目和 2 个公民效能项目混合在一起，简单平均会同等对待它们，尽管测量不同的构造。仅在你相信等权重和单维性时使用加法指数。否则，使用 EFA 来发现实际结构。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Averaging survey items is like averaging the heights of apples and oranges to measure "fruit quality." Apples and oranges might measure different things (sweetness vs. juice content), and simply averaging them loses information about their different qualities. EFA discovers which items cluster together (apples with apples, oranges with oranges).</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>平均调查项目就像平均苹果和橙子的高度来测量「水果质量」。苹果和橙子可能测量不同的东西（甜味对果汁含量），简单平均它们会丢失关于它们不同品质的信息。EFA 发现哪些项目聚集在一起（苹果与苹果，橙子与橙子）。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Exploratory Factor Analysis (EFA)</h3>
    <h3 data-lang="zh">探索性因子分析（EFA）</h3>
    <p class="method-desc" data-lang="en">EFA assumes your items are driven by latent factors you haven't observed. It estimates how much of each item's variation is explained by shared factors (communality) and how much is unique error. The goal: reduce many items into fewer underlying dimensions. You start by choosing the number of factors to retain (use scree plot, Kaiser criterion, or parallel analysis; typically you retain factors with eigenvalues > 1). Then you extract factors (principal axis factoring or ML estimation) and rotate them (varimax for orthogonal, promax for oblique) to improve interpretability. Factor loadings show how strongly each item correlates with each factor. High loadings (> 0.6) on one factor suggest items measure the same construct. After rotation, you should see a clear "simple structure": each item loads highly on one factor and lowly on others. This tells you the underlying dimensions. You can then create factor scores (weights × item scores) for each person, or simply note the items that load on each factor and average them.</p>
    <p class="method-desc" data-lang="zh">EFA 假设你的项目由你未观察到的潜在因子驱动。它估计每个项目的变化有多少由共享因子解释（公因子方差），有多少是唯一错误。目标：将许多项目简化为较少的底层维度。你首先选择要保留的因子数量（使用碎石图、Kaiser 准则或平行分析；通常你保留特征值 > 1 的因子）。然后你提取因子（主轴因子分析或 ML 估计）并旋转它们（正交的 varimax，斜交的 promax）以改进可解释性。因子载荷显示每个项目与每个因子的相关强度。一个因子上的高载荷（> 0.6）提示项目测量相同的构造。旋转后，你应该看到一个清晰的「简单结构」：每个项目在一个因子上有高载荷，在其他项目上有低载荷。这告诉你底层维度。然后你可以为每个人创建因子分数（权重 × 项目分数），或简单地注意在每个因子上加载的项目并平均它们。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> EFA is like audio mixing. You have many input channels (survey items) and you're trying to find a smaller number of underlying "tracks" (latent factors) that explain most of the signal. High correlation among items suggests they're coming from the same track; low correlation suggests different tracks. Rotating factors is like adjusting the mix to make each track's contribution clear.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>EFA 就像音频混音。你有很多输入通道（调查项目），你试图找到较少数量的底层「轨道」（潜在因子）来解释大部分信号。项目之间的高相关性表明它们来自相同的轨道；低相关性表明不同的轨道。旋转因子就像调整混音以使每个轨道的贡献清晰。</div>
    <div class="method-when" data-lang="en"><strong>When to use EFA:</strong> When you have multiple items and don't know if they measure one construct or several. When you want to discover the underlying structure. <strong>When NOT to use:</strong> When you already have a strong theory about the factor structure (use CFA instead, covered in Section 4). When you have very few items (< 3 per expected factor). Sample size matters: aim for at least 100-200 respondents; more is better.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 EFA：</strong>当你有多个项目且不知道它们是否测量一个构造或几个。当你想发现底层结构。<strong>何时不用：</strong>当你已经对因子结构有强理论时（改用 CFA，见第 4 节）。当你有很少项目（< 每个预期因子 3 个）。样本大小很重要：目标至少 100-200 受访者；越多越好。</p>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Inglehart's post-materialism index (1977). Original battery: 4 items on values (emphasize economic stability vs. self-expression). EFA revealed two underlying dimensions: materialist (economic/safety items) and post-materialist (self-expression/quality-of-life items). Inglehart created separate scales for each dimension, rather than averaging all 4 items. This discovery improved understanding of value change across societies.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>Inglehart 的后物质主义指数（1977）。原始问卷：4 个关于价值观的项目（强调经济稳定对比自我表达）。EFA 显示了两个底层维度：唯物主义（经济/安全项目）和后物质主义（自我表达/生活质量项目）。Inglehart 为每个维度创建了单独的量表，而不是平均所有 4 个项目。这个发现改进了对社会中价值观变化的理解。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">EFA vs. PCA: Know the Difference</h3>
    <h3 data-lang="zh">EFA 对 PCA：了解区别</h3>
    <p class="method-desc" data-lang="en">Many software packages default to PCA (Principal Components Analysis) when you ask for factor analysis. They're different. PCA finds linear combinations of your items that maximize explained variance. It doesn't assume latent factors. EFA assumes latent factors exist and tries to estimate them. For measurement (creating indices), use EFA, not PCA. PCA is useful for dimension reduction (e.g., if you have 50 variables and need 5 to feed into a regression), but it's not ideal for discovering latent constructs. EFA explicitly models measurement error (each item has unique error); PCA does not. If your goal is creating a psychological scale or validating a construct, use EFA. If your goal is reducing dimensionality for computational efficiency, PCA is fine.</p>
    <p class="method-desc" data-lang="zh">许多软件包在你要求因子分析时默认为 PCA（主成分分析）。他们不同。PCA 找到你的项目的线性组合，最大化解释的方差。它不假设潜在因子。EFA 假设潜在因子存在并尝试估计它们。对于测量（创建指数），使用 EFA，不是 PCA。PCA 对于降维很有用（例如，如果你有 50 个变量，需要 5 个来输入回归），但对于发现潜在构造不理想。EFA 明确模拟测量误差（每个项目有唯一错误）；PCA 没有。如果你的目标是创建心理量表或验证构造，使用 EFA。如果你的目标是为了计算效率降低维度，PCA 很好。</p>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 3 -->
<div class="section" id="ms-s3">
  <h2 data-lang="en">How Do I Know If My Measure Is Reliable and Valid?</h2>
  <h2 data-lang="zh">我怎么知道我的测量是否可靠且有效？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You've created an index of political trust from 5 survey items. But is it reliable? (Does it consistently measure the same construct, or does it fluctuate due to noise?) Is it valid? (Does it actually measure political trust, or something else?) Reliability is about consistency; validity is about measuring what you claim. Both matter. A bathroom scale can be reliable (gives same reading each time) but invalid (actually measures your shoe weight, not your body weight). You need statistics to evaluate both.</p>
    <p data-lang="zh"><strong>问题：</strong>你从 5 个调查项目创建了政治信任的指数。但它可靠吗？（它是否一致地测量相同的构造，或由于噪音而波动？）它有效吗？（它是否实际测量政治信任或其他东西？）信度是关于一致性；效度是关于测量你声称的东西。两者都很重要。浴室秤可以是可靠的（每次给出相同的读数）但无效的（实际上测量你的鞋重，不是你的体重）。你需要统计来评估两者。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Reliability: Internal Consistency and Test-Retest</h3>
    <h3 data-lang="zh">信度：内部一致性与测试-重测</h3>
    <p class="method-desc" data-lang="en">Internal consistency: do your items correlate with each other? If you measure political trust with 5 items, they should correlate. Low correlations suggest they're measuring different things. Cronbach's alpha measures this: α = k × [mean correlation / (1 + (k−1) × mean correlation)] where k = number of items. Standard rule: α > 0.7 is acceptable, α > 0.8 is good, α > 0.9 is very good (but can indicate redundancy). However, α is not the final word on reliability. High α requires many moderately correlated items; low α can result from too few items, not truly low reliability. Test-retest reliability: give your scale to the same people at two time points. If they answer similarly, the scale is reliable. This requires waiting (weeks or months between test and retest). Use this when you want to show stability. Inter-rater reliability (Cohen's kappa): if coding involves human judgment (e.g., coding open-ended responses), have multiple coders rate the same data. If they agree (kappa > 0.6), reliability is good.</p>
    <p class="method-desc" data-lang="zh">内部一致性：你的项目是否相互相关？如果你用 5 个项目测量政治信任，它们应该相关。低相关性表明它们在测量不同的东西。Cronbach's alpha 测量这一点：α = k × [平均相关 / (1 + (k−1) × 平均相关)]，其中 k = 项目数。标准规则：α > 0.7 是可接受的，α > 0.8 是好的，α > 0.9 是很好的（但可能表示冗余）。然而，α 不是信度的最后定论。高 α 需要许多中等相关的项目；低 α 可能来自项目太少，不是真正低信度。测试-重测信度：在两个时间点将你的量表给相同的人。如果他们类似地回答，量表是可靠的。这需要等待（测试和重测之间的几周或几个月）。当你想显示稳定性时使用这个。评分者间信度（Cohen's kappa）：如果编码涉及人工判断（例如，编码开放式回答），让多个编码者评分相同的数据。如果他们同意（kappa > 0.6），信度很好。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Internal consistency is like asking: "If I use the same scale on the same person three times today, do I get similar answers?" Test-retest is asking: "If I use the scale on the same person today and in three months, do I get similar answers?" A good measure should pass both tests.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>内部一致性就像问：「如果我今天在同一个人上使用相同的量表三次，我会得到相似的答案吗？」测试-重测是问：「如果我今天和三个月后在同一个人上使用量表，我会得到相似的答案吗？」一个好的测量应该通过两个测试。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Validity: Criterion, Convergent, and Discriminant</h3>
    <h3 data-lang="zh">效度：标准、收敛与判别</h3>
    <p class="method-desc" data-lang="en">Criterion validity: does your measure predict related outcomes? A political knowledge scale should predict news consumption, voting participation, and informed voting. Correlate your scale with these outcomes. High correlations (r > 0.3) suggest criterion validity. Convergent validity: does your measure correlate with other measures of the same construct? If you create a new political trust scale, it should correlate with existing trust scales (r > 0.5). Discriminant validity: does your measure NOT correlate with unrelated constructs? Your trust scale should not predict height, shoe size, or income (unless theory says otherwise). Low correlations with unrelated constructs (r < 0.3) suggest discriminant validity. Validity is harder to establish than reliability — it requires external evidence, and reasonable people can disagree on whether a measure captures its intended construct.</p>
    <p class="method-desc" data-lang="zh">标准效度：你的测度是否预测相关的结果？政治知识量表应预测新闻消费、投票参与和知情投票。将你的量表与这些结果关联。高相关性（r > 0.3）建议标准效度。收敛效度：你的测度是否与相同构造的其他测度相关？如果你创建一个新的政治信任量表，它应该与现有信任量表相关（r > 0.5）。判别效度：你的测度是否不与无关构造相关？你的信任量表不应预测身高、鞋码或收入（除非理论另有说明）。与无关构造的低相关性（r < 0.3）建议判别效度。效度比信度更难建立——它需要外部证据，理性的人可能对测度是否捕获其预期构造有分歧。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Criterion validity is like asking "Does my political knowledge scale predict who votes and engages with news?" Convergent validity is "Does it correlate with other knowledge scales?" Discriminant validity is "Does it fail to predict unrelated things like height?" All three together establish that you're measuring knowledge, not luck or demographics.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>标准效度就像问「我的政治知识量表是否预测谁投票和参与新闻？」收敛效度是「它是否与其他知识量表相关？」判别效度是「它是否未能预测无关的东西如身高？」三者一起确立了你在测量知识，不是运气或人口统计。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Problem with α as the Only Reliability Benchmark</h3>
    <h3 data-lang="zh">仅用 α 作为唯一信度基准的问题</h3>
    <p class="method-desc" data-lang="en">Cronbach's alpha has become the default reliability statistic, but it has limits. High α requires many items with moderate intercorrelations; a scale with 3 highly correlated items might have low α (say, 0.6) simply because there are few items, not because reliability is low. Conversely, adding redundant items artificially inflates α without improving measurement. Modern alternatives include McDonald's omega (ω), which accommodates different factor loadings, and composite reliability. If you publish a scale, report both α and at least one alternative. Report how α varies if you remove one item at a time (item-total correlation). A scale is strong if removing any one item doesn't substantially lower α.</p>
    <p class="method-desc" data-lang="zh">Cronbach's alpha 已成为默认的信度统计，但它有限制。高 α 需要许多项目具有中等的相互相关；一个有 3 个高度相关项目的量表可能有低 α（比如，0.6），仅因为项目很少，不是因为信度低。相反，添加冗余项目会人为地膨胀 α 而不改进测量。现代替代方案包括 McDonald's omega（ω），它容纳不同的因子载荷，和复合信度。如果你发布一个量表，报告 α 和至少一个替代方案。报告如果你一次移除一个项目，α 如何变化（项目-总相关）。如果移除任何一个项目都不会显著降低 α，则量表是强的。</p>
  </div>

</div>

<hr class="section-divider">

<!-- SECTION 4 -->
<div class="section" id="ms-s4">
  <h2 data-lang="en">How Do I Build a New Scale From Scratch?</h2>
  <h2 data-lang="zh">我怎么从零构建一个新量表？</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You want to create a new measure of political cynicism. No existing validated scale fits your needs. You must develop one from the ground up: theory-driven item generation, pilot testing, factor analysis, confirmation with new data, and validation across groups. This is a multi-stage process. Done right, it produces a scale others can trust and use. Done poorly, it's just a arbitrary collection of questions.</p>
    <p data-lang="zh"><strong>问题：</strong>你想创建一个政治犬儒主义的新度量。没有现有的验证量表适合你的需要。你必须从头开始开发它：理论驱动的项目生成、试点测试、因子分析、用新数据确认和跨组验证。这是一个多阶段的过程。做得好，它产生一个其他人可以信任和使用的量表。做得不好，它只是一个任意的问题集合。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Scale Development Process</h3>
    <h3 data-lang="zh">量表开发过程</h3>
    <p class="method-desc" data-lang="en">Step 1: Theory. Define cynicism. Literature review. What does cynicism include? Distrust in institutions? Belief that politicians are self-interested? Hopelessness about change? Step 2: Item generation. Write 20–30 candidate items based on your definition. Review with experts. Remove duplicates and unclear items. Step 3: Pilot study. Administer items to 100–300 respondents. Compute item-total correlations and Cronbach's alpha. Remove items with low correlations (< 0.3) or low item-alpha if deleted. Step 4: EFA. Use the pilot data to discover factor structure. Do items cluster into cynicism? Or do they split into sub-dimensions (institutional cynicism, efficacy cynicism)? Retain factors with eigenvalues > 1. Step 5: New sample. Collect fresh data from a different sample (new respondents, different context). Step 6: CFA (Confirmatory Factor Analysis). Test whether the factor structure from EFA fits the new data. CFA is more rigorous — it tests your specific model rather than discovering structure. Model fit indices: CFI > 0.95, RMSEA < 0.06, SRMR < 0.08 indicate good fit. Step 7: Validation. Correlate your scale with external variables (political behavior, news consumption, etc.). Test convergent validity (correlates with other cynicism measures) and discriminant validity (doesn't correlate with unrelated constructs). Step 8: Measurement invariance. Test whether the scale functions the same across demographic groups (age, gender, education). If it does, your scale is generalizable.</p>
    <p class="method-desc" data-lang="zh">第 1 步：理论。定义犬儒主义。文献综述。犬儒主义包括什么？对制度的不信任？相信政治家是自利的？对变化感到无望？第 2 步：项目生成。根据你的定义写 20-30 个候选项目。与专家审查。删除重复和不清楚的项目。第 3 步：试点研究。对 100-300 受访者进行项目管理。计算项目-总相关和 Cronbach's alpha。移除相关性低（< 0.3）或删除项目-alpha 低的项目。第 4 步：EFA。使用试点数据发现因子结构。项目是否聚集到犬儒主义？或它们分裂成子维度（制度犬儒主义、效能犬儒主义）？保留特征值 > 1 的因子。第 5 步：新样本。从不同的样本（新受访者、不同背景）收集新数据。第 6 步：CFA（验证性因子分析）。测试 EFA 中的因子结构是否适合新数据。CFA 更严格——它测试你的特定模型而不是发现结构。模型拟合指标：CFI > 0.95、RMSEA < 0.06、SRMR < 0.08 表示拟合很好。第 7 步：验证。将你的量表与外部变量（政治行为、新闻消费等）关联。测试收敛效度（与其他犬儒主义测度相关）和判别效度（不与无关构造相关）。第 8 步：测量不变性。测试量表在人口统计组（年龄、性别、教育）中是否功能相同。如果是，你的量表是可推广的。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Building a new scale is like designing a new airplane. Theory is your blueprint. Item generation is manufacturing parts. EFA is test flights (discovering how the parts work together). CFA is validation tests on multiple new prototypes. External validation is real-world performance. Measurement invariance is ensuring it works across different environments. Rush any step, and the airplane crashes.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>构建新量表就像设计新飞机。理论是你的蓝图。项目生成是制造零件。EFA 是试飞（发现零件如何协同工作）。CFA 是多个新原型的验证测试。外部验证是现实性能。测量不变性是确保它在不同环境中工作。匆忙任何步骤，飞机就会坠毁。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">CFA vs. EFA: Testing vs. Discovering Structure</h3>
    <h3 data-lang="zh">CFA 对 EFA：测试对发现结构</h3>
    <p class="method-desc" data-lang="en">EFA discovers factor structure from data (data-driven). CFA tests a pre-specified factor structure (theory-driven). Use EFA on your pilot data to discover which items go together. Use CFA on new data to confirm the structure holds in independent samples. CFA fits a structural equation model (SEM) where you specify: (1) which items load on which factors, (2) whether factors correlate, and (3) whether error terms correlate. CFA then tests whether this model fits the data. If CFI > 0.95 and RMSEA < 0.06, fit is good. CFA is more conservative — it won't accept poor-fitting models even if EFA suggested the structure. This is a feature: it prevents overfitting. When developing a scale, use EFA → CFA → external validation → measurement invariance testing.</p>
    <p class="method-desc" data-lang="zh">EFA 从数据中发现因子结构（数据驱动）。CFA 测试预指定的因子结构（理论驱动）。在你的试点数据上使用 EFA 来发现哪些项目聚集在一起。在新数据上使用 CFA 来确认结构在独立样本中保持。CFA 拟合结构方程模型（SEM），其中你指定：（1）哪些项目在哪些因子上加载，（2）因子是否相关，（3）误差项是否相关。然后 CFA 测试这个模型是否适合数据。如果 CFI > 0.95 且 RMSEA < 0.06，拟合很好。CFA 更保守——即使 EFA 建议了结构，它也不会接受拟合差的模型。这是一个特性：它防止过度拟合。开发量表时，使用 EFA → CFA → 外部验证 → 测量不变性测试。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Model Fit Indices and Interpretation</h3>
    <h3 data-lang="zh">模型拟合指标与解释</h3>
    <p class="method-desc" data-lang="en">CFA reports multiple fit indices. CFI (Comparative Fit Index): compares your model to a null model (all correlations = 0). CFI > 0.95 is excellent. RMSEA (Root Mean Square Error of Approximation): smaller is better. RMSEA < 0.06 is excellent, < 0.10 acceptable. SRMR (Standardized Root Mean Square Residual): average residual (difference between observed and predicted correlations). SRMR < 0.08 is excellent. χ² test: tests whether model-implied covariances equal observed covariances. Significant χ² (p < 0.05) suggests poor fit, but χ² is overly sensitive with large samples (N > 300). Report fit indices instead. Modification indices (MI) show how much fit improves by adding new paths (e.g., allowing error terms to correlate). Be cautious: modifying a model increases false positive risk (you're hunting for good fit). Document any modifications and justify them theoretically — don't just chase better statistics.</p>
    <p class="method-desc" data-lang="zh">CFA 报告多个拟合指标。CFI（比较拟合指数）：将你的模型与空模型（所有相关性 = 0）比较。CFI > 0.95 是优秀的。RMSEA（近似均方根误差）：更小更好。RMSEA < 0.06 是优秀的，< 0.10 可接受。SRMR（标准化均方根剩余）：平均剩余（观察到和预测相关性之间的差异）。SRMR < 0.08 是优秀的。χ² 检验：测试模型隐含的协方差是否等于观察到的协方差。显著 χ²（p < 0.05）表示拟合差，但 χ² 对于大样本（N > 300）过于敏感。改为报告拟合指标。修改指标（MI）显示通过添加新路径（例如，允许误差项相关）拟合改进多少。谨慎：修改模型增加了假阳性风险（你在寻求好拟合）。记录任何修改并在理论上证明它们——不要只是追逐更好的统计。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Fit indices are like medical diagnostic tests. CFI and RMSEA are different tests for the same underlying condition (does my model fit?). You don't need all tests to agree perfectly; they should converge. Good fit (CFI > 0.95, RMSEA < 0.06) is like all tests saying "healthy." Modification indices are like a doctor suggesting "try removing salt" — it might help, but it changes the treatment plan, so document why.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>拟合指标就像医学诊断测试。CFI 和 RMSEA 是同一底层情况的不同测试（我的模型是否拟合？）。你不需要所有测试完全同意；它们应该收敛。好拟合（CFI > 0.95、RMSEA < 0.06）就像所有测试说「健康」。修改指标就像医生建议「尝试移除盐」——它可能有帮助，但它改变了处理方案，所以记录为什么。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Measurement Invariance Across Groups</h3>
    <h3 data-lang="zh">跨组的测量不变性</h3>
    <p class="method-desc" data-lang="en">Does your scale mean the same thing to everyone? Measurement invariance tests whether factor structure, loadings, and intercepts are the same across groups (gender, age, country, etc.). Start with configural invariance (factor structure is the same — same items load on same factors). Then test metric invariance (factor loadings are the same — items contribute equally). Then scalar invariance (intercepts are the same — baseline levels are comparable). Progressive constraint testing: fit a model with equal loadings; if CFI drops by more than 0.01, the constraint is too strict. If all three levels hold, your scale is invariant and can be used for group comparisons. If not, the scale might have different meanings across groups (e.g., trust items might mean something different to people from low-trust vs. high-trust societies).</p>
    <p class="method-desc" data-lang="zh">你的量表对每个人都意味着同样的东西吗？测量不变性测试因子结构、载荷和截距是否在组（性别、年龄、国家等）间相同。从配置不变性开始（因子结构相同——相同项目在相同因子上加载）。然后测试度量不变性（因子载荷相同——项目贡献相等）。然后标量不变性（截距相同——基线水平可比）。渐进约束测试：拟合具有相等加载的模型；如果 CFI 下降超过 0.01，约束太严格。如果所有三个水平成立，你的量表是不变的，可以用于组比较。如果不成立，量表可能对不同组有不同的含义（例如，信任项目对来自低信任对高信任社会的人可能意味着不同的东西）。</p>
  </div>

</div>

<hr class="section-divider">


<!-- SECTION 5 -->
<div class="section" id="ms-s5">
  <h2 data-lang="en">Item Response Theory: When Items Differ in Difficulty</h2>
  <h2 data-lang="zh">项目反应理论：当题项难度不同时</h2>

  <div class="challenge-overview">
    <p data-lang="en"><strong>The problem:</strong> You measure political knowledge with 5 questions: "Who is the Vice President?" (easy), "Which party controls the Senate?" (medium), "Name the three branches of government" (medium-hard), "What is the filibuster threshold?" (hard), "Name a recent Supreme Court ruling" (very hard). Classical Test Theory (CTT) just sums correct answers. But getting the VP question wrong means something very different than getting the filibuster question wrong. Item Response Theory (IRT) acknowledges that items differ in difficulty, discriminating power, and guessing probability, and uses these differences to estimate respondents' true latent traits more precisely.</p>
    <p data-lang="zh"><strong>问题：</strong>你用 5 个问题测量政治知识：「谁是副总统？」（简单）、「哪个政党控制参议院？」（中等）、「说出政府的三个分支」（中等难度）、「什么是冗长辩论阈值？」（困难）、「说出一个近期最高法院裁决」（非常困难）。经典测试理论（CTT）只是将正确答案相加。但回答错误副总统问题与回答错误冗长辩论问题意义大不相同。项目反应理论（IRT）承认题项在难度、辨别力和猜测概率上的不同，并利用这些差异更精确地估计受访者真正的潜在特质。</p>
  </div>

  <div class="method-section">
    <h3 data-lang="en">Classical Test Theory vs. IRT: The Core Difference</h3>
    <h3 data-lang="zh">经典测试理论对 IRT：核心区别</h3>
    <p class="method-desc" data-lang="en">In Classical Test Theory (CTT), an observed score = true score + error. This is simple but has two major limitations: (1) item properties are sample-dependent (an item is "difficult" only relative to the sample that took it), and (2) all items are treated as equally informative at all trait levels. IRT breaks both assumptions. In IRT, items have fixed properties that describe how they discriminate along the latent trait continuum — regardless of who takes the test. And importantly, items provide different amounts of information at different points on the trait continuum: a hard item gives more information about high-knowledge respondents; an easy item gives more information about low-knowledge respondents.</p>
    <p class="method-desc" data-lang="zh">在经典测试理论（CTT）中，观察分数 = 真实分数 + 误差。这很简单，但有两个主要限制：（1）项目属性依赖于样本（一个项目只是相对于接受它的样本才「困难」），和（2）所有项目被视为在所有特质水平上同等具有信息性。IRT 打破了这两个假设。在 IRT 中，项目有固定的属性，描述它们如何沿潜在特质连续体区分——与谁参加测试无关。重要的是，项目在特质连续体的不同点提供不同量的信息：困难项目提供关于高知识受访者的更多信息；简单项目提供关于低知识受访者的更多信息。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> CTT is like grading 20 students on a 10-question test by counting right answers. A student who got 7/10 on easy questions scores the same as one who got 7/10 on hard questions — but they are clearly different students. IRT is like a personalized quiz: easy questions tell you a lot about low-ability students; hard questions tell you more about high-ability students. The scoring system accounts for which items you got right, not just how many.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>CTT 就像通过计算正确答案为 10 题测试的 20 名学生打分。在简单问题上得 7/10 的学生与在困难问题上得 7/10 的学生得分相同——但他们显然是不同的学生。IRT 就像个性化测验：简单问题告诉你很多关于低能力学生的事情；困难问题告诉你更多关于高能力学生的事情。评分系统考虑你答对了哪些项目，不仅仅是多少。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The Rasch Model (1PL): One Parameter</h3>
    <h3 data-lang="zh">Rasch 模型（1PL）：单参数</h3>
    <p class="method-desc" data-lang="en">The simplest IRT model: Rasch (1PL, one-parameter logistic). For dichotomous items (correct/incorrect), the probability of a correct response is: P(correct | θ, b) = logistic(θ − b). Where θ is the respondent's latent trait (ability, knowledge, ideology) and b is the item difficulty parameter. Higher θ = smarter respondent. Higher b = harder item. If θ = b (ability equals difficulty), probability of correct = 0.5. The Rasch model assumes all items discriminate equally — what differs is only their difficulty. This makes it parsimonious but restrictive: real items often differ in how strongly they discriminate.</p>
    <p class="method-desc" data-lang="zh">最简单的 IRT 模型：Rasch（1PL，单参数 logistic）。对于二分项目（正确/错误），正确响应的概率为：P(正确 | θ, b) = logistic(θ − b)。其中 θ 是受访者的潜在特质（能力、知识、意识形态）和 b 是项目难度参数。更高的 θ = 更聪明的受访者。更高的 b = 更难的项目。如果 θ = b（能力等于难度），正确概率 = 0.5。Rasch 模型假设所有项目的辨别力相同——不同的只是它们的难度。这使它简洁但有限制：真实项目通常在辨别力上不同。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Each question on a quiz has a "difficulty level" (b). A student's knowledge (θ) determines how they fare. A very knowledgeable student easily clears even hard questions. A less knowledgeable one fails them. The Rasch model assumes all questions are equally "sharp" at distinguishing ability — they just sit at different points on the difficulty scale.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>测验中的每个问题都有一个「难度级别」（b）。学生的知识（θ）决定了他们的表现。非常有知识的学生轻松通过即使是困难的问题。知识少的人会失败。Rasch 模型假设所有问题在区分能力方面同样「锋利」——它们只是在难度刻度上处于不同的点。</div>
    <div class="method-when" data-lang="en"><strong>When to use the Rasch model:</strong> When you have strong theoretical reasons to believe all items are equally discriminating. When you want to calibrate items for item banks (adaptive testing). When your items are designed to be parallel (same format, same difficulty range). Test Rasch fit by examining item fit statistics (χ² or infit/outfit statistics); misfitting items violate the equal discrimination assumption. Use the TAM package in R or ltm for Rasch estimation.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 Rasch 模型：</strong>当你有充分的理论理由相信所有项目的辨别力相同时。当你想为题库（自适应测试）校准项目时。当你的项目设计为平行（相同格式、相同难度范围）。通过检查项目拟合统计（χ² 或 infit/outfit 统计）来测试 Rasch 拟合；拟合差的项目违反了等辨别力假设。使用 R 中的 TAM 包或 ltm 进行 Rasch 估计。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Measuring political sophistication from a national survey. 10 political knowledge items ranging from easy ("Name the Speaker of the House") to hard ("What is the Filibuster threshold?"). A Rasch model estimates each item's difficulty (b) and each respondent's knowledge level (θ). This reveals that many respondents cluster in the "low knowledge" zone, and the measurement instrument provides very little information at the high end (no sufficiently hard items). This insight leads to adding harder items to improve measurement at the top of the distribution.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>从全国调查测量政治成熟度。10 个政治知识项目，从简单（「说出众议院议长的名字」）到困难（「什么是冗长辩论阈值？」）。Rasch 模型估计每个项目的难度（b）和每个受访者的知识水平（θ）。这揭示了许多受访者集中在「低知识」区域，测量工具在高端（没有足够困难的项目）提供的信息很少。这一洞见导致添加更难的项目以改善在分布顶部的测量。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">The 2PL Model: Adding Discrimination</h3>
    <h3 data-lang="zh">2PL 模型：加入辨别力</h3>
    <p class="method-desc" data-lang="en">The two-parameter logistic (2PL) model adds a discrimination parameter (a) for each item: P(correct | θ, a, b) = logistic(a(θ − b)). Higher a = steeper item characteristic curve (ICC) = the item more sharply differentiates between high- and low-ability respondents. Higher b = the item is harder. In practice, some political knowledge questions discriminate very sharply (only the very knowledgeable answer them correctly); others are flatter (all ability levels have moderate probability of getting them right). The 2PL captures this. The Item Characteristic Curve (ICC) plots P(correct) against θ for each item. Items with steep ICCs (high a) are most useful for distinguishing respondents near b. Items with flat ICCs (low a) don't help much regardless of θ. A poorly written question might have near-zero discrimination — remove it.</p>
    <p class="method-desc" data-lang="zh">双参数 logistic（2PL）模型为每个项目添加了辨别力参数（a）：P(正确 | θ, a, b) = logistic(a(θ − b))。更高的 a = 更陡的项目特征曲线（ICC）= 项目更锋利地区分高能力和低能力的受访者。更高的 b = 项目更难。在实践中，一些政治知识问题辨别力非常强（只有非常有知识的人才能正确回答它们）；其他问题更平坦（所有能力水平都有适度的正确概率）。2PL 捕捉了这一点。项目特征曲线（ICC）绘制每个项目的 P(正确) 对 θ。具有陡峭 ICC（高 a）的项目对于区分 b 附近的受访者最有用。具有平坦 ICC（低 a）的项目无论 θ 如何都帮助不大。写得不好的问题可能辨别力接近零——移除它。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> Discrimination (a) is like the sharpness of a sorter. A highly discriminating item is like a sieve with a precise mesh size: exactly the right knowledge level gets you through. A low-discrimination item is like a loose net: many ability levels can pass. When measuring political knowledge, "Name the VP" (everyone knows it) and "Explain marginal revenue" (no one knows it in a mass survey) both have low discrimination. Items in the middle range, answered by only highly knowledgeable respondents, have high discrimination.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>辨别力（a）就像分拣机的锋利程度。高辨别力的项目就像具有精确网格尺寸的筛子：正好合适的知识水平才能通过。低辨别力的项目就像宽松的网：许多能力水平都能通过。在测量政治知识时，「说出副总统名字」（所有人都知道）和「解释边际收入」（在大众调查中没有人知道）两者的辨别力都低。只有高知识受访者回答的中间范围项目具有高辨别力。</div>
    <div class="method-when" data-lang="en"><strong>When to use 2PL:</strong> When you expect items to differ in how strongly they discriminate. More realistic than Rasch for most social science surveys. Requires larger samples (~200–500 respondents per item for stable estimation). Use the ltm or mirt package in R. Compare 2PL fit to Rasch using likelihood ratio tests (LRT); if the 2PL significantly improves fit, use it. <strong>3PL model</strong> adds a guessing parameter (c): useful for multiple-choice knowledge tests where guessing is likely. Rarely necessary for attitude/ideology scales. Always plot ICCs and inspect item information functions to understand where each item contributes most measurement precision.</div>
    <div class="method-when" data-lang="zh"><strong>何时用 2PL：</strong>当你期望项目在辨别力上不同时。对于大多数社会科学调查比 Rasch 更现实。需要更大的样本（~每个项目 200-500 受访者以进行稳定估计）。使用 R 中的 ltm 或 mirt 包。使用似然比检验（LRT）将 2PL 拟合与 Rasch 比较；如果 2PL 显著改进拟合，使用它。<strong>3PL 模型</strong>添加猜测参数（c）：对于可能猜测的多选知识测试很有用。对于态度/意识形态量表很少必要。始终绘制 ICC 并检查项目信息函数，以了解每个项目在哪里贡献最多的测量精度。</div>
    <div class="method-example" data-lang="en"><strong>Social science example:</strong> Estimating legislative ideology from congressional roll-call votes using IRT (W-NOMINATE, DW-NOMINATE, or Bayesian IRT). Each vote is an "item." The yes/no vote is the response. The legislator's ideology (θ) is the latent trait. The 2PL estimates: (1) item difficulty = how ideologically centrist or extreme a vote is (a vote where all members agree has low discrimination; a contentious partisan vote has high discrimination), (2) item discrimination = how sharply the vote separates liberals from conservatives. The result: a continuous ideology score for every legislator on a liberal-conservative dimension, with uncertainty estimates. This is exactly the DW-NOMINATE measure used throughout political science.</div>
    <div class="method-example" data-lang="zh"><strong>社会科学例子：</strong>使用 IRT 从国会点名投票估计立法意识形态（W-NOMINATE、DW-NOMINATE 或贝叶斯 IRT）。每次投票是一个「项目」。赞成/反对投票是响应。立法者的意识形态（θ）是潜在特质。2PL 估计：（1）项目难度 = 投票在意识形态上有多中心或极端（所有成员同意的投票辨别力低；有争议的党派投票辨别力高），（2）项目辨别力 = 投票如何锋利地将自由派与保守派分开。结果：每个立法者在自由-保守维度上的连续意识形态分数，带有不确定性估计。这正是整个政治学中使用的 DW-NOMINATE 测量。</div>
  </div>

  <div class="method-section">
    <h3 data-lang="en">IRT Advantages: Scale-Free Measurement and Adaptive Testing</h3>
    <h3 data-lang="zh">IRT 优势：无刻度测量与自适应测试</h3>
    <p class="method-desc" data-lang="en">IRT's biggest advantage: person and item parameters are on the same scale (the latent trait scale θ), and item parameters are independent of the sample. This allows: (1) Item banking: develop large pools of calibrated items; use different subsets for different respondents without losing comparability. (2) Computer Adaptive Testing (CAT): select the next question based on the respondent's current estimated θ — ask harder questions to those who've answered correctly so far, easier ones to those who've struggled. This reduces test length without sacrificing measurement precision. (3) Cross-study comparison: because item parameters are sample-independent, you can equate scales from different surveys if they share some anchor items. In political science, IRT bridges different survey datasets to create long-term trend estimates.</p>
    <p class="method-desc" data-lang="zh">IRT 最大的优势：人员和项目参数在同一刻度（潜在特质刻度 θ）上，项目参数独立于样本。这允许：（1）题库：开发大型校准项目池；为不同受访者使用不同子集而不损失可比性。（2）计算机自适应测试（CAT）：根据受访者当前估计的 θ 选择下一个问题——向目前答对的人提问更难的问题，向挣扎的人提问更简单的问题。这在不牺牲测量精度的情况下减少测试长度。（3）跨研究比较：由于项目参数与样本无关，如果调查共享一些锚项目，你可以从不同调查等化刻度。在政治学中，IRT 桥接不同的调查数据集来创建长期趋势估计。</p>
    <div class="method-analogy" data-lang="en"><strong>Analogy:</strong> IRT is like Celsius and Fahrenheit having a known conversion formula — different thermometers (items) produce comparable readings (scores) because the underlying temperature scale (θ) is the same for everyone. CTT is like thermometers without units — you can say "one thermometer reads higher," but you can't compare readings across different thermometer brands.</div>
    <div class="method-analogy" data-lang="zh"><strong>类比：</strong>IRT 就像摄氏度和华氏度有一个已知的转换公式——不同的温度计（项目）产生可比较的读数（分数），因为底层温度刻度（θ）对每个人都相同。CTT 就像没有单位的温度计——你可以说「一个温度计读数更高」，但你无法比较不同温度计品牌的读数。</div>
    <div class="method-when" data-lang="en"><strong>When IRT is worth the complexity:</strong> When you have 10+ items and large samples (N > 300). When you need to compare across subgroups with potentially different item functioning (use Differential Item Functioning, DIF, tests to check if items work differently for different groups — if so, those items cannot be used for group comparisons). When building reusable scales or item banks. When your outcome of interest is a latent construct with known range. <strong>Use CTT instead when:</strong> You have very few items, small samples, or your items are more formative (reflecting rather than caused by a latent trait).</div>
    <div class="method-when" data-lang="zh"><strong>IRT 何时值得复杂性：</strong>当你有 10 个以上的项目和大样本（N > 300）。当你需要跨具有潜在不同项目功能的子组进行比较时（使用差异项目功能 DIF 检验来检查项目是否对不同组的工作方式不同——如果是，这些项目不能用于组比较）。当构建可重用量表或题库时。当你的感兴趣结果是具有已知范围的潜在构造时。<strong>何时改用 CTT：</strong>当你有很少的项目、小样本，或你的项目更多是形成性的（反映而不是由潜在特质引起）。</div>
  </div>

</div>

<hr class="section-divider">
<!-- GUIDES -->
<div class="section" id="ms-guides">
  <h2 data-lang="en">Guides</h2>
  <h2 data-lang="zh">配套指南</h2>
  <div class="m-list">
    <a class="m-card" href="/methods/guides/reg-meas-irt.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">IRT Item Characteristic Curve Explorer</div>
        <div class="m-title" data-lang="zh">IRT 项目特征曲线探索器</div>
        <div class="m-desc" data-lang="en">Interactive ICC visualization for Rasch and 2PL models — adjust discrimination and difficulty.</div>
        <div class="m-desc" data-lang="zh">Rasch 与 2PL 模型 ICC 可视化——调整辨别力与难度参数实时查看。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
    <a class="m-card" href="/methods/guides/reg-meas-efa.html">
      <div class="m-num">▶</div>
      <div class="m-info">
        <div class="m-title" data-lang="en">EFA/CFA in R: psych & lavaan</div>
        <div class="m-title" data-lang="zh">EFA/CFA R 实战</div>
        <div class="m-desc" data-lang="en">Complete scale development workflow from EFA to CFA with fit indices and code.</div>
        <div class="m-desc" data-lang="zh">从 EFA 到 CFA 的完整量表开发流程，含拟合指标与 R 代码。</div>
      </div>
      <div class="m-arrow">→</div>
    </a>
  </div>
</div>

<hr class="section-divider">
<!-- RESOURCES -->
<hr class="section-divider">
<div class="section" id="ms-resources">
  <h2 data-lang="en">Resources</h2>
  <h2 data-lang="zh">资源</h2>
  <div class="method-section">
    <p class="method-desc" data-lang="en"><strong>Scale development standards:</strong> DeVellis (2016) on scale development methodology; Churchill (1979) classic framework. <strong>EFA/CFA:</strong> Fabrigar et al. (1999) on EFA; Kline (2015) on SEM and CFA. <strong>Reliability:</strong> Cronbach (1951); McDonald (1999) on omega. <strong>IRT:</strong> Embretson & Reise (2000) <em>Item Response Theory for Psychologists</em>; Hambleton, Swaminathan & Rogers (1991) for classic coverage. In political science: Clinton, Jackman & Rivers (2004) on Bayesian IRT for roll-call votes; Treier & Jackman (2008) on IRT for democracy indices. <strong>Software:</strong> R packages: psych (EFA), lavaan (CFA/SEM), ltm/mirt/TAM (IRT), mediation. Stata: factor (EFA), sem (CFA), rasch. Always report: (1) item development process, (2) sample characteristics, (3) factor structure (EFA + CFA), (4) reliability (α, ω), (5) validity evidence (convergent, discriminant, criterion), (6) measurement invariance across groups, (7) IRT model fit and item information functions if using IRT.</p>
    <p class="method-desc" data-lang="zh"><strong>量表开发标准：</strong>DeVellis (2016) 关于量表开发方法论；Churchill (1979) 经典框架。<strong>EFA/CFA：</strong>Fabrigar et al. (1999) 关于 EFA；Kline (2015) 关于 SEM 和 CFA。<strong>信度：</strong>Cronbach (1951)；McDonald (1999) 关于 omega。<strong>IRT：</strong>Embretson & Reise (2000) <em>心理学家的项目反应理论</em>；Hambleton, Swaminathan & Rogers (1991) 提供经典覆盖。在政治学中：Clinton, Jackman & Rivers (2004) 关于点名投票的贝叶斯 IRT；Treier & Jackman (2008) 关于民主指数的 IRT。<strong>软件：</strong>R 包：psych（EFA）、lavaan（CFA/SEM）、ltm/mirt/TAM（IRT）。Stata：factor（EFA）、sem（CFA）、rasch。总是报告：（1）项目开发过程、（2）样本特征、（3）因子结构（EFA + CFA）、（4）信度（α、ω）、（5）效度证据（收敛、判别、标准）、（6）跨组测量不变性、（7）如果使用 IRT，则 IRT 模型拟合和项目信息函数。</p>
  </div>
</div>

<!-- PAGE NAV -->
<div class="page-nav">
  <a class="pn-link pn-prev" href="/methods/reg-causal.html">
    <span class="pn-arrow">&larr;</span>
    <span><span class="pn-title">Causal Inference</span></span>
  </a>
  <a class="pn-link pn-next" href="/methods/reg-timeseries.html">
    <span><span class="pn-title">Time Series & Panel Data</span></span>
    <span class="pn-arrow">&rarr;</span>
  </a>
</div>
