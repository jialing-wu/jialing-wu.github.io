<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Text as Data — Research Methods Notebook</title>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400;1,500&family=EB+Garamond:ital,wght@0,400;0,500;1,400&family=Caveat:wght@400;500&family=IBM+Plex+Sans:wght@300;400;500&family=IBM+Plex+Mono:wght@400;500&family=Noto+Serif+SC:wght@300;400;500;600&family=Noto+Sans+SC:wght@300;400;500&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">
<style>
.info-box{border:1px solid var(--parchment);border-radius:4px;padding:16px 18px;margin:16px 0;background:var(--warm)}
.info-box .label{font-family:var(--sans);font-size:10px;font-weight:600;letter-spacing:.1em;text-transform:uppercase;color:var(--red);margin-bottom:8px}
.info-box p{font-size:14px;line-height:1.7;color:var(--ink-faded);margin-bottom:0}
.article-box{border:1px solid var(--parchment);border-radius:4px;padding:18px;margin:12px 0;background:var(--paper)}
.article-method{font-family:var(--sans);font-size:11px;font-weight:600;letter-spacing:.06em;text-transform:uppercase;color:var(--gold);margin-top:8px}
.sw-row{display:grid;grid-template-columns:repeat(3,1fr);gap:12px;margin:12px 0}
.sw-pill{border:1px solid var(--parchment);border-radius:4px;padding:14px;background:var(--paper)}
.sw-pill.gold{background:var(--ink);color:rgba(248,244,236,.85);border-color:var(--ink)}
.sw-pill h4{font-family:var(--sans);font-size:13px;font-weight:600;margin-bottom:4px}
.sw-pill.gold h4{color:var(--gold)}
.sw-pill p{font-size:12.5px;line-height:1.5;color:var(--ink-faded);margin:0}
.sw-pill.gold p{color:rgba(248,244,236,.7)}
.pipeline{display:flex;align-items:stretch;gap:0;margin:20px 0;overflow-x:auto}
.pipe-step{flex:1;text-align:center;padding:16px 10px;border:1px solid var(--parchment);background:var(--paper);position:relative;min-width:110px}
.pipe-step:first-child{border-radius:4px 0 0 4px}
.pipe-step:last-child{border-radius:0 4px 4px 0}
.pipe-step:not(:last-child){border-right:none}
.pipe-step.active{background:var(--ink);border-color:var(--ink);color:var(--paper)}
.pipe-num{font-family:var(--sans);font-size:10px;font-weight:600;letter-spacing:.1em;color:var(--gold);margin-bottom:4px}
.pipe-step.active .pipe-num{color:var(--gold)}
.pipe-title{font-family:var(--sans);font-size:11px;font-weight:600;color:var(--ink);line-height:1.3}
.pipe-step.active .pipe-title{color:var(--paper)}
.pipe-desc{font-size:11px;color:var(--ink-ghost);margin-top:4px;line-height:1.4}
.pipe-step.active .pipe-desc{color:rgba(248,244,236,.6)}
.method-card{border:1px solid var(--parchment);border-radius:4px;padding:20px;margin:10px 0;background:var(--paper);transition:all .2s}
.method-card:hover{border-color:var(--tea);background:var(--warm)}
.method-card h4{font-family:var(--sans);font-size:13px;font-weight:600;color:var(--ink);margin-bottom:6px}
.method-card .mc-tag{font-family:var(--sans);font-size:9px;font-weight:600;letter-spacing:.08em;text-transform:uppercase;padding:2px 8px;border-radius:10px;color:var(--paper);display:inline-block;margin-bottom:8px}
.mc-tag.unsup{background:#5a7a6b}
.mc-tag.sup{background:var(--red)}
.mc-tag.rep{background:var(--leather)}
.mc-tag.dl{background:var(--gold)}
.mc-tag.scale{background:var(--ink-ghost)}
.mc-tag.anno{background:var(--leather)}
.method-card p{font-size:13.5px;line-height:1.65;color:var(--ink-faded);margin-bottom:6px}
.method-card .formula{font-family:var(--mono);font-size:12px;color:var(--ink-soft);background:var(--warm);padding:8px 12px;border-radius:3px;margin:6px 0;line-height:1.5;overflow-x:auto;white-space:nowrap;border:1px solid rgba(222,212,192,.5)}
.method-card .when{font-family:var(--sans);font-size:12px;color:var(--gold);margin-top:8px;padding:6px 10px;background:rgba(194,153,61,.06);border-radius:3px;line-height:1.4}
.method-card .when strong{color:var(--leather)}
.method-card .analogy{margin:8px 0;padding:10px 14px;border-radius:4px;background:rgba(194,153,61,.06);border:1px solid rgba(194,153,61,.15);font-size:13px;line-height:1.65;color:var(--ink-faded);font-style:italic}
.method-card .eg{margin:8px 0;padding:10px 14px;border-radius:4px;background:rgba(181,55,42,.04);border:1px solid rgba(181,55,42,.1);font-size:13px;line-height:1.65;color:var(--ink-faded)}
.method-card .eg::before{content:'';display:inline-block;width:6px;height:6px;background:var(--red);border-radius:50%;margin-right:8px;vertical-align:middle}
@media(max-width:860px){
  .sw-row{grid-template-columns:1fr}
  .pipeline{flex-wrap:wrap}
  .pipe-step{min-width:auto;border-radius:4px!important;border:1px solid var(--parchment)!important;margin:3px}
}
</style>
</head>
<body class="zh">
<div class="layout">
  <aside class="sidebar" id="sidebar">
    <a class="sb-brand" href="index.html"><h2>Methods <span>Notebook</span></h2><div class="sb-sub">Jialing Wu</div></a>
    <div class="sb-cat">Person-Centered Quantitative Methods</div>
    <a class="sb-link" href="lpa.html"><span class="sb-num">01</span> Latent Profile Analysis</a>
    <div class="sb-cat">Computational Social Science</div>
    <div class="sb-subcat">Foundations</div>
    <a class="sb-link" href="machine-learning.html"><span class="sb-num">01</span> Machine Learning</a>
    <a class="sb-link" href="llm.html"><span class="sb-num">02</span> LLM &amp; NLP</a>
    <a class="sb-link active" href="text-analysis.html"><span class="sb-num">03</span> Text as Data</a>
    <a class="sb-link" href="theoretical-modeling.html"><span class="sb-num">04</span> Theoretical Modeling</a>
    <div class="sb-cat">Statistics</div>
    <div class="sb-subcat">Foundations</div>
    <a class="sb-link" href="empirical-modeling.html"><span class="sb-num">01</span> Empirical Modeling</a>
    <div class="sb-footer"><a href="https://jialing-wu.github.io">&larr; My Website</a></div>
  </aside>
  <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.toggle('open');document.getElementById('overlay').classList.toggle('show')"></div>
  <div class="main">
    <div class="topbar">
      <button class="menu-toggle" onclick="document.getElementById('sidebar').classList.toggle('open');document.getElementById('overlay').classList.toggle('show')"><span></span></button>
      <div class="breadcrumb">Computational Social Science<span class="sep">/</span> Foundations<span class="sep">/</span> Text as Data</div>
      <div class="topbar-lang">
        <button class="lang-btn" id="btn-zh" onclick="setLang('zh')">中文</button>
        <button class="lang-btn" id="btn-en" onclick="setLang('en')">EN</button>
      </div>
    </div>
    <div class="content">

      <!-- HEADER -->
      <div class="method-header">
        <h1>Text as Data</h1>
        <div class="method-meta">Computational Social Science &middot; Foundations 03</div>
        <div data-lang="en"><p class="subtitle">Notes from ICPSR 2024 "Data Science &amp; Text Analysis", University of Michigan &middot; <a href="https://www.polisci.pitt.edu/people/yaoyao-dai" target="_blank" style="color:var(--red);border-bottom:1px solid var(--red)">Prof. Yaoyao Dai</a></p></div>
        <div data-lang="zh"><p class="subtitle">笔记整理自 ICPSR 2024「Data Science &amp; Text Analysis」, University of Michigan &middot; <a href="https://www.polisci.pitt.edu/people/yaoyao-dai" target="_blank" style="color:var(--red);border-bottom:1px solid var(--red)">Prof. Yaoyao Dai</a></p></div>
      </div>


      <!-- ========== PART I: TEXT ANALYSIS PIPELINE ========== -->

      <div class="section">
        <h2 data-lang="en">The Text Analysis Pipeline</h2>
        <h2 data-lang="zh">文本分析流程</h2>
        <div data-lang="en">
          <p>Imagine you have 10,000 newspaper articles and want to find out what topics they discuss. You can't read them all — but a computer can, if we first turn words into numbers. That's what text analysis does: it converts messy human language into structured data a computer can crunch. The process follows a clear pipeline: collect text, clean it up, turn it into numbers, analyze it, and validate the results.</p>
        </div>
        <div data-lang="zh">
          <p>假设你手上有 10,000 篇新闻报道，想知道它们都在讨论什么话题。你没法一篇篇读完——但计算机可以，前提是我们先把文字变成数字。这就是文本分析做的事：把杂乱的人类语言转化为计算机能处理的结构化数据。整个过程遵循一条清晰的流水线：收集文本、清洗整理、转化为数字、分析建模、验证结果。</p>
        </div>

        <div class="pipeline">
          <div class="pipe-step">
            <div class="pipe-num">01</div>
            <div class="pipe-title" data-lang="en">Collect</div>
            <div class="pipe-title" data-lang="zh">数据收集</div>
            <div class="pipe-desc" data-lang="en">APIs, scraping, corpora</div>
            <div class="pipe-desc" data-lang="zh">API、爬虫、语料库</div>
          </div>
          <div class="pipe-step active">
            <div class="pipe-num">02</div>
            <div class="pipe-title" data-lang="en">Preprocess</div>
            <div class="pipe-title" data-lang="zh">预处理</div>
            <div class="pipe-desc" data-lang="en">Tokenize, clean, normalize</div>
            <div class="pipe-desc" data-lang="zh">分词、清洗、标准化</div>
          </div>
          <div class="pipe-step">
            <div class="pipe-num">03</div>
            <div class="pipe-title" data-lang="en">Represent</div>
            <div class="pipe-title" data-lang="zh">特征表示</div>
            <div class="pipe-desc" data-lang="en">BoW, TF-IDF, embeddings</div>
            <div class="pipe-desc" data-lang="zh">词袋、TF-IDF、词嵌入</div>
          </div>
          <div class="pipe-step">
            <div class="pipe-num">04</div>
            <div class="pipe-title" data-lang="en">Analyze</div>
            <div class="pipe-title" data-lang="zh">分析建模</div>
            <div class="pipe-desc" data-lang="en">Classify, cluster, scale</div>
            <div class="pipe-desc" data-lang="zh">分类、聚类、量表化</div>
          </div>
          <div class="pipe-step">
            <div class="pipe-num">05</div>
            <div class="pipe-title" data-lang="en">Validate</div>
            <div class="pipe-title" data-lang="zh">验证评估</div>
            <div class="pipe-desc" data-lang="en">Evaluate, interpret</div>
            <div class="pipe-desc" data-lang="zh">评价、解读</div>
          </div>
        </div>
      </div>

      <!-- PREPROCESSING -->
      <div class="section">
        <h2 data-lang="en">Text Preprocessing</h2>
        <h2 data-lang="zh">文本预处理</h2>
        <div data-lang="en">
          <p>Think of raw text like vegetables fresh from the market — before cooking, you need to wash, peel, and chop them. Text preprocessing is similar: we clean and standardize raw text so the computer can work with it effectively. The main steps are: <strong>tokenization</strong> (chopping a sentence into individual words), <strong>lowercasing</strong> ("The" and "the" become the same word), <strong>removing stop words</strong> (common words like "the," "is," "and" that carry little meaning), and <strong>stemming/lemmatization</strong> (reducing words to their root form, so "running," "ran," and "runs" all become "run"). These choices matter a lot — different preprocessing can lead to different conclusions (Denny &amp; Spirling, 2018).</p>
        </div>
        <div data-lang="zh">
          <p>把原始文本想象成刚从菜市场买回来的蔬菜——做菜之前要洗净、削皮、切好。文本预处理也是如此：我们清洗和标准化原始文本，让计算机能有效处理。主要步骤包括：<strong>分词</strong>（把一句话切成一个个词语）、<strong>小写化</strong>（"The"和"the"统一成同一个词）、<strong>去除停用词</strong>（"the""is""and"这类没有实际含义的常见词）、<strong>词干提取/词形还原</strong>（把词还原到词根形式，让"running""ran""runs"都变成"run"）。这些选择非常重要——不同的预处理方式可能导致不同的结论（Denny &amp; Spirling, 2018）。</p>
        </div>
        <div class="compare-grid">
          <div class="compare-col">
            <div class="compare-label" data-lang="en">Stemming</div>
            <div class="compare-label" data-lang="zh">词干提取 (Stemming)</div>
            <div data-lang="en">
              <p><strong>Method:</strong> Rule-based suffix stripping (e.g., Porter Stemmer)</p>
              <p><strong>Example:</strong> "running" &rarr; "run", "studies" &rarr; "studi"</p>
              <p><strong>Pro:</strong> Fast, simple</p>
              <p><strong>Con:</strong> Can produce non-words ("studi")</p>
            </div>
            <div data-lang="zh">
              <p><strong>方法：</strong>基于规则的后缀截断（如 Porter Stemmer）</p>
              <p><strong>示例：</strong>"running" &rarr; "run", "studies" &rarr; "studi"</p>
              <p><strong>优点：</strong>速度快、简单</p>
              <p><strong>缺点：</strong>可能产生非词形式（"studi"）</p>
            </div>
          </div>
          <div class="compare-col alt">
            <div class="compare-label" data-lang="en">Lemmatization</div>
            <div class="compare-label" data-lang="zh">词形还原 (Lemmatization)</div>
            <div data-lang="en">
              <p><strong>Method:</strong> Dictionary-based reduction to canonical form</p>
              <p><strong>Example:</strong> "running" &rarr; "run", "better" &rarr; "good"</p>
              <p><strong>Pro:</strong> Produces real words, context-aware</p>
              <p><strong>Con:</strong> Slower, requires POS tagging</p>
            </div>
            <div data-lang="zh">
              <p><strong>方法：</strong>基于词典还原为标准词形</p>
              <p><strong>示例：</strong>"running" &rarr; "run", "better" &rarr; "good"</p>
              <p><strong>优点：</strong>产生真实词汇、考虑上下文</p>
              <p><strong>缺点：</strong>较慢，需要词性标注</p>
            </div>
          </div>
          <div class="compare-footer" data-lang="en">Denny &amp; Spirling (2018): Test preprocessing choices systematically — they can change your substantive conclusions.</div>
          <div class="compare-footer" data-lang="zh">Denny &amp; Spirling (2018)：系统地测试预处理选择——它们可能改变你的实质性结论。</div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART II: TEXT REPRESENTATION ========== -->

      <div class="section">
        <h2 data-lang="en">Text Representation</h2>
        <h2 data-lang="zh">文本表示</h2>
        <div data-lang="en">
          <p>Computers only understand numbers, not words. So before we can analyze text, we need to convert it into numerical form. There are two broad strategies: counting words (simple but effective) and learning "word meanings" as vectors (more powerful but complex).</p>
        </div>
        <div data-lang="zh">
          <p>计算机只懂数字，不懂文字。所以在分析文本之前，我们需要把文字转成数字。主要有两种策略：数词频（简单但有效）和把"词义"学习为向量（更强大但更复杂）。</p>
        </div>

        <!-- BoW & TF-IDF -->
        <div class="method-card">
          <span class="mc-tag rep">Representation</span>
          <h4>Bag of Words &amp; TF-IDF</h4>
          <div data-lang="en">
            <p><strong>Bag of Words (BoW)</strong> is the simplest idea: throw all the words of a document into a "bag," shake it up (forget the order), and just count how many times each word appears. The result is a big table (called a Document-Term Matrix) where each row is a document and each column is a word.</p>
            <div class="analogy">Analogy: Imagine dumping all the words of an essay onto a table and sorting them into piles. You know what words were used and how often, but you've lost the order they appeared in — "dog bites man" and "man bites dog" look the same.</div>
            <p><strong>TF-IDF</strong> improves on raw word counts by asking: "Is this word special to THIS document, or does it appear everywhere?" Words that appear in almost every document (like "the" or "is") get downweighted, while words unique to a few documents get boosted. This helps surface the words that truly distinguish one document from another.</p>
          </div>
          <div data-lang="zh">
            <p><strong>词袋模型 (BoW)</strong> 是最简单的思路：把一篇文档的所有词丢进一个"袋子"，摇一摇（忘掉顺序），然后数每个词出现了几次。结果是一张大表格（叫文档-词项矩阵），每一行是一篇文档，每一列是一个词。</p>
            <div class="analogy">类比：想象把一篇作文的所有字词倒在桌上，按词分堆。你知道用了哪些词、用了几次，但丢失了它们出现的顺序——"狗咬人"和"人咬狗"看起来一样。</div>
            <p><strong>TF-IDF</strong> 在原始词频的基础上多问了一个问题："这个词是这篇文档独有的，还是到处都有？"几乎每篇文档都出现的词（如"的""是""了"）被降权，而只在少数文档中出现的词被升权。这样就能找出真正让一篇文档区别于其他文档的关键词。</p>
          </div>
          <div class="formula">TF-IDF(t, d) = TF(t, d) &times; log(N / DF(t))</div>
          <div class="when" data-lang="en"><strong>When to use:</strong> A great starting point for most text projects. Works well for document classification and search. Limitation: treats words as independent — doesn't understand that "happy" and "joyful" are similar.</div>
          <div class="when" data-lang="zh"><strong>适用场景：</strong>大多数文本项目的绝佳起点。适合文档分类和搜索。局限：把每个词视为独立的——不理解"开心"和"高兴"是近义词。</div>
        </div>

        <!-- Word Embeddings -->
        <div class="method-card">
          <span class="mc-tag rep">Representation</span>
          <h4>Word Embeddings (Word2Vec, GloVe)</h4>
          <div data-lang="en">
            <p>BoW treats every word as completely unrelated — but we know "happy" and "joyful" are similar! Word embeddings fix this by representing each word as a list of numbers (a "vector") where similar words have similar numbers. The core idea is beautifully simple: <strong>"you shall know a word by the company it keeps"</strong> (Firth, 1957). If "cat" and "dog" often appear near the same words ("pet," "feed," "cute"), they must mean similar things.</p>
            <div class="analogy">Analogy: Think of words as people at a party. If two people always hang out in the same social circles, they probably have similar interests — even if they've never met each other directly.</div>
            <p><strong>Word2Vec</strong> (Mikolov et al., 2013) learns these vectors by training a simple neural network to predict either a word from its neighbors (CBOW) or neighbors from a word (Skip-gram). The famous result: vec("king") &minus; vec("man") + vec("woman") &asymp; vec("queen") — the model learned gender relationships just from reading text!</p>
            <p><strong>GloVe</strong> (Pennington et al., 2014) takes a different approach: instead of looking at local word windows, it analyzes the overall co-occurrence statistics of the entire corpus.</p>
          </div>
          <div data-lang="zh">
            <p>词袋模型把每个词都当作完全无关的——但我们知道"开心"和"高兴"是近义词！词嵌入解决了这个问题：把每个词表示为一串数字（"向量"），意思相近的词，数字也相近。核心思想非常优美：<strong>"通过一个词的伙伴来了解它"</strong>（Firth, 1957）。如果"猫"和"狗"经常出现在相同的词旁边（"宠物""喂""可爱"），它们的意思一定很接近。</p>
            <div class="analogy">类比：把词想象成派对上的人。如果两个人总在同一个社交圈里出现，他们大概有相似的兴趣——即使他们从未直接见过面。</div>
            <p><strong>Word2Vec</strong>（Mikolov et al., 2013）通过训练一个简单的神经网络来学习这些向量：要么从邻居词预测中心词（CBOW），要么从中心词预测邻居词（Skip-gram）。著名的结果：vec("king") &minus; vec("man") + vec("woman") &asymp; vec("queen")——模型仅仅通过阅读文本就学会了性别关系！</p>
            <p><strong>GloVe</strong>（Pennington et al., 2014）采用不同的方法：不是看局部词窗口，而是分析整个语料库的全局共现统计。</p>
          </div>
          <div class="when" data-lang="en"><strong>When to use:</strong> When you need the computer to understand that words can be similar. Great for measuring word similarity, detecting bias in language, or as input features for more advanced models.</div>
          <div class="when" data-lang="zh"><strong>适用场景：</strong>当你需要计算机理解词与词之间可以相似时。非常适合度量词汇相似度、检测语言中的偏见，或作为更高级模型的输入特征。</div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART III: UNSUPERVISED METHODS ========== -->

      <div class="section">
        <h2 data-lang="en">Unsupervised Methods</h2>
        <h2 data-lang="zh">无监督方法</h2>
        <div data-lang="en">
          <p>What if you have thousands of documents but no labels — nobody has told you which category each one belongs to? Unsupervised methods let the data speak for itself. They discover hidden patterns, topics, and dimensions without any labeled training data. Think of it as asking "what's in here?" rather than "is this positive or negative?"</p>
        </div>
        <div data-lang="zh">
          <p>如果你有成千上万的文档，但没有标签——没人告诉你每篇属于什么类别怎么办？无监督方法让数据自己"说话"。它们在没有任何标注训练数据的情况下发现隐藏的模式、主题和维度。可以把它想象成在问"这里面有什么？"而不是"这是正面还是负面？"</p>
        </div>

        <!-- Dictionary Methods -->
        <div class="method-card">
          <span class="mc-tag unsup">Unsupervised</span>
          <h4 data-lang="en">Dictionary Methods &amp; Sentiment Analysis</h4>
          <h4 data-lang="zh">词典方法与情感分析</h4>
          <div data-lang="en">
            <p>The simplest text analysis you can do: make a word list, then count. For example, a sentiment dictionary might say "happy" = +1, "terrible" = -1, "okay" = 0. To score a movie review, just add up the scores of all its words. That's it!</p>
            <div class="analogy">Analogy: It's like grading an essay by counting how many "good" words vs. "bad" words it uses — crude, but surprisingly useful as a first pass.</div>
            <p>Popular dictionaries include LIWC (psychology), AFINN (sentiment), and Hu &amp; Liu (opinion mining). <strong>Strengths:</strong> Transparent (you can see exactly why a document got its score), reproducible, and requires zero training data. <strong>Limitations:</strong> Can't handle context — it thinks "not good" is positive (it sees "good"!), misses sarcasm entirely, and the same word can mean different things in different domains ("sick" is negative in healthcare but positive in slang).</p>
          </div>
          <div data-lang="zh">
            <p>最简单的文本分析：建个词表，然后数数。比如，一个情感词典可能规定"开心"= +1，"糟糕"= -1，"一般"= 0。给一条影评打分，就是把所有词的分数加起来。就这么简单！</p>
            <div class="analogy">类比：就像通过数一篇文章里有多少"好词"和"坏词"来打分——粗糙，但作为初步分析效果出奇地好。</div>
            <p>常见词典包括 LIWC（心理学）、AFINN（情感）和 Hu &amp; Liu（意见挖掘）。<strong>优势：</strong>透明（你能清楚看到为什么一篇文档得了这个分数）、可复现、完全不需要训练数据。<strong>局限：</strong>不理解上下文——它会认为"不好"是正面的（因为它看到了"好"！），完全忽略讽刺，而且同一个词在不同领域含义不同（"有毒"在化学里是危险的，在网络用语里可能是"太搞笑了"）。</p>
          </div>
        </div>

        <!-- Topic Models -->
        <div class="method-card">
          <span class="mc-tag unsup">Unsupervised</span>
          <h4 data-lang="en">Topic Models (LDA)</h4>
          <h4 data-lang="zh">主题模型 (LDA)</h4>
          <div data-lang="en">
            <p>"What are people talking about?" — that's the question topic models answer. <strong>Latent Dirichlet Allocation (LDA)</strong> (Blei et al., 2003) imagines that every document was written by first picking a few topics, then picking words from those topics. LDA works backwards: given the words we observe, it figures out what the hidden topics must be.</p>
            <div class="analogy">Analogy: Imagine a newspaper. Each article mixes a few themes — an article about climate policy might be 60% "environment," 30% "politics," 10% "economics." LDA discovers these themes automatically by noticing that words like "carbon," "emissions," "temperature" tend to appear together.</div>
            <p>The researcher must choose how many topics <em>K</em> to look for — there's no single right answer, just like there's no single right number of folders to organize your files. You try different values and see which gives the most interpretable results. Common tools: held-out likelihood, topic coherence scores, and good old-fashioned "do these topics make sense to a human?"</p>
          </div>
          <div data-lang="zh">
            <p>"大家都在讨论什么？"——这就是主题模型要回答的问题。<strong>潜在狄利克雷分配（LDA）</strong>（Blei et al., 2003）想象每篇文档是这样写出来的：先选几个主题，再从这些主题里选词。LDA 做的是反向推理：给定我们看到的词语，推断出背后隐藏的主题是什么。</p>
            <div class="analogy">类比：想象一份报纸。每篇文章混合了几个主题——一篇关于气候政策的文章可能是 60%"环境"、30%"政治"、10%"经济"。LDA 通过发现"碳""排放""温度"这些词总是同时出现，自动发现这些主题。</div>
            <p>研究者需要选择寻找多少个主题 <em>K</em>——没有唯一正确的答案，就像整理文件时没有唯一正确的文件夹数量一样。你试不同的值，看哪个给出最容易解读的结果。常用工具：留出似然、主题连贯性分数，以及最经典的"这些主题对人类来说有道理吗？"</p>
          </div>
          <div class="when" data-lang="en"><strong>When to use:</strong> You have a large collection of texts and want to discover what topics they cover — without reading them all yourself. Great for content analysis at scale.</div>
          <div class="when" data-lang="zh"><strong>适用场景：</strong>你有大量文本，想知道它们涵盖了哪些话题——不用自己全部读完。非常适合大规模内容分析。</div>
        </div>

        <!-- STM -->
        <div class="method-card">
          <span class="mc-tag unsup">Unsupervised</span>
          <h4 data-lang="en">Structural Topic Model (STM)</h4>
          <h4 data-lang="zh">结构主题模型 (STM)</h4>
          <div data-lang="en">
            <p>Regular LDA discovers topics but can't tell you <em>why</em> topics vary across documents. STM (Roberts et al., 2014) is LDA's smarter cousin: it lets you plug in extra information about each document — like who wrote it, when, or where — and then the model can tell you how topics differ across those categories.</p>
            <div class="eg">Example: Feed STM a collection of Congressional speeches with party labels. STM can tell you not just what topics are discussed, but that Democrats talk about healthcare 3x more than Republicans, and when they both discuss "economy," they use different words.</div>
          </div>
          <div data-lang="zh">
            <p>普通的 LDA 能发现主题，但没法告诉你主题<em>为什么</em>在不同文档间变化。STM（Roberts et al., 2014）是 LDA 的"升级版"：它允许你加入每篇文档的额外信息——比如谁写的、什么时候写的、在哪里发表的——然后模型就能告诉你主题是如何随这些因素变化的。</p>
            <div class="eg">例子：把一批国会演讲和政党标签一起输入 STM。STM 不仅能发现讨论了哪些话题，还能告诉你民主党讨论医疗的频率是共和党的 3 倍，而且两党讨论"经济"时用的词都不一样。</div>
          </div>
          <div class="when" data-lang="en"><strong>When to use:</strong> When you have metadata (party, time, source) and want to know how topics or language change across groups or over time. The go-to tool for social scientists.</div>
          <div class="when" data-lang="zh"><strong>适用场景：</strong>当你有元数据（党派、时间、来源）并想知道主题或语言如何在群体间或随时间变化时。社会科学家的首选工具。</div>
        </div>

        <!-- Text Scaling -->
        <div class="method-card">
          <span class="mc-tag scale">Scaling</span>
          <h4 data-lang="en">Text Scaling: Wordscores &amp; Wordfish</h4>
          <h4 data-lang="zh">文本量表化：Wordscores 与 Wordfish</h4>
          <div data-lang="en">
            <p>Sometimes you don't want topics — you want to know <em>where</em> someone stands. Text scaling places documents (or their authors) on a spectrum, like a political left–right scale, based purely on word choices.</p>
            <div class="analogy">Analogy: If someone keeps saying "freedom," "market," and "deregulation," they're probably on the right. If they say "equality," "welfare," and "regulation," they're probably on the left. Text scaling automates this intuition.</div>
            <p><strong>Wordscores</strong> (Laver et al., 2003) needs reference texts — you give it a few documents with known positions (e.g., a clearly left-wing and a clearly right-wing manifesto), and it figures out where new documents fall by looking at which reference they use more similar words to.</p>
            <p><strong>Wordfish</strong> (Slapin &amp; Proksch, 2008) doesn't need references at all — it discovers the latent dimension purely from how word frequencies differ across documents. Think of it as "let the data tell me who's on which end."</p>
          </div>
          <div data-lang="zh">
            <p>有时候你关心的不是主题，而是想知道某人<em>站在哪里</em>。文本量表化根据用词选择把文档（或作者）放在一条光谱上，比如政治左右轴。</p>
            <div class="analogy">类比：如果一个人总说"自由""市场""放松管制"，他大概偏右。如果总说"平等""福利""监管"，大概偏左。文本量表化把这种直觉自动化了。</div>
            <p><strong>Wordscores</strong>（Laver et al., 2003）需要参考文本——你给它几篇已知立场的文档（比如一篇明确左翼和一篇明确右翼的宣言），它通过看新文档与哪个参考更像来确定位置。</p>
            <p><strong>Wordfish</strong>（Slapin &amp; Proksch, 2008）完全不需要参考——它纯粹从词频在文档间的差异中发现潜在维度。可以理解为"让数据告诉我谁在哪一端"。</p>
          </div>
          <div class="when" data-lang="en"><strong>When to use:</strong> Political science — estimating party positions from manifestos, legislative speeches, or policy documents along a single dimension (e.g., left vs. right, pro vs. anti).</div>
          <div class="when" data-lang="zh"><strong>适用场景：</strong>政治学——沿单一维度（如左vs右、支持vs反对）从宣言、立法演讲或政策文件中估计政党位置。</div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART IV: SUPERVISED METHODS ========== -->

      <div class="section">
        <h2 data-lang="en">Supervised Methods</h2>
        <h2 data-lang="zh">有监督方法</h2>
        <div data-lang="en">
          <p>Unlike unsupervised methods, supervised methods need a "teacher" — a set of documents that humans have already labeled (e.g., "positive" / "negative," or "about healthcare" / "about economy"). The model learns from these examples and then predicts labels for new, unseen documents. Think of it as training with an answer key, then taking the test solo.</p>
        </div>
        <div data-lang="zh">
          <p>和无监督方法不同，有监督方法需要一个"老师"——一批人类已经标好标签的文档（比如"正面"/"负面"，或"关于医疗"/"关于经济"）。模型从这些例子中学习，然后预测新文档的标签。就像先用答案集训练，再独立考试。</p>
        </div>

        <!-- Random Forest -->
        <div class="method-card">
          <span class="mc-tag sup">Supervised</span>
          <h4>Random Forest</h4>
          <div data-lang="en">
            <p>Imagine asking 100 people to classify a document, but each person only sees a random portion of the words. Each person makes an imperfect judgment, but when you take a majority vote, the crowd is surprisingly accurate. That's Random Forest: it builds hundreds of decision trees, each looking at a random subset of features, then averages their predictions.</p>
            <div class="analogy">Analogy: One quiz from a single question is unreliable. But average 100 quizzes, each with different random questions, and you get a very reliable assessment.</div>
            <p>A bonus: Random Forest tells you which words matter most for the prediction (called <strong>feature importance</strong>), so you can peek inside and understand why the model classifies things the way it does.</p>
          </div>
          <div data-lang="zh">
            <p>想象让 100 个人给一篇文档分类，但每个人只看到随机一部分词。每个人的判断都不完美，但投票表决后，群体的准确率出奇地高。这就是随机森林：它构建几百棵决策树，每棵树只看随机一部分特征，然后综合它们的预测。</p>
            <div class="analogy">类比：一道题的随堂测验不可靠。但把 100 次测验（每次随机出不同题）的结果平均，你就能得到非常可靠的评估。</div>
            <p>额外好处：随机森林会告诉你哪些词对预测最重要（叫<strong>特征重要性</strong>），让你能窥探模型内部，理解它为什么这样分类。</p>
          </div>
        </div>

        <!-- Neural Networks -->
        <div class="method-card">
          <span class="mc-tag dl">Deep Learning</span>
          <h4 data-lang="en">Neural Networks for Text</h4>
          <h4 data-lang="zh">用于文本的神经网络</h4>
          <div data-lang="en">
            <p>Neural networks don't need you to tell them what features to look for — they figure it out themselves from the data. For text, three architectures matter:</p>
            <p><strong>CNNs</strong> — Like a sliding magnifying glass that scans text for useful local patterns. Good at spotting short phrases like "not good" or "highly recommended."</p>
            <p><strong>RNNs / LSTMs</strong> — Read text word by word, remembering what came before. Like a person reading a book from start to finish, keeping track of the plot. LSTMs are the improved version that can remember things from much earlier in the text.</p>
            <p><strong>Transformers</strong> — The breakthrough architecture behind ChatGPT. Instead of reading word by word, transformers look at ALL words at once and figure out which words are important for understanding each other. (More details on the <a href="llm.html" style="color:var(--red)">LLM &amp; NLP page</a>.)</p>
          </div>
          <div data-lang="zh">
            <p>神经网络不需要你告诉它该关注什么特征——它自己从数据中摸索出来。对于文本，有三种重要的架构：</p>
            <p><strong>CNN</strong>——像一个滑动的放大镜，扫描文本寻找有用的局部模式。擅长发现短语，如"not good"或"强烈推荐"。</p>
            <p><strong>RNN / LSTM</strong>——逐词阅读文本，记住之前读过的内容。就像一个人从头到尾读一本书，一直跟踪剧情。LSTM 是改进版，能记住文本中更早出现的信息。</p>
            <p><strong>Transformer</strong>——ChatGPT 背后的突破性架构。不是逐词阅读，而是同时看所有词，搞清楚哪些词对理解彼此最重要。（详见 <a href="llm.html" style="color:var(--red)">LLM &amp; NLP 页面</a>。）</p>
          </div>
        </div>

        <!-- LLMs -->
        <div class="method-card">
          <span class="mc-tag dl">Deep Learning</span>
          <h4 data-lang="en">Large Language Models (BERT, GPT)</h4>
          <h4 data-lang="zh">大语言模型 (BERT, GPT)</h4>
          <div data-lang="en">
            <p>LLMs have read billions of web pages during pre-training, so they already "understand" language before you give them any task-specific data. Two key players:</p>
            <p><strong>BERT</strong> — Reads text in both directions (like reading a sentence forwards AND backwards at once). Give it a few hundred labeled examples and it quickly learns to classify your specific task. The "fine-tuning" approach.</p>
            <p><strong>GPT (ChatGPT, GPT-4)</strong> — A text generator trained to predict the next word. The magic: you can simply <em>describe</em> what you want in natural language (a "prompt") and it does it — no training data needed. Give it zero examples (zero-shot), a few examples (few-shot), or step-by-step instructions (chain-of-thought).</p>
            <div class="eg">For text-as-data research: "Classify this tweet as pro-climate or anti-climate" → GPT can do this with zero training examples. Need higher accuracy? Fine-tune BERT on a few hundred hand-labeled tweets. Need even better? Combine both: use GPT to pre-label thousands of documents cheaply, then fine-tune BERT on the best ones.</div>
          </div>
          <div data-lang="zh">
            <p>LLM 在预训练阶段已经读过了数十亿网页，所以在你给它任何特定任务数据之前，它就已经"理解"语言了。两个关键角色：</p>
            <p><strong>BERT</strong>——双向阅读文本（像同时正着和倒着读一句话）。给它几百个标注样本，它就能快速学会你的特定分类任务。这是"微调"路线。</p>
            <p><strong>GPT（ChatGPT、GPT-4）</strong>——一个文本生成器，训练来预测下一个词。神奇之处：你只需用自然语言<em>描述</em>你想要什么（一个"提示词"），它就能做到——不需要训练数据。给零个例子（零样本）、几个例子（少样本），或者逐步说明（思维链）都行。</p>
            <div class="eg">用于文本即数据研究："请把这条推文分类为支持气候行动或反对气候行动"→ GPT 不需要任何训练样本就能做这件事。需要更高准确率？用几百条人工标注的推文微调 BERT。还要更好？两者结合：用 GPT 低成本地预标注数千文档，然后在最佳样本上微调 BERT。</div>
          </div>
        </div>

        <!-- Evaluation Metrics -->
        <div class="info-box">
          <div class="label" data-lang="en">How Do We Know If It's Working? — Evaluation Metrics</div>
          <div class="label" data-lang="zh">怎么知道模型好不好？——评估指标</div>
          <div data-lang="en">
            <p><strong>Accuracy</strong> = how many did you get right overall? Simple but misleading if your data is imbalanced (99% accuracy is easy if 99% of documents are in one class). <strong>Precision</strong> = "of the documents you labeled positive, how many actually were?" (avoiding false alarms). <strong>Recall</strong> = "of all the actual positives, how many did you catch?" (avoiding misses). <strong>F1</strong> = the balance between precision and recall. Always report multiple metrics and use cross-validation!</p>
          </div>
          <div data-lang="zh">
            <p><strong>准确率</strong> = 总共答对了多少？简单但在数据不平衡时会误导（如果 99% 的文档属于同一类，99% 的准确率很容易）。<strong>精确率</strong> ="你标为正面的文档中，有多少真的是正面？"（避免误报）。<strong>召回率</strong> ="所有真正的正面文档中，你抓到了多少？"（避免遗漏）。<strong>F1</strong> = 精确率和召回率的平衡。一定要报告多个指标，并使用交叉验证！</p>
          </div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART V: ANNOTATION ========== -->

      <div class="section">
        <h2 data-lang="en">Annotation &amp; Human-in-the-Loop</h2>
        <h2 data-lang="zh">标注与人在回路中</h2>
        <div data-lang="en">
          <p>Supervised learning is only as good as its labels — "garbage in, garbage out." If your training labels are wrong or inconsistent, even the fanciest model will learn the wrong patterns. Good annotation is like building a solid foundation for a house: invisible but essential.</p>
        </div>
        <div data-lang="zh">
          <p>有监督学习的效果取决于标签质量——"垃圾进，垃圾出"。如果你的训练标签有误或不一致，再花哨的模型也会学到错误的模式。好的标注就像盖房子打地基：看不见但至关重要。</p>
        </div>

        <div class="compare-grid">
          <div class="compare-col">
            <div class="compare-label" data-lang="en">How to Measure Agreement</div>
            <div class="compare-label" data-lang="zh">如何衡量一致性</div>
            <div data-lang="en">
              <p>If two people label the same 100 tweets, how often do they agree? Raw agreement isn't enough — two people flipping coins would agree 50% of the time by chance!</p>
              <p><strong>Cohen's Kappa (&kappa;):</strong> Corrects for chance agreement between two annotators. &kappa; > .80 = good, &kappa; < .40 = poor.</p>
              <p><strong>Krippendorff's Alpha (&alpha;):</strong> More flexible — works with multiple annotators, missing data, and different label types. &alpha; > .80 = reliable; .67–.80 = tentative conclusions only.</p>
            </div>
            <div data-lang="zh">
              <p>如果两个人标注同样的 100 条推文，他们多久达成一致？单纯看一致率不够——两个人随机扔硬币也有 50% 的概率一致！</p>
              <p><strong>Cohen's Kappa (&kappa;):</strong> 校正了随机一致的两人一致性。&kappa; > .80 = 好, &kappa; < .40 = 差。</p>
              <p><strong>Krippendorff's Alpha (&alpha;):</strong> 更灵活——可处理多人、缺失数据和不同标签类型。&alpha; > .80 = 可靠；.67–.80 = 只能得出初步结论。</p>
            </div>
          </div>
          <div class="compare-col alt">
            <div class="compare-label" data-lang="en">LLMs as Annotators</div>
            <div class="compare-label" data-lang="zh">LLM 作为标注者</div>
            <div data-lang="en">
              <p>Why hire 10 research assistants when GPT-4 can label 10,000 documents overnight? Recent research shows LLMs can match or exceed crowd-worker quality on many tasks — and they never get tired or bored.</p>
              <p><strong>But be careful:</strong> Prompt design matters enormously (clear instructions + a few examples = much better results), and you still need to validate against expert human labels. LLMs have biases too, and they might be systematically wrong in ways that are hard to detect.</p>
            </div>
            <div data-lang="zh">
              <p>为什么雇 10 个研究助理？让 GPT-4 一晚上标注 10,000 篇文档就行了。最新研究表明 LLM 在许多任务上能达到甚至超过众包工人的质量——而且它们永远不会累或无聊。</p>
              <p><strong>但要小心：</strong>提示设计极为重要（清晰的指示 + 几个例子 = 好得多的结果），而且你仍然需要对照专家人工标签进行验证。LLM 也有偏见，它们可能在难以察觉的方面系统性地出错。</p>
            </div>
          </div>
        </div>

        <div class="info-box">
          <div class="label" data-lang="en">Active Learning — Be Smart About What You Label</div>
          <div class="label" data-lang="zh">主动学习——聪明地选择标注什么</div>
          <div data-lang="en">
            <p>Labeling data is expensive and slow. Active learning is a clever shortcut: instead of randomly picking documents to label, let the model tell you which ones it's <em>most confused about</em>, and label those first. This way, every label you add teaches the model the most. It's like studying for an exam by focusing on the questions you got wrong, not reviewing what you already know.</p>
          </div>
          <div data-lang="zh">
            <p>标注数据既贵又慢。主动学习是一个聪明的捷径：不随机选文档来标注，而是让模型告诉你它<em>最困惑</em>的是哪些文档，先标注那些。这样每一个标签都能让模型学到最多。就像备考时专注做错题，而不是复习已经会的内容。</p>
          </div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART VI: ETHICS ========== -->

      <div class="section">
        <h2 data-lang="en">Ethics in Text Analysis</h2>
        <h2 data-lang="zh">文本分析中的伦理</h2>
        <div data-lang="en">
          <p>Just because you <em>can</em> analyze text doesn't mean you always <em>should</em> — or that it's safe to do so carelessly. Working with text data raises real ethical concerns that researchers must think about from day one, not as an afterthought.</p>
        </div>
        <div data-lang="zh">
          <p>你<em>能</em>分析文本，不代表你总<em>应该</em>这么做——也不代表可以粗心大意。文本数据研究带来了真实的伦理问题，研究者必须从一开始就考虑，而不是事后补救。</p>
        </div>
        <div class="card-row">
          <div class="card">
            <h4 data-lang="en">Privacy</h4>
            <h4 data-lang="zh">隐私</h4>
            <div data-lang="en">
              <ul>
                <li>Text often reveals who people are — names, locations, writing style</li>
                <li>Just because a tweet is public doesn't mean the person agreed to be in your study</li>
                <li>Even "anonymized" text can be traced back — people have unique writing fingerprints</li>
              </ul>
            </div>
            <div data-lang="zh">
              <ul>
                <li>文本常常暴露身份——姓名、地点、写作风格</li>
                <li>一条推文是公开的，不代表发布者同意被纳入你的研究</li>
                <li>即使"匿名化"了也可能被追溯——每个人的写作风格都是独特的指纹</li>
              </ul>
            </div>
          </div>
          <div class="card highlight">
            <div class="card-tag" data-lang="en">Critical</div>
            <div class="card-tag" data-lang="zh">关键</div>
            <h4 data-lang="en">Bias</h4>
            <h4 data-lang="zh">偏见</h4>
            <div data-lang="en">
              <ul>
                <li>Models learn from human-written text, which reflects historical biases — and then amplify them</li>
                <li>Word embeddings literally encode stereotypes: "doctor" closer to "man," "nurse" closer to "woman"</li>
                <li>Sentiment tools may rate African American English as more negative — the tool isn't neutral</li>
              </ul>
            </div>
            <div data-lang="zh">
              <ul>
                <li>模型从人类写的文本中学习，而这些文本反映了历史偏见——然后模型还会放大这些偏见</li>
                <li>词嵌入真的编码了刻板印象："医生"离"男性"更近，"护士"离"女性"更近</li>
                <li>情感分析工具可能把非裔美国人英语评为更负面——工具并不中立</li>
              </ul>
            </div>
          </div>
          <div class="card">
            <h4 data-lang="en">Reproducibility</h4>
            <h4 data-lang="zh">可重复性</h4>
            <div data-lang="en">
              <ul>
                <li>Share your code and data (when possible) so others can check your work</li>
                <li>Record every choice: which model, which version, which random seed, which preprocessing</li>
                <li>LLM outputs change between runs — always report your exact prompt and model version (e.g., "GPT-4-turbo, Jan 2024")</li>
              </ul>
            </div>
            <div data-lang="zh">
              <ul>
                <li>共享代码和数据（可能时），让别人可以检查你的工作</li>
                <li>记录每个选择：哪个模型、哪个版本、哪个随机种子、哪种预处理</li>
                <li>LLM 输出每次运行都可能不同——务必报告确切的提示词和模型版本（如"GPT-4-turbo, 2024年1月"）</li>
              </ul>
            </div>
          </div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART VII: METHOD COMPARISON ========== -->

      <div class="section">
        <h2 data-lang="en">Choosing a Method</h2>
        <h2 data-lang="zh">方法选择指南</h2>
        <div data-lang="en">
          <p>With so many methods, how do you pick? The biggest deciding factor is often the simplest: <strong>do you have labeled data or not?</strong> Here's a quick guide:</p>
        </div>
        <div data-lang="zh">
          <p>这么多方法，怎么选？最大的决定因素往往也是最简单的：<strong>你有标注数据吗？</strong>这里有一个快速指南：</p>
        </div>
        <div class="compare-grid">
          <div class="compare-col">
            <div class="compare-label" data-lang="en">No Labeled Data</div>
            <div class="compare-label" data-lang="zh">无标注数据</div>
            <div data-lang="en">
              <p><strong>Dictionary methods</strong> — "I know what categories I want, just count the words" (simplest)</p>
              <p><strong>Topic models (LDA/STM)</strong> — "What topics are hiding in this pile of text?" (exploratory)</p>
              <p><strong>Wordfish</strong> — "Where does each author stand on a left-right scale?" (scaling)</p>
              <p><strong>LLM zero-shot</strong> — "Just ask ChatGPT to classify it" (quick &amp; surprisingly good)</p>
            </div>
            <div data-lang="zh">
              <p><strong>词典方法</strong>——"我知道想要什么类别，数词就行"（最简单）</p>
              <p><strong>主题模型 (LDA/STM)</strong>——"这堆文本里藏着什么话题？"（探索性）</p>
              <p><strong>Wordfish</strong>——"每个作者在左右光谱上站在哪？"（量表化）</p>
              <p><strong>LLM 零样本</strong>——"直接让 ChatGPT 分类"（快速且效果出奇地好）</p>
            </div>
          </div>
          <div class="compare-col alt">
            <div class="compare-label" data-lang="en">With Labeled Data</div>
            <div class="compare-label" data-lang="zh">有标注数据</div>
            <div data-lang="en">
              <p><strong>Wordscores</strong> — "I have reference texts, tell me where new ones fall" (scaling)</p>
              <p><strong>Random Forest / SVM</strong> — "Give me a solid, interpretable baseline" (classic ML)</p>
              <p><strong>Fine-tuned BERT</strong> — "I want the best accuracy possible" (state-of-the-art)</p>
              <p><strong>LLM few-shot</strong> — "I only have a handful of labeled examples" (flexible)</p>
            </div>
            <div data-lang="zh">
              <p><strong>Wordscores</strong>——"我有参考文本，告诉我新文本在哪个位置"（量表化）</p>
              <p><strong>随机森林 / SVM</strong>——"给我一个靠谱、可解释的基线"（经典 ML）</p>
              <p><strong>微调 BERT</strong>——"我要最高的准确率"（最先进）</p>
              <p><strong>LLM 少样本</strong>——"我只有几个标注样本"（灵活）</p>
            </div>
          </div>
          <div class="compare-footer" data-lang="en">Golden rule: start simple (dictionaries, BoW + Random Forest), see how well it works, THEN try fancier methods if needed. You'd be surprised how often the simple approach is good enough.</div>
          <div class="compare-footer" data-lang="zh">黄金法则：从简单的开始（词典、BoW + 随机森林），看看效果如何，然后再根据需要尝试更复杂的方法。你会惊讶地发现，简单的方法往往就够用了。</div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ========== PART VIII: REFERENCES & SOFTWARE ========== -->

      <div class="section">
        <h2 data-lang="en">References &amp; Software</h2>
        <h2 data-lang="zh">参考文献与软件</h2>
        <h3 data-lang="en">Key References</h3>
        <h3 data-lang="zh">核心文献</h3>
        <ul class="ref-list">
          <li><span class="ref-tag beginner">Beginner</span>Grimmer, J., Roberts, M. E., &amp; Stewart, B. M. (2022). <em>Text as Data: A New Framework for Machine Learning and the Social Sciences.</em> Princeton University Press.</li>
          <li><span class="ref-tag">Preprocessing</span>Denny, M. J., &amp; Spirling, A. (2018). Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it. <em>Political Analysis, 26</em>(2), 168–189.</li>
          <li><span class="ref-tag fit">Topic Models</span>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet allocation. <em>Journal of Machine Learning Research, 3</em>, 993–1022.</li>
          <li><span class="ref-tag">STM</span>Roberts, M. E., Stewart, B. M., Tingley, D., et al. (2014). Structural topic models for open-ended survey responses. <em>American Journal of Political Science, 58</em>(4), 1064–1082.</li>
          <li><span class="ref-tag applied">Scaling</span>Slapin, J. B., &amp; Proksch, S.-O. (2008). A scaling model for estimating time-series party positions from texts. <em>American Journal of Political Science, 52</em>(3), 705–722.</li>
          <li><span class="ref-tag applied">Scaling</span>Laver, M., Benoit, K., &amp; Garry, J. (2003). Extracting policy positions from political texts using words as data. <em>American Political Science Review, 97</em>(2), 311–331.</li>
          <li><span class="ref-tag">Embeddings</span>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. <em>arXiv:1301.3781</em>.</li>
          <li><span class="ref-tag">LLM</span>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. <em>NAACL-HLT</em>.</li>
        </ul>
        <h3 data-lang="en">Software</h3>
        <h3 data-lang="zh">软件工具</h3>
        <div class="sw-row">
          <div class="sw-pill gold">
            <h4>Python — NLTK / spaCy</h4>
            <div data-lang="en"><p>Core NLP libraries. NLTK for learning, spaCy for production. Tokenization, POS, NER, lemmatization.</p></div>
            <div data-lang="zh"><p>核心 NLP 库。NLTK 适合学习，spaCy 适合生产。分词、词性标注、NER、词形还原。</p></div>
          </div>
          <div class="sw-pill">
            <h4>Python — scikit-learn</h4>
            <div data-lang="en"><p>TF-IDF vectorizer, Random Forest, SVM, evaluation metrics, cross-validation. The ML workhorse.</p></div>
            <div data-lang="zh"><p>TF-IDF 向量化器、随机森林、SVM、评估指标、交叉验证。ML 主力工具。</p></div>
          </div>
          <div class="sw-pill">
            <h4>Python — Gensim</h4>
            <div data-lang="en"><p>Topic modeling (LDA), Word2Vec, Doc2Vec. Efficient for large corpora.</p></div>
            <div data-lang="zh"><p>主题模型 (LDA)、Word2Vec、Doc2Vec。大型语料库高效处理。</p></div>
          </div>
        </div>
        <div class="sw-row">
          <div class="sw-pill">
            <h4>R — quanteda</h4>
            <div data-lang="en"><p>Text analysis in R. DTM construction, dictionaries, Wordfish, Wordscores. Social science standard.</p></div>
            <div data-lang="zh"><p>R 中的文本分析。DTM 构建、词典方法、Wordfish、Wordscores。社会科学标准工具。</p></div>
          </div>
          <div class="sw-pill">
            <h4>R — stm</h4>
            <div data-lang="en"><p>Structural Topic Models with covariate effects. Roberts et al.'s implementation.</p></div>
            <div data-lang="zh"><p>带协变量效应的结构主题模型。Roberts et al. 的实现。</p></div>
          </div>
          <div class="sw-pill">
            <h4>HuggingFace Transformers</h4>
            <div data-lang="en"><p>Pre-trained BERT, GPT, and other LLMs. Fine-tuning and inference. Python ecosystem.</p></div>
            <div data-lang="zh"><p>预训练的 BERT、GPT 及其他 LLM。微调和推理。Python 生态。</p></div>
          </div>
        </div>
      </div>

      <!-- PAGE NAV -->
      <div class="page-nav">
        <a href="llm.html">
          <div class="nav-label">&larr; Previous</div>
          <div class="nav-title">LLM &amp; NLP</div>
        </a>
        <a class="next" href="theoretical-modeling.html">
          <div class="nav-label">Next &rarr;</div>
          <div class="nav-title">Theoretical Modeling</div>
        </a>
      </div>

    </div>
  </div>
</div>

<script>
function setLang(lang){
  document.body.className=lang==='zh'?'zh':'';
  document.getElementById('btn-en').className='lang-btn'+(lang==='en'?' active':'');
  document.getElementById('btn-zh').className='lang-btn'+(lang==='zh'?' active':'');
  localStorage.setItem('selectedLang',lang);
}
window.addEventListener('DOMContentLoaded',function(){
  var lang=localStorage.getItem('selectedLang')||'zh';
  setLang(lang);
});
</script>
</body>
</html>
