<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Machine Learning — Research Methods Notebook</title>
<link href="https://fonts.googleapis.com/css2?family=Cormorant+Garamond:ital,wght@0,300;0,400;0,500;0,600;1,300;1,400;1,500&family=EB+Garamond:ital,wght@0,400;0,500;1,400&family=Caveat:wght@400;500&family=IBM+Plex+Sans:wght@300;400;500&family=IBM+Plex+Mono:wght@400;500&family=Noto+Serif+SC:wght@300;400;500;600&family=Noto+Sans+SC:wght@300;400;500&display=swap" rel="stylesheet">
<link rel="stylesheet" href="style.css">
<style>
/* ── Method sections (TM-style) ──────────────────── */
.method-section{margin:24px 0}
.method-section h3{font-family:var(--sans);font-size:15px;font-weight:600;color:var(--ink);margin-bottom:12px;margin-top:16px}
.method-desc{font-size:14.5px;line-height:1.75;color:var(--ink-faded);margin-bottom:12px}
.method-desc strong{color:var(--leather)}
.method-example{margin:12px 0;padding:10px 14px;border-radius:4px;background:rgba(181,55,42,.04);border:1px solid rgba(181,55,42,.1);font-size:14.5px;line-height:1.65;color:var(--ink-faded)}
.method-example::before{content:'';display:inline-block;width:6px;height:6px;background:var(--red);border-radius:50%;margin-right:8px;vertical-align:middle}
.method-analogy{margin:12px 0;padding:10px 14px;border-radius:4px;background:rgba(194,153,61,.06);border:1px solid rgba(194,153,61,.15);font-size:14.5px;line-height:1.65;color:var(--ink-faded);font-style:italic}
.method-when{margin:12px 0;padding:10px 14px;border-radius:4px;background:rgba(90,122,107,.05);border:1px solid rgba(90,122,107,.2);font-size:14px;line-height:1.65;color:var(--ink-faded)}
.method-when strong{color:var(--leather);font-style:normal}
.challenge-overview{margin:16px 0;padding:12px 14px;border-left:3px solid var(--gold);background:rgba(194,153,61,.04)}
.challenge-overview p{font-size:14.5px;line-height:1.65;color:var(--ink-faded);margin-bottom:0}
.challenge-overview strong{color:var(--leather)}
.formula{font-family:var(--mono);font-size:12.5px;color:var(--ink-soft);background:rgba(30,24,15,.04);padding:8px 12px;border-radius:3px;margin:8px 0;overflow-x:auto;white-space:nowrap;border:1px solid var(--parchment);display:block}

/* ── Intro cards ────────────────────────────────── */
.intro-cards{display:grid;grid-template-columns:1fr 1fr 1fr;gap:12px;margin:16px 0}
@media(max-width:860px){.intro-cards{grid-template-columns:1fr}}
.intro-card{border:1px solid var(--parchment);border-radius:4px;padding:14px 16px;background:var(--paper)}
.intro-card .card-label{font-family:var(--sans);font-size:10px;font-weight:600;letter-spacing:.1em;text-transform:uppercase;color:var(--red);margin-bottom:6px}
.intro-card p{font-size:13.5px;line-height:1.65;color:var(--ink-faded);margin-bottom:4px}
.intro-card p:last-child{margin-bottom:0}

/* ── Modules overview grid ──────────────────────── */
.modules-overview{display:grid;grid-template-columns:1fr 1fr;gap:16px;margin:16px 0}
@media(max-width:860px){.modules-overview{grid-template-columns:1fr}}
.module-card{border:1px solid var(--parchment);border-radius:4px;padding:16px;background:var(--paper)}
.module-card h4{font-family:var(--sans);font-size:13px;font-weight:600;color:var(--red);margin-bottom:6px}
.module-card p{font-size:14.5px;line-height:1.65;color:var(--ink-faded);margin-bottom:0}

/* ── Pipeline diagram ───────────────────────────── */
.pipeline{display:flex;align-items:stretch;gap:0;margin:20px 0;overflow-x:auto}
.pipe-step{flex:1;text-align:center;padding:16px 10px;border:1px solid var(--parchment);background:var(--paper);position:relative;min-width:110px}
.pipe-step:first-child{border-radius:4px 0 0 4px}
.pipe-step:last-child{border-radius:0 4px 4px 0}
.pipe-step:not(:last-child){border-right:none}
.pipe-step.active{background:var(--ink);border-color:var(--ink);color:var(--paper)}
.pipe-num{font-family:var(--sans);font-size:10px;font-weight:600;letter-spacing:.1em;color:var(--gold);margin-bottom:4px}
.pipe-step.active .pipe-num{color:var(--gold)}
.pipe-title{font-family:var(--sans);font-size:11px;font-weight:600;color:var(--ink);line-height:1.3}
.pipe-step.active .pipe-title{color:var(--paper)}
.pipe-desc{font-size:11px;color:var(--ink-ghost);margin-top:4px;line-height:1.4}
.pipe-step.active .pipe-desc{color:rgba(248,244,236,.6)}
@media(max-width:860px){.pipeline{flex-wrap:wrap}.pipe-step{min-width:auto;border-radius:4px!important;border:1px solid var(--parchment)!important;margin:3px}}

/* ── Compare grid ───────────────────────────────── */
.compare-grid{display:grid;grid-template-columns:1fr 1fr;gap:0;margin:16px 0;border:1px solid var(--parchment);border-radius:4px;overflow:hidden}
.compare-col{padding:18px;background:var(--paper)}
.compare-col.alt{background:var(--warm)}
.compare-label{font-family:var(--sans);font-size:10px;font-weight:700;letter-spacing:.1em;text-transform:uppercase;color:var(--red);margin-bottom:12px}
.compare-footer{grid-column:1/-1;padding:12px 18px;background:rgba(194,153,61,.05);border-top:1px solid var(--parchment);font-size:13px;color:var(--ink-faded);font-style:italic}
.compare-col p{font-size:14px;line-height:1.65;color:var(--ink-faded);margin-bottom:8px}
@media(max-width:860px){.compare-grid{grid-template-columns:1fr}}

/* ── Software pills ─────────────────────────────── */
.sw-row{display:grid;grid-template-columns:repeat(3,1fr);gap:12px;margin:12px 0}
.sw-pill{border:1px solid var(--parchment);border-radius:4px;padding:14px;background:var(--paper)}
.sw-pill.gold{background:var(--ink);color:rgba(248,244,236,.85);border-color:var(--ink)}
.sw-pill h4{font-family:var(--sans);font-size:13px;font-weight:600;margin-bottom:4px}
.sw-pill.gold h4{color:var(--gold)}
.sw-pill p{font-size:12.5px;line-height:1.5;color:var(--ink-faded);margin:0}
.sw-pill.gold p{color:rgba(248,244,236,.7)}
@media(max-width:860px){.sw-row{grid-template-columns:1fr}}

/* ── Resources ──────────────────────────────────── */
.resources-section{margin-top:32px;padding-top:20px;border-top:2px solid var(--parchment)}
.resource-list{list-style:none;padding:0;margin:12px 0}
.resource-list li{margin:8px 0;font-size:14.5px;color:var(--ink-faded)}
.resource-list em{color:var(--ink);font-style:italic}
.resource-list a{color:var(--red);text-decoration:none}
.resource-list a:hover{border-bottom:1px solid var(--red)}

/* ── Guide links ─────────────────────────────────── */
a.sim-link{display:inline-block;margin:14px 0 4px;color:var(--paper);background:var(--red);text-decoration:none;font-family:var(--sans);font-size:12.5px;font-weight:600;padding:7px 16px;border-radius:3px;letter-spacing:.03em;transition:background .2s}
a.sim-link:hover{background:var(--leather)}
a.learn-more{color:var(--red);text-decoration:none;font-family:var(--sans);font-size:13px;font-weight:500}
a.learn-more:hover{border-bottom:1px solid var(--red)}

/* ── Page nav ───────────────────────────────────── */
.page-nav{display:flex;justify-content:space-between;margin-top:40px;padding-top:20px;border-top:1px solid var(--parchment);font-size:14px}
.page-nav a{color:var(--red);text-decoration:none}
.page-nav a:hover{text-decoration:underline}
.nav-label{font-family:var(--sans);font-size:11px;color:var(--ink-ghost);margin-bottom:3px}
.nav-title{font-size:14.5px;color:var(--ink)}

/* ── Typography ─────────────────────────────────── */
h2{font-size:20px;font-weight:600;margin-top:28px;margin-bottom:16px}
p{font-size:14.5px;line-height:1.75}
@media(max-width:860px){h2{font-size:18px}p,.method-desc,.method-example,.method-analogy{font-size:14.5px}}

/* ── Right-side Page TOC ─────────────────────────── */
.section,.method-section{scroll-margin-top:68px}
.page-toc{position:fixed;right:20px;top:90px;width:190px;max-height:calc(100vh - 120px);overflow-y:auto;font-family:var(--sans);z-index:50;padding:12px 14px;background:var(--paper);border:1px solid var(--parchment);border-radius:4px;}
@media(max-width:1300px){.page-toc{display:none}}
.page-toc .toc-header{font-size:9px;font-weight:700;letter-spacing:.12em;text-transform:uppercase;color:var(--ink-ghost);margin-bottom:10px;padding-bottom:7px;border-bottom:1px solid var(--parchment);}
.page-toc a{display:block;font-size:11.5px;color:var(--ink-ghost);text-decoration:none;padding:3px 0 3px 10px;border-left:2px solid transparent;line-height:1.35;transition:color .15s,border-color .15s;margin-bottom:1px;}
.page-toc a.toc-sub{font-size:10px;padding-left:20px;opacity:.75;}
.page-toc a:hover{color:var(--ink);}
.page-toc a.active{color:var(--red);border-left-color:var(--red);font-weight:600;}
.page-toc a.toc-sub.active{opacity:1;}
</style>
</head>
<body class="en">
<div class="layout">
  <aside class="sidebar" id="sidebar">
    <a class="sb-brand" href="index.html"><h2>Methods <span>Notebook</span></h2><div class="sb-sub">Jialing Wu</div></a>
    <div class="sb-cat">Person-Centered Quantitative Methods</div>
    <a class="sb-link" href="lpa.html"><span class="sb-num">01</span> Latent Profile Analysis</a>
    <div class="sb-cat">Computational Social Science</div>
    <div class="sb-subcat">Foundations</div>
    <a class="sb-link active" href="machine-learning.html"><span class="sb-num">01</span> Machine Learning</a>
    <a class="sb-link" href="llm.html" style="display:none"><span class="sb-num">02</span> LLM &amp; NLP</a>
    <a class="sb-link" href="text-analysis.html"><span class="sb-num">03</span> Text as Data</a>
    <a class="sb-link" href="theoretical-modeling.html"><span class="sb-num">04</span> Theoretical Modeling</a>
    <div class="sb-cat">Statistics</div>
    <div class="sb-subcat">Foundations</div>
    <a class="sb-link" href="empirical-modeling.html"><span class="sb-num">01</span> Empirical Modeling</a>
    <div class="sb-footer"><a href="https://jialing-wu.github.io">&larr; My Website</a></div>
  </aside>
  <div class="sidebar-overlay" id="overlay" onclick="document.getElementById('sidebar').classList.toggle('open');document.getElementById('overlay').classList.toggle('show')"></div>
  <div class="main">
    <div class="topbar">
      <button class="menu-toggle" onclick="document.getElementById('sidebar').classList.toggle('open');document.getElementById('overlay').classList.toggle('show')"><span></span></button>
      <div class="breadcrumb">Computational Social Science<span class="sep">/</span>Foundations<span class="sep">/</span>Machine Learning</div>
      <div class="topbar-lang">
        <button class="lang-btn" id="btn-zh" onclick="setLang('zh')">中文</button>
        <button class="lang-btn" id="btn-en" onclick="setLang('en')">EN</button>
      </div>
    </div>
    <div class="content">

      <!-- HEADER -->
      <div class="method-header">
        <h1>Machine Learning</h1>
        <div class="method-meta">Computational Social Science &middot; Foundations 01</div>
        <div data-lang="en"><p class="subtitle">Notes from ECE 5307, The Ohio State University &middot; <a href="https://www.youtube.com/playlist?list=PLsN6ERo2QGXKeQ7QFwbTxLW0sXPB-kFmq" target="_blank" style="color:var(--red);border-bottom:1px solid var(--red)">Video Lectures</a> &middot; <a href="https://ece.osu.edu/people/schniter.1" target="_blank" style="color:var(--red);border-bottom:1px solid var(--red)">Prof. Schniter</a></p></div>
        <div data-lang="zh"><p class="subtitle">笔记整理自 ECE 5307, The Ohio State University &middot; <a href="https://www.youtube.com/playlist?list=PLsN6ERo2QGXKeQ7QFwbTxLW0sXPB-kFmq" target="_blank" style="color:var(--red);border-bottom:1px solid var(--red)">课程视频</a> &middot; <a href="https://ece.osu.edu/people/schniter.1" target="_blank" style="color:var(--red);border-bottom:1px solid var(--red)">Prof. Schniter</a></p></div>
      </div>

      <!-- INTRO CARDS -->
      <div class="intro-cards">
        <div class="intro-card">
          <div class="card-label" data-lang="en">What Is ML?</div>
          <div class="card-label" data-lang="zh">什么是机器学习？</div>
          <div data-lang="en"><p>Machine learning is about algorithms that learn patterns from data. Instead of writing explicit rules, we train models by showing them examples, allowing them to discover underlying patterns and make predictions on new, unseen data.</p></div>
          <div data-lang="zh"><p>机器学习是研究能从数据中学习规律的算法。与其手动编写规则，我们训练模型来学习数据中的模式，让它能对新数据做出预测。</p></div>
        </div>
        <div class="intro-card">
          <div class="card-label" data-lang="en">Prerequisites</div>
          <div class="card-label" data-lang="zh">前置知识</div>
          <div data-lang="en"><p>Basic statistics (mean, variance, probability) and programming experience are essential. Familiarity with linear algebra (matrices, vectors) and calculus (derivatives) is strongly recommended for understanding model mathematics.</p></div>
          <div data-lang="zh"><p>需要基本的统计学知识（均值、方差、概率）和编程经验。熟悉线性代数和微积分有助于理解模型数学原理。</p></div>
        </div>
        <div class="intro-card">
          <div class="card-label" data-lang="en">Software</div>
          <div class="card-label" data-lang="zh">软件工具</div>
          <div data-lang="en"><p>Python with scikit-learn (classical ML), PyTorch (deep learning), pandas (data manipulation). R users: caret, tidymodels. The concepts are language-agnostic.</p></div>
          <div data-lang="zh"><p>Python：scikit-learn（经典 ML）、PyTorch（深度学习）、pandas（数据处理）。R 用户可用 caret、tidymodels。这些概念不限于特定语言。</p></div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ===== ML OVERVIEW ===== -->
      <div class="section" id="ml-overview">
        <div class="challenge-overview">
          <div data-lang="en"><p>Machine learning solves a fundamental social science problem: <strong>when do we have too much data to analyze by hand, or relationships too complex to model with simple formulas?</strong> From predicting voter turnout to detecting sentiment in millions of tweets, from identifying influential networks to classifying protest rhetoric, machine learning provides a systematic framework. This page organizes models by the problem they solve: regression (predict a number), classification (predict a category), unsupervised learning (discover hidden structure), and the training techniques that make it all work.</p></div>
          <div data-lang="zh"><p>机器学习解决了社会科学的一个根本问题：<strong>当我们有太多数据无法手工分析，或关系太复杂无法用简单公式建模时怎么办？</strong>从预测选民投票率到检测数百万条推文的情感，从识别影响力网络到分类抗议言论，机器学习提供了系统化框架。本页按解决的问题组织模型：回归（预测数字）、分类（预测类别）、无监督学习（发现隐藏结构）、以及使一切成为可能的训练技术。</p></div>
        </div>

        <div class="modules-overview">
          <div class="module-card">
            <h4 data-lang="en">Q1 · What Problem Are You Solving?</h4>
            <h4 data-lang="zh">Q1 · 你要解决什么问题？</h4>
            <p data-lang="en">Predict a number (regression), assign a label (classification), or discover hidden structure (unsupervised). Three sub-modules cover all three.</p>
            <p data-lang="zh">预测数字（回归）、分配标签（分类）、还是发现隐藏结构（无监督）。三个子模块分别覆盖。</p>
          </div>
          <div class="module-card">
            <h4 data-lang="en">Q2 · How Do You Train a Model?</h4>
            <h4 data-lang="zh">Q2 · 模型怎么训练？</h4>
            <p data-lang="en">Gradient descent to optimize, loss functions to measure error, regularization to prevent overfitting.</p>
            <p data-lang="zh">梯度下降做优化、损失函数衡量误差、正则化防止过拟合。</p>
          </div>
          <div class="module-card">
            <h4 data-lang="en">Q3 · How Do Neural Networks Work?</h4>
            <h4 data-lang="zh">Q3 · 神经网络怎么工作？</h4>
            <p data-lang="en">Architecture, activation functions, backpropagation, and CNNs for images.</p>
            <p data-lang="zh">网络架构、激活函数、反向传播、图像 CNN。</p>
          </div>
          <div class="module-card">
            <h4 data-lang="en">Q4 · What Math Do You Need?</h4>
            <h4 data-lang="zh">Q4 · 需要什么数学？</h4>
            <p data-lang="en">Linear algebra for data, probability for inference, optimization for training — with cross-references to where each is used.</p>
            <p data-lang="zh">线性代数处理数据、概率做推断、优化做训练——附交叉引用。</p>
          </div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ===== SECTION 1: REGRESSION ===== -->
      <div class="section" id="ml-regression">
        <h2 data-lang="en">Module 1a · Predicting Numbers: Regression</h2>
        <h2 data-lang="zh">模块 1a · 预测数值：回归</h2>

        <div class="challenge-overview">
          <div data-lang="en"><p>Picture this: your supervisor hands you a spreadsheet with 120 rows, one per country. Each row has the country's GDP per capita, trade openness, and Polity IV democracy score (−10 = dictatorship, +10 = full democracy). A new country appears — you know its GDP and trade, but not its democracy score. <strong>Can you make a reasonable prediction from the patterns in your data?</strong> That's regression. Unlike classification (which guesses a category — war vs. peace, winner vs. loser), regression predicts a <em>continuous number</em>: a democracy score, a GDP growth rate, an election margin, a refugee count. The four methods in this module form a ladder from simplest to most powerful — knowing when to step up is the core skill.</p></div>
          <div data-lang="zh"><p>想象一下：你的导师给你一个含有 120 行数据的表格，每行对应一个国家，记录着人均 GDP、贸易开放度和 Polity IV 民主程度得分（−10 = 独裁，+10 = 完全民主）。现在出现了一个新国家——你知道它的 GDP 和贸易情况，但不知道民主程度。<strong>你能从已有数据的规律中做出合理预测吗？</strong>这就是回归。与分类（猜一个类别——有战争/没有战争、胜者/败者）不同，回归预测的是<em>连续数字</em>：民主程度得分、GDP 增长率、选举差距、难民数量。本模块的四种方法构成一个从简单到强大的阶梯——知道何时需要升级，才是真正的核心技能。</p></div>
        </div>

        <!-- Linear Regression -->
        <div class="method-section">
          <h3 data-lang="en">Linear Regression (OLS)</h3>
          <h3 data-lang="zh">线性回归（OLS）</h3>

          <div class="method-analogy" data-lang="en">Open a scatter plot: 120 dots, each dot one country, x-axis = log GDP, y-axis = democracy score. Some rich countries are autocracies (outliers, top-left), some poor countries are democracies (bottom-right), but there's a rough upward trend. You want to draw the single straight line that splits the difference across all 120 dots simultaneously — not chasing any one outlier, but finding the best overall compromise. That line is your model. Its slope tells you: "for every 1-unit increase in log GDP, democracy score goes up by about X points." Finding that best line is exactly what OLS does — it's the same regression you run in Stata or R.</div>
          <div class="method-analogy" data-lang="zh">打开一张散点图：120 个点，每个点是一个国家，横轴是 log GDP，纵轴是民主程度得分。一些富有国家是独裁政体（右上角的异常点），一些贫穷国家是民主政体（左下角的异常点），但整体上有一个向上的趋势。你想画一条直线，对全部 120 个点同时取得最好的折中——不追某一个异常点，而是找到整体最优的妥协。这条线就是你的模型。它的斜率告诉你："log GDP 每增加 1 个单位，民主程度大约高出 X 分。"找到这条最佳直线，正是 OLS 的工作——和你在 Stata 或 R 里跑的回归完全是一回事。</div>

          <div class="method-desc" data-lang="en">The model's central idea: your outcome (democracy score) is approximately a recipe — take each input variable, multiply it by a learned weight, add them all up, and you get a prediction. Those weights are called <em>coefficients</em>. A coefficient of +2.3 on log GDP means: "holding trade openness constant, a 10× increase in GDP is associated with 2.3 more democracy points on average." The model learns these weights from your data by finding the combination that makes the fewest, smallest prediction errors across all 120 countries.</div>
          <div class="method-desc" data-lang="zh">模型的核心思想：你的结果（民主程度得分）大约等于一个"配方"——把每个输入变量乘以一个学到的权重，全部加在一起，就得到预测值。这些权重叫做<em>系数</em>。log GDP 的系数 +2.3 意味着："控制贸易开放度不变，GDP 增加 10 倍平均对应民主程度高出 2.3 分。"模型通过找出一组系数，使 120 个国家的预测误差尽可能小，从而从数据中学到这些权重。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">How OLS finds the best coefficients — built from scratch</span><span data-lang="zh">OLS 如何找到最佳系数——从零开始搭建</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>Step 1: Write the prediction formula.</strong> Start with just one predictor. If your only input is log GDP, the model says: ŷ = θ₀ + θ₁ × log_GDP. Here θ₀ is the "starting point" (predicted score when log GDP = 0) and θ₁ is the slope. Add more predictors by adding more terms: ŷ = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₖxₖ. Each θⱼ is one weight to be learned.</p>
              <p data-lang="zh"><strong>步骤 1：写出预测公式。</strong>先从一个预测变量开始。如果你唯一的输入是 log GDP，模型说：ŷ = θ₀ + θ₁ × log_GDP。θ₀ 是"起点"（log GDP = 0 时的预测得分），θ₁ 是斜率。添加更多预测变量只需增加更多项：ŷ = θ₀ + θ₁x₁ + θ₂x₂ + ... + θₖxₖ。每个 θⱼ 是一个待学习的权重。</p>

              <p data-lang="en"><strong>Step 2: Measure how wrong you are.</strong> For each country i, your error is (yᵢ − ŷᵢ). But if you just add up errors, positive and negative mistakes cancel out — a model that's +5 wrong on half the countries and −5 wrong on the other half would look perfect. So instead, square each error (making all errors positive, and punishing big mistakes much more heavily), then average:</p>
              <p data-lang="zh"><strong>步骤 2：衡量你错了多少。</strong>对每个国家 i，误差是 (yᵢ − ŷᵢ)。但如果直接把误差加总，正负误差会相消——对一半国家偏高 5 分、另一半偏低 5 分的模型看起来"完美"。所以改为对每个误差取平方（使所有误差为正，并对大错误施以更重惩罚），再取平均：</p>

              <div class="formula">J(θ) = (1/2n) · Σᵢ (yᵢ − ŷᵢ)²</div>

              <p data-lang="en"><strong>Step 3: Find the coefficients that minimize J.</strong> J is shaped like a smooth bowl — one unique lowest point. We can find it exactly using calculus: set the derivative of J with respect to each θⱼ to zero and solve the system of equations. The result is the <em>Normal Equation</em> — a formula that gives the optimal weights in one shot:</p>
              <p data-lang="zh"><strong>步骤 3：找到使 J 最小的系数。</strong>J 的形状像一个光滑的碗——有且只有一个最低点。用微积分可以精确找到它：把 J 对每个 θⱼ 的导数都设为零，解方程组。结果就是<em>法线方程</em>——一个一步给出最优权重的公式：</p>

              <div class="formula">θ* = (X^T X)^{−1} X^T y</div>

              <p data-lang="en"><strong>Reading the formula:</strong> X^T y asks "how does each feature correlate with the outcome?" (X^T X)^{−1} then adjusts for the fact that features may be correlated with <em>each other</em> — so if GDP and trade openness move together, neither gets full credit alone. Together they give every variable its fair share of explanatory weight, controlling for all other variables. You don't iterate — just plug in the data and get the answer.</p>
              <p data-lang="zh"><strong>解读这个公式：</strong>X^T y 问的是"每个特征与结果有多大相关性？"(X^T X)^{−1} 再调整特征之间互相相关的问题——如果 GDP 和贸易开放度本身就高度相关，两者不能都独揽全部功劳。合在一起，每个变量得到公平的解释权重，同时控制了其他所有变量。不需要反复迭代——输入数据直接得出答案。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">120 countries, three predictors: log GDP per capita, trade openness (% of GDP), log population. Normal Equation gives the exact coefficients in one calculation. Result: θ₁ ≈ +2.3 on log GDP — meaning a 10× increase in GDP is associated with 2.3 more democracy points, holding trade and population constant. In Stata, this is exactly <code>reg polity log_gdp trade log_pop</code>; in R, <code>lm(polity ~ log_gdp + trade + log_pop)</code>.</div>
          <div class="method-example" data-lang="zh">120 个国家，三个预测变量：人均 log GDP、贸易开放度（占 GDP %）、log 人口规模。法线方程一次计算就得出精确系数。结果：log GDP 的 θ₁ ≈ +2.3——控制贸易和人口后，GDP 增加 10 倍对应民主程度高出 2.3 分。在 Stata 里，这就是 <code>reg polity log_gdp trade log_pop</code>；在 R 里，是 <code>lm(polity ~ log_gdp + trade + log_pop)</code>。</div>

          <p data-lang="en"><a class="sim-link" href="guides/ml-linear-regression.html">▶ Deep Dive: Linear Regression from First Principles</a></p>
          <p data-lang="zh"><a class="sim-link" href="guides/ml-linear-regression.html">▶ 深入：从第一性原理理解线性回归</a></p>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> The relationship looks roughly linear; you need interpretable coefficients to report in a paper (each θⱼ has a clear "one unit increase → this much change" meaning); dataset has fewer than ~50,000 rows and a few hundred predictors at most. This is the right default starting point for almost any social science quantitative analysis.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>关系大致是线性的；你需要在论文中报告可解释的系数（每个 θⱼ 有清晰的"增加一个单位→变化多少"含义）；数据集不超过约 5 万行，预测变量最多几百个。这是几乎所有社会科学定量分析的正确默认起点。</div>
        </div>

        <!-- Ridge vs Lasso -->
        <div class="challenge-overview" style="margin-top:8px">
          <div data-lang="en"><p><strong>The overfitting problem — why plain OLS sometimes fails.</strong> Suppose you have 50 countries but decide to include 60 predictors (every economic, political, and social indicator you can find). OLS will "overfit": it finds weights that fit your 50 training countries almost perfectly — but only by memorizing each country's quirks, not learning any real pattern. On 10 new countries it has never seen, predictions collapse. The deeper problem: even with more observations than predictors, if you have many correlated predictors (say, 20 different economic indicators that all move together), OLS gives wildly unstable coefficients — the model is too confident in noisy patterns. <strong>Regularization</strong> fixes this by adding a small penalty for large coefficients, shrinking them toward zero. You sacrifice a tiny bit of in-sample fit to gain much better out-of-sample reliability.</p></div>
          <div data-lang="zh"><p><strong>过拟合问题——为什么普通 OLS 有时会失败。</strong>假设你有 50 个国家，但决定放入 60 个预测变量（各种能找到的经济、政治、社会指标）。OLS 会"过拟合"：它找到的权重几乎完美地拟合了 50 个训练国家——但只是在记忆每个国家的特有怪癖，而不是学到任何真实规律。放到 10 个从未见过的新国家上，预测会完全失准。更深层的问题：即使观测数多于预测变量数，如果你有很多相关的预测变量（比如 20 个都同向变动的经济指标），OLS 给出极不稳定的系数——模型对噪声过于自信。<strong>正则化</strong>通过对大系数施加小额惩罚、使其向零收缩来修复这个问题。你牺牲一点样本内拟合，换取更好的样本外可靠性。</p></div>
        </div>

        <div class="compare-grid">
          <div style="grid-column:1/-1;padding:14px 18px 10px;border-bottom:1px solid var(--parchment)">
            <p data-lang="en" style="margin:0"><strong>Ridge and Lasso</strong> both add a penalty term to the OLS cost function. The difference: Ridge shrinks all coefficients a little but keeps them all; Lasso forces some coefficients all the way to exactly zero, effectively removing those variables from the model entirely.</p>
            <p data-lang="zh" style="margin:0"><strong>Ridge 和 Lasso</strong> 都向 OLS 成本函数增加一个惩罚项。区别在于：Ridge 把所有系数都缩小一点但保留全部；Lasso 把部分系数强制压缩到精确为零，相当于把那些变量彻底从模型中删除。</p>
          </div>

          <div class="compare-col">
            <div class="compare-label" data-lang="en">Ridge (L2 Regularization)</div>
            <div class="compare-label" data-lang="zh">岭回归（L2 正则化）</div>
            <div data-lang="en">
              <p class="formula">min (1/2n)||y−Xθ||² + λ · Σⱼ θⱼ²</p>
              <p><strong>Think of it as:</strong> every coefficient has a rubber band pulling it toward zero. The band is weak (controlled by λ), so no coefficient snaps to zero — they all shrink a little. The larger the coefficient, the harder the band pulls. Closed-form solution still exists:</p>
              <p class="formula">θ* = (X^T X + λI)^{−1} X^T y</p>
              <p><strong>Best for:</strong> situations where all your predictors plausibly matter and you just want to prevent any single one from dominating — e.g., 20 correlated demographic variables in a voter turnout model.</p>
            </div>
            <div data-lang="zh">
              <p class="formula">min (1/2n)||y−Xθ||² + λ · Σⱼ θⱼ²</p>
              <p><strong>类比：</strong>每个系数都有一根橡皮筋把它往零的方向拉。橡皮筋力度由 λ 控制——力度弱，所以没有系数会被拉到零，但都会收缩一些。系数越大，橡皮筋拉力越大。仍有闭式解：</p>
              <p class="formula">θ* = (X^T X + λI)^{−1} X^T y</p>
              <p><strong>最适合：</strong>所有预测变量可能都有用、只是想避免某个变量过度主导的情况——比如投票率模型中 20 个相关的人口统计变量。</p>
            </div>
          </div>

          <div class="compare-col alt">
            <div class="compare-label" data-lang="en">Lasso (L1 Regularization)</div>
            <div class="compare-label" data-lang="zh">套索回归（L1 正则化）</div>
            <div data-lang="en">
              <p class="formula">min (1/2n)||y−Xθ||² + λ · Σⱼ |θⱼ|</p>
              <p><strong>Think of it as:</strong> coefficients that don't earn their keep get set to exactly zero. If you have 200 word-count features to predict news sentiment, but only 15 words actually signal the sentiment, Lasso zeros out the other 185 — automatic variable selection built in. No closed-form solution; solved by coordinate descent.</p>
              <p><strong>Best for:</strong> high-dimensional settings where you suspect most predictors are irrelevant — text data, genetic data, survey batteries. The sparse output (many zeros) is easy to interpret.</p>
            </div>
            <div data-lang="zh">
              <p class="formula">min (1/2n)||y−Xθ||² + λ · Σⱼ |θⱼ|</p>
              <p><strong>类比：</strong>不"赚回自身成本"的系数会被精确设为零。如果你有 200 个词频特征来预测新闻情感，但只有 15 个词真正传递情感信号，Lasso 会把其余 185 个归零——自动变量选择内置其中。没有闭式解，用坐标下降法求解。</p>
              <p><strong>最适合：</strong>你怀疑大多数预测变量无关的高维情境——文本数据、基因数据、调查量表。稀疏的输出结果（很多零）非常容易解读。</p>
            </div>
          </div>

          <div class="compare-footer" data-lang="en"><strong>The key parameter λ</strong> controls how strong the penalty is. λ = 0 means plain OLS; larger λ means more shrinkage. You don't guess λ — you test a range of values on held-out data (cross-validation) and pick the one that predicts best on data the model hasn't seen.</div>
          <div class="compare-footer" data-lang="zh"><strong>关键参数 λ</strong> 控制惩罚力度。λ = 0 就是普通 OLS；λ 越大，收缩越强。你不需要猜 λ——在保留数据上测试一系列 λ 值（交叉验证），选出对未见过数据预测最准的那个。</div>
        </div>

        <!-- Polynomial Regression -->
        <div class="method-section">
          <h3 data-lang="en">Polynomial Regression</h3>
          <h3 data-lang="zh">多项式回归</h3>

          <div class="method-analogy" data-lang="en">Look at the relationship between inequality (Gini coefficient) and political conflict across countries. At very low inequality, conflict is rare — society is roughly equal, little to fight over. At very high inequality, conflict is also surprisingly rare — elites dominate completely, opposition can't organize. But at medium inequality? That's where conflict peaks, because grievances are strong but challengers still have resources to act. This inverted-U pattern is invisible to a straight line — a line can only go up or down. Polynomial regression gives the line permission to bend, capturing curves that OLS misses entirely.</div>
          <div class="method-analogy" data-lang="zh">看不平等程度（基尼系数）与各国政治冲突之间的关系。极低不平等时，冲突很少——社会相对平等，没什么值得争夺的。极高不平等时，冲突也出奇地少——精英完全主导，反对力量无法组织起来。但中等不平等呢？那才是冲突的峰值，因为怨恨强烈，而挑战者还有资源付诸行动。这种倒 U 形对直线是看不见的——直线只能上升或下降。多项式回归允许线条弯曲，捕捉 OLS 完全错过的曲线关系。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">The polynomial trick — making curves from a straight-line model</span><span data-lang="zh">多项式技巧——用直线模型画出曲线</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>The core insight:</strong> a curve is still "linear in the coefficients" if you treat x², x³, etc. as separate input variables. You don't need a new model — just transform your data and run the same OLS machinery.</p>
              <p data-lang="zh"><strong>核心洞见：</strong>如果把 x²、x³ 等看作独立的输入变量，曲线仍然对系数是"线性的"。你不需要新模型——只需转换数据，跑同样的 OLS 即可。</p>

              <p data-lang="en"><strong>Step 1:</strong> Start with one feature x (e.g., Gini coefficient = 0.45 for a specific country). A plain OLS model predicts: ŷ = θ₀ + θ₁ × 0.45 — constant slope regardless of where on the scale you are.</p>
              <p data-lang="zh"><strong>步骤 1：</strong>从一个特征 x 开始（例如某国的基尼系数 = 0.45）。普通 OLS 预测：ŷ = θ₀ + θ₁ × 0.45——无论 x 在哪里，斜率都恒定。</p>

              <p data-lang="en"><strong>Step 2:</strong> Create new columns x² and x³ by squaring and cubing your original feature. No new surveys needed — purely mathematical transformation. Your dataset now has columns: [1, x, x², x³].</p>
              <p data-lang="zh"><strong>步骤 2：</strong>对原始特征取平方和立方，生成新的列 x² 和 x³。不需要新调查——纯数学变换。你的数据集现在有列：[1, x, x², x³]。</p>

              <div class="formula">ŷ = θ₀ + θ₁x + θ₂x² + θ₃x³</div>

              <p data-lang="en"><strong>Step 3:</strong> Run plain OLS on these four columns as if they were completely separate variables. The math is identical to before — only the inputs changed. The resulting curve can bend, peak, and curve back down. A degree-2 polynomial can fit a U-shape or inverted-U; degree-3 can fit S-curves.</p>
              <p data-lang="zh"><strong>步骤 3：</strong>把这四列当作完全独立的变量，照常跑 OLS。数学和之前完全一样——只是输入变了。结果曲线可以弯曲、到达峰值、再弯回来。2 次多项式可以拟合 U 形或倒 U 形；3 次可以拟合 S 形曲线。</p>

              <p data-lang="en"><strong>The catch:</strong> degree-10 polynomial fits your training data almost perfectly — but wobbles wildly between data points and fails terribly on new data. More flexibility = more overfitting risk. Use cross-validation to choose the degree d that generalizes best.</p>
              <p data-lang="zh"><strong>注意：</strong>10 次多项式几乎能完美拟合训练数据——但在数据点之间剧烈抖动，在新数据上表现极差。灵活性越大 = 过拟合风险越高。用交叉验证选择泛化最好的次数 d。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">Gini coefficient and conflict onset across 90 countries: a straight OLS line shows a weak positive slope (more inequality → more conflict), R² = 0.08. A degree-2 polynomial (quadratic) fits a clear inverted-U — conflict peaks at Gini ≈ 0.42, R² = 0.21. The bend in the curve is theoretically meaningful: at very high inequality, repression is complete enough to suppress conflict.</div>
          <div class="method-example" data-lang="zh">90 个国家的基尼系数与冲突爆发：普通 OLS 直线显示微弱的正斜率（不平等越高→冲突越多），R² = 0.08。2 次多项式（二次方程）拟合出清晰的倒 U 形——冲突在基尼系数 ≈ 0.42 时达到峰值，R² = 0.21。曲线的弯折在理论上有意义：极高不平等时，镇压已足够彻底，足以压制冲突。</div>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> You have theoretical or visual evidence of a curved relationship (try plotting your variables first); you have only a few input dimensions (polynomial features explode in number with many variables); you've checked via cross-validation that your chosen degree generalizes to new data.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>你有理论或可视化证据表明存在曲线关系（先画图看看）；只有几个输入维度（多变量时多项式特征数会爆炸式增长）；通过交叉验证确认所选次数能泛化到新数据。</div>
        </div>

        <!-- Neural Network Regression -->
        <div class="method-section">
          <h3 data-lang="en">Neural Network Regression</h3>
          <h3 data-lang="zh">神经网络回归</h3>

          <div class="method-analogy" data-lang="en">Imagine predicting social stability from 50 country-level variables. A linear model assumes each variable contributes independently — add up all the weighted values and you're done. But reality is messier: high inequality is destabilizing, but only when unemployment is also high and state capacity is low. These interactions — "this factor matters more when that other factor is present" — are invisible to OLS and even polynomial regression when you have many variables. A neural network handles this by processing data through multiple layers, like a detective agency where junior analysts each spot one pattern ("poverty and weak courts together"), and senior analysts combine their reports into higher-level signals, until the final analyst outputs one stability score. Each layer is learning its own internal representation of what matters — automatically, from data.</div>
          <div class="method-analogy" data-lang="zh">想象用 50 个国家层面的变量预测社会稳定程度。线性模型假设每个变量独立发挥作用——把所有加权值加起来就完成了。但现实更复杂：高不平等会带来不稳定，但只有当失业率也高、同时国家能力也低时才会如此。这些"交互效应"——"某个因素在另一个因素存在时才重要"——对 OLS 甚至多变量多项式回归来说都是不可见的。神经网络通过多层处理数据来应对这一挑战，就像一家侦探事务所：初级分析师各自发现一个规律（"贫困加上弱法院体系"），高级分析师将他们的报告综合成更高层次的信号，最终由一名分析师输出一个稳定程度分数。每一层都在自动地从数据中学习：什么组合才是真正重要的。</div>

          <div class="method-desc" data-lang="en">A neural network regression model uses the same goal as OLS — minimize the squared prediction error — but replaces the single linear formula with a chain of layers. Each layer transforms its inputs, and crucially, includes a nonlinear step that allows the model to capture interactions and curves that no polynomial could efficiently represent. The final layer outputs a single raw number (no transformation), just like OLS. The trade-off: much more predictive power, but no simple coefficient table to report.</div>
          <div class="method-desc" data-lang="zh">神经网络回归模型的目标与 OLS 相同——最小化预测的平方误差——但用一系列层来替代单一的线性公式。每一层都对输入做变换，关键的是，包含一个非线性步骤，使模型能够捕捉任何多项式都无法高效表示的交互效应和曲线关系。最后一层输出一个原始数字（不做任何变换），和 OLS 一样。代价：预测能力强得多，但没有简洁的系数表可以汇报。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">Inside a neural network — what each layer actually does</span><span data-lang="zh">神经网络内部——每一层实际在做什么</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>Layer 1 (Input):</strong> Your raw features enter as a list of numbers — [GDP=9.2, inequality=0.45, unemployment=0.08, ...]. No computation here; this is just where data arrives.</p>
              <p data-lang="zh"><strong>第 1 层（输入层）：</strong>你的原始特征以一列数字的形式进入——[GDP=9.2, 不平等=0.45, 失业=0.08, ...]。这里不做任何计算，只是数据的入口。</p>

              <p data-lang="en"><strong>Layer 2 (Hidden, repeated):</strong> Each neuron computes a weighted sum of all previous outputs (like OLS), then applies a squashing function called ReLU: if the result is negative, output 0; if positive, keep it as-is. This squashing is where nonlinearity enters — without it, stacking layers would be pointless (adding linear transformations together just gives another linear transformation). Each hidden layer learns to detect different patterns in the data.</p>
              <p data-lang="zh"><strong>第 2 层（隐层，可重复多次）：</strong>每个神经元对上一层所有输出做加权求和（像 OLS 一样），然后应用一个叫 ReLU 的"压缩函数"：如果结果为负，输出 0；如果为正，原样保留。这个压缩就是非线性进入的地方——没有它，叠加层毫无意义（把多个线性变换加在一起只会得到另一个线性变换）。每个隐层学会检测数据中不同的模式。</p>

              <p data-lang="en"><strong>Final layer (Output):</strong> Takes all the patterns detected by the last hidden layer, computes one final weighted sum — no squashing this time — and outputs a single continuous number. That's your prediction. Same shape of output as OLS: one number per observation.</p>
              <p data-lang="zh"><strong>最后一层（输出层）：</strong>接收最后一个隐层检测到的所有模式，计算一次最终的加权求和——这次不压缩——输出一个连续数字。这就是你的预测值。与 OLS 输出形式相同：每个观测输出一个数字。</p>

              <p data-lang="en"><strong>Training:</strong> The network starts with random weights, makes predictions, compares them to actual values using the same MSE formula as OLS — J = (1/n)Σ(yᵢ − ŷᵢ)² — then uses an algorithm called backpropagation to figure out which weights to nudge in which direction. This process repeats thousands of times (each full pass through the data is one "epoch") until J stops improving.</p>
              <p data-lang="zh"><strong>训练：</strong>网络从随机权重开始，做出预测，用与 OLS 相同的 MSE 公式——J = (1/n)Σ(yᵢ − ŷᵢ)²——与真实值比较，再用一种叫反向传播的算法计算出每个权重应该朝哪个方向调整多少。这个过程重复数千次（每次完整地过一遍数据叫一个"轮次"），直到 J 不再改善。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">Conflict onset prediction across 180 countries × 30 years, using 40+ indicators. A plain OLS model achieves F1 = 0.31. A 3-layer neural network (64 neurons → 32 → 16 → 1 output) reaches F1 = 0.47 by learning "poverty × weak institutions" interactions that OLS treats as additive. The cost: you cannot report a simple "log GDP coefficient" — you can only say "the model predicts higher conflict here." Use feature importance tools (SHAP values) to partially recover interpretability.</div>
          <div class="method-example" data-lang="zh">180 个国家 × 30 年、40 余项指标的冲突爆发预测。普通 OLS 达到 F1 = 0.31。3 层神经网络（64 个神经元 → 32 → 16 → 1 个输出）通过学习 OLS 视为简单相加的"贫困 × 制度脆弱"交互效应，达到 F1 = 0.47。代价：你无法报告简单的"log GDP 系数"——只能说"模型在这里预测冲突概率更高"。可以用特征重要性工具（SHAP 值）部分恢复可解释性。</div>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> Relationships are genuinely complex with many interacting variables and no obvious functional form; you have at least several thousand observations (neural networks need data to learn from); prediction accuracy matters more than the ability to explain each coefficient. For causal claims in academic social science, OLS or Ridge is almost always preferred — neural networks are best for pure prediction tasks where "it works well" is sufficient justification.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>关系真正复杂、有许多交互变量、且没有明显的函数形式；你有至少几千个观测（神经网络需要足够的数据来学习）；预测准确率比解释每个系数的能力更重要。在学术社会科学的因果主张中，OLS 或 Ridge 几乎总是首选——神经网络最适合"能用就行"的纯预测任务。</div>
        </div>
      </div>

      <!-- ===== SECTION 2: CLASSIFICATION ===== -->
      <div class="section" id="ml-classification">
        <h2 data-lang="en">Module 1b · Assigning Labels: Classification</h2>
        <h2 data-lang="zh">模块 1b · 分配标签：分类</h2>

        <div class="challenge-overview">
          <div data-lang="en"><p>Module 1a predicted a <em>number</em> — democracy score, GDP, refugee count. But many social science questions have categorical answers: Does civil war break out? (yes/no.) Which party wins the election? (Party A / B / C.) Is this tweet promoting violence? (yes/no.) These are <strong>classification problems</strong> — your model must assign an observation to one of several discrete categories. The key difference from regression: instead of predicting where on a number line something falls, you predict which bucket it belongs to. Crucially, most classifiers don't just output a label — they output a <em>probability</em> for each possible label, which is often far more useful than a hard yes/no.</p></div>
          <div data-lang="zh"><p>模块 1a 预测的是<em>数字</em>——民主程度得分、GDP、难民数量。但许多社会科学问题的答案是类别型的：内战会爆发吗？（是/否）哪个政党赢得选举？（A党/B党/C党）这条推文是否在煽动暴力？（是/否）这些都是<strong>分类问题</strong>——模型必须把一个观测分配到几个离散类别之一。与回归的关键区别：不是预测某个值落在数轴哪里，而是预测它属于哪个桶。关键是，大多数分类模型不只是输出一个标签——它们输出每个可能标签的<em>概率</em>，这往往比简单的"是/否"有用得多。</p></div>
        </div>

        <!-- Logistic Regression -->
        <div class="method-section">
          <h3 data-lang="en">Logistic Regression</h3>
          <h3 data-lang="zh">逻辑回归</h3>

          <div class="method-analogy" data-lang="en">Imagine a political scientist trying to predict democratic backsliding. She has data on 150 countries: GDP per capita, executive constraints, polarization scores, press freedom. She doesn't want to say "Country X will definitely slide into authoritarianism" or "it definitely won't" — she wants a probability: "72% chance of democratic erosion in the next 5 years." Why probability and not just a number? Because different stakeholders care about different thresholds: an election monitor might act at >50%, an international organization at >80%. Logistic regression produces exactly this — a probability for each observation, derived from a weighted sum of your inputs squeezed through a clever curve.</div>
          <div class="method-analogy" data-lang="zh">想象一位研究民主倒退的政治学家。她有 150 个国家的数据：人均 GDP、行政约束、极化程度、新闻自由。她不想说"X 国肯定会滑向威权"或"肯定不会"——她想要一个概率："未来 5 年内发生民主侵蚀的可能性是 72%。"为什么要概率而不是简单的数字？因为不同利益方关心不同的阈值：选举观察员可能在 >50% 时行动，国际组织在 >80% 时。逻辑回归恰好输出这个——把输入变量的加权求和通过一条聪明的曲线压缩，得到每个观测的概率。</div>

          <div class="method-desc" data-lang="en">Despite the name "regression," logistic regression is a <strong>binary classifier</strong>. It builds on the same idea as OLS — compute a weighted sum of inputs — but instead of outputting that raw number directly, it passes it through the sigmoid function, which squashes any number into a probability between 0 and 1. The resulting probability is your model's confidence that this observation belongs to category 1. Apply a threshold (usually 0.5) to get the final label.</div>
          <div class="method-desc" data-lang="zh">尽管名叫"回归"，逻辑回归是一个<strong>二元分类器</strong>。它建立在与 OLS 相同的基础上——计算输入的加权求和——但不直接输出这个原始数值，而是通过 sigmoid 函数把它压缩成一个 0 到 1 之间的概率。这个概率就是模型对"这个观测属于类别 1"的把握程度。设定一个阈值（通常是 0.5）即可得到最终标签。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">From linear score to probability — how logistic regression works</span><span data-lang="zh">从线性得分到概率——逻辑回归如何工作</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>Step 1: Compute a raw score (just like OLS).</strong> z = w₀ + w₁×GDP + w₂×polarization + ... + wₖxₖ. This score can be any real number — positive means "leaning toward class 1," negative means "leaning toward class 0." This is identical to linear regression so far.</p>
              <p data-lang="zh"><strong>步骤 1：计算原始得分（和 OLS 完全一样）。</strong>z = w₀ + w₁×GDP + w₂×极化 + ... + wₖxₖ。这个得分可以是任意实数——正数表示"倾向于类别 1"，负数表示"倾向于类别 0"。到这里为止，和线性回归完全相同。</p>

              <p data-lang="en"><strong>Step 2: Squeeze through the sigmoid.</strong> The sigmoid function σ(z) = 1 / (1 + e^{−z}) converts any raw score to a probability. It has a characteristic S-shape: very negative z → probability near 0; z = 0 → probability = 0.5; very positive z → probability near 1.</p>
              <p data-lang="zh"><strong>步骤 2：通过 sigmoid 压缩。</strong>sigmoid 函数 σ(z) = 1 / (1 + e^{−z}) 将任意原始得分转化为概率。它有特征性的 S 形曲线：z 很负 → 概率接近 0；z = 0 → 概率 = 0.5；z 很正 → 概率接近 1。</p>

              <div class="formula">P(y=1 | x) = σ(z) = 1 / (1 + e^{−z})</div>

              <p data-lang="en"><strong>Step 3: Training with cross-entropy loss.</strong> We don't use squared error here — if the true label is 1 and you predict probability 0.01, you're catastrophically wrong; MSE underpenalizes this. Cross-entropy instead: J = −(1/n) Σᵢ [yᵢ log(pᵢ) + (1−yᵢ) log(1−pᵢ)]. If y=1 and you predict p=0.9 → loss ≈ 0.1. If you predict p=0.01 → loss ≈ 4.6. Being confidently wrong is severely punished.</p>
              <p data-lang="zh"><strong>步骤 3：用交叉熵损失训练。</strong>这里不用平方误差——如果真实标签是 1 而你预测概率为 0.01，你错得离谱；MSE 对此惩罚不够。改用交叉熵：J = −(1/n) Σᵢ [yᵢ log(pᵢ) + (1−yᵢ) log(1−pᵢ)]。若 y=1 且预测 p=0.9 → 损失 ≈ 0.1。若预测 p=0.01 → 损失 ≈ 4.6。越有把握地犯错，惩罚越重。</p>

              <p data-lang="en"><strong>Reading the coefficients: odds ratios.</strong> Each coefficient wⱼ has a direct interpretation: e^{wⱼ} is the <em>odds ratio</em> for a 1-unit increase in xⱼ. If w₁ = 0.8 for polarization, then e^{0.8} ≈ 2.2 — a 1-unit increase in polarization multiplies the odds of democratic backsliding by 2.2, holding other variables constant.</p>
              <p data-lang="zh"><strong>解读系数：几率比。</strong>每个系数 wⱼ 有直接解释：e^{wⱼ} 是 xⱼ 增加 1 个单位对应的<em>几率比</em>。如果极化程度的系数 w₁ = 0.8，则 e^{0.8} ≈ 2.2——控制其他变量不变，极化程度增加 1 个单位，民主倒退的几率乘以 2.2。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">Predicting democratic backsliding across 120 country-years. Logistic regression with 4 predictors: coefficient on executive constraints = −1.4, e^{−1.4} ≈ 0.25 — each additional constraint on executive power multiplies the odds of backsliding by 0.25 (i.e., reduces it by 75%). This odds ratio can be directly reported in a paper. Model achieves AUC = 0.78 — meaning 78% of the time, a randomly chosen "backslid" case gets a higher predicted probability than a "stable" case.</div>
          <div class="method-example" data-lang="zh">预测 120 个国家-年的民主倒退。包含 4 个预测变量的逻辑回归：行政约束系数 = −1.4，e^{−1.4} ≈ 0.25——行政权力约束每多一个，倒退的几率乘以 0.25（即降低 75%）。这个几率比可以直接在论文中报告。模型 AUC = 0.78——即 78% 的情况下，随机选取一个"倒退"案例和一个"稳定"案例，倒退案例的预测概率更高。</div>

          <p data-lang="en"><a class="sim-link" href="guides/ml-logistic-regression.html">▶ Deep Dive: Logistic Regression from First Principles</a></p>
          <p data-lang="zh"><a class="sim-link" href="guides/ml-logistic-regression.html">▶ 深入：从第一性原理理解逻辑回归</a></p>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> Binary outcome (yes/no, 1/0); you need interpretable coefficients (odds ratios) to report in a paper; modest sample sizes (hundreds to tens of thousands); your first-choice classifier for almost any social science binary prediction task.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>二元结果（是/否、1/0）；需要在论文中报告可解释的系数（几率比）；样本量适中（数百到数万）；几乎所有社会科学二元预测任务的首选分类器。</div>
        </div>

        <!-- Support Vector Machine -->
        <div class="method-section">
          <h3 data-lang="en">Support Vector Machine (SVM)</h3>
          <h3 data-lang="zh">支持向量机（SVM）</h3>

          <div class="method-analogy" data-lang="en">Imagine two groups of students sitting in a lecture hall — pro-government and anti-government protesters, separated by their seating preferences. You need to draw an aisle between them. Many aisles would work, but logistic regression just finds one that separates them — it doesn't care how close to the edge some students sit. SVM insists on the <em>widest possible aisle</em>: it positions the dividing line as far as possible from the nearest student on each side. The students sitting right at the edge of the aisle — the ones defining where the boundary goes — are called the <em>support vectors</em>. A wider aisle means more confidence in the classification and better generalization to new students you haven't seen before.</div>
          <div class="method-analogy" data-lang="zh">想象一个报告厅里坐着两组学生——亲政府和反政府的抗议者，按立场分开就座。你需要在他们之间画一条过道。很多过道都能把两组隔开，但逻辑回归只找一条能分开的——不管有没有学生坐得离边界很近。SVM 要求<em>尽可能宽的过道</em>：它把分界线放在与每侧最近的学生尽可能远的位置。那些坐在过道边缘、决定边界位置的学生——就叫做<em>支持向量</em>。过道越宽，分类越自信，对没见过的新学生的泛化能力也越强。</div>

          <div class="method-desc" data-lang="en">SVM finds the <em>maximum-margin hyperplane</em> — the decision boundary that maximizes the distance to the nearest training examples on each side. It doesn't use all training data equally: only the support vectors (the data points sitting right on the margin edges) determine the boundary. This makes SVM robust to outliers far from the boundary and particularly strong when training data is limited.</div>
          <div class="method-desc" data-lang="zh">SVM 找到<em>最大间距超平面</em>——使两侧最近训练样本距离最大的决策边界。它不平等对待所有训练数据：只有支持向量（坐在间距边缘的数据点）决定边界。这使 SVM 对远离边界的异常值具有鲁棒性，在训练数据有限时尤为出色。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">The margin and the kernel trick</span><span data-lang="zh">间距与核技巧</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>The margin:</strong> The "aisle width" is called the margin = 2/||w||. Maximizing the margin means minimizing ||w|| — finding the flattest possible decision boundary. The constraint ensures every training point is correctly classified with margin ≥ 1: yᵢ(wᵀxᵢ + b) ≥ 1 for all i.</p>
              <p data-lang="zh"><strong>间距：</strong>"过道宽度"叫做间距 = 2/||w||。最大化间距等价于最小化 ||w||——找到尽可能"平坦"的决策边界。约束条件确保每个训练点被正确分类，且间距 ≥ 1：对所有 i，yᵢ(wᵀxᵢ + b) ≥ 1。</p>

              <p data-lang="en"><strong>Soft margin (real data is messy):</strong> Real groups always have some overlap — a few pro-government students sitting on the anti-government side. Soft-margin SVM allows some misclassification, controlled by parameter C. Large C = insist on correct classification, narrow margin; small C = allow more violations, wider margin.</p>
              <p data-lang="zh"><strong>软间距（真实数据总是混乱的）：</strong>真实的两组之间总有重叠——一些亲政府学生坐在了反政府一侧。软间距 SVM 允许一定程度的误分类，由参数 C 控制。C 大 = 坚持正确分类，间距窄；C 小 = 允许更多违规，间距宽。</p>

              <p data-lang="en"><strong>The kernel trick (nonlinear boundaries):</strong> Sometimes no straight aisle exists — the groups interleave in complicated ways. The kernel trick maps data to a higher-dimensional space where a straight aisle does exist, without ever computing the transformation explicitly. RBF kernel: K(x,z) = exp(−γ||x−z||²) effectively asks "how similar are x and z?" treating similar points as neighbors regardless of their raw coordinates.</p>
              <p data-lang="zh"><strong>核技巧（非线性边界）：</strong>有时不存在直线过道——两组以复杂方式交织在一起。核技巧把数据映射到一个更高维的空间，在那里直线过道确实存在，而且不需要显式计算这个变换。RBF 核：K(x,z) = exp(−γ||x−z||²) 本质上在问"x 和 z 有多相似？"，把相似的点视为邻居，无论它们的原始坐标如何。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">Classifying news articles as "conflict-related" or "not" using word-count features (10,000 vocabulary words, one feature per word). SVM with linear kernel handles this sparse, high-dimensional input efficiently — each word's coefficient directly shows its discriminative power. SVM outperforms logistic regression when training data is small relative to feature dimension (n=500 articles, p=10,000 features).</div>
          <div class="method-example" data-lang="zh">用词频特征（10,000 个词汇，每个词一个特征）将新闻文章分类为"涉及冲突"或"不涉及"。线性核 SVM 高效处理这类稀疏高维输入——每个词的系数直接显示其判别能力。当训练数据相对特征维度较少时（n=500 篇文章，p=10,000 个特征），SVM 超过逻辑回归。</div>

          <p data-lang="en"><a class="sim-link" href="guides/ml-svm-lagrange.html">▶ Deep Dive: SVM Dual Problem &amp; Kernel Trick</a></p>
          <p data-lang="zh"><a class="sim-link" href="guides/ml-svm-lagrange.html">▶ 深入：SVM 对偶问题与核技巧</a></p>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> High-dimensional sparse features (text, genomics); moderate datasets (hundreds to tens of thousands); when you need a strong baseline with good theoretical guarantees; boundary is complex but you have limited data for neural networks.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>高维稀疏特征（文本、基因组学）；中等规模数据集（数百到数万）；需要理论保证良好的强基线；边界复杂但数据不足以训练神经网络时。</div>
        </div>

        <!-- Softmax / Multinomial -->
        <div class="method-section">
          <h3 data-lang="en">Softmax / Multinomial Logistic Regression</h3>
          <h3 data-lang="zh">Softmax / 多项逻辑回归</h3>

          <div class="method-analogy" data-lang="en">Logistic regression handles two categories — war or peace, backsliding or stable. But what if you have five political regime types: full democracy, partial democracy, anocracy, partial autocracy, full autocracy? Softmax extends logistic regression to any number of categories. Think of it as running five separate "am I this type?" detectors simultaneously, each producing a raw score, then converting all five scores into percentages that must add up to exactly 100%. The category with the highest percentage is your prediction — but you keep all five probabilities for richer interpretation.</div>
          <div class="method-analogy" data-lang="zh">逻辑回归处理两个类别——战争/和平，倒退/稳定。但如果你有五种政治政体类型：完全民主、部分民主、混合政体、部分威权、完全威权呢？Softmax 把逻辑回归扩展到任意数量的类别。把它想象成同时运行五个"我是这个类型吗？"探测器，每个产生一个原始得分，再把五个得分转化为加起来恰好 100% 的百分比。百分比最高的类别就是你的预测——但你保留全部五个概率，以供更丰富的解读。</div>

          <div class="method-desc" data-lang="en">For each class k, compute a separate linear score zₖ = wₖᵀx + bₖ — like running one logistic regression per class. Then softmax converts all scores to probabilities using the formula below. The probabilities sum to 1, and the model is trained with the same cross-entropy loss as binary logistic regression. Softmax is also the standard output layer of neural network classifiers — any classification network ends with a softmax.</div>
          <div class="method-desc" data-lang="zh">对每个类别 k，计算一个独立的线性得分 zₖ = wₖᵀx + bₖ——相当于为每个类别跑一个逻辑回归。然后 softmax 用下面的公式把所有得分转化为概率。概率之和为 1，模型用与二元逻辑回归相同的交叉熵损失训练。Softmax 也是神经网络分类器的标准输出层——任何分类网络都以 softmax 结尾。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">Softmax formula and interpretation</span><span data-lang="zh">Softmax 公式与解读</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>One score per class:</strong> zₖ = wₖᵀx + bₖ for k = 1, ..., K. These are raw numbers — could be anything. Large zₖ means the model thinks class k is likely, but we can't compare them directly yet (different scales).</p>
              <p data-lang="zh"><strong>每个类别一个得分：</strong>zₖ = wₖᵀx + bₖ，k = 1, ..., K。这些是原始数字——可以是任意值。zₖ 大说明模型认为类别 k 可能性高，但还不能直接比较（量纲不同）。</p>

              <div class="formula">P(y=k | x) = e^{zₖ} / (e^{z₁} + e^{z₂} + ... + e^{zK})</div>

              <p data-lang="en"><strong>Why exponentiate?</strong> Exponentials make all scores positive (no negative probabilities), amplify differences (a score of 5 vs 4 becomes a much bigger probability gap than 1 vs 0), and the division ensures everything sums to 1. With K=2, softmax reduces exactly to the sigmoid function from logistic regression.</p>
              <p data-lang="zh"><strong>为什么取指数？</strong>指数使所有得分为正（概率不能为负），放大差异（得分 5 vs 4 比 1 vs 0 造成更大的概率差距），除法确保总和为 1。当 K=2 时，softmax 恰好退化为逻辑回归的 sigmoid 函数。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">Classifying political regime type (5 categories) from economic and political indicators for 160 countries. Softmax outputs probability vectors like [0.05, 0.12, 0.23, 0.48, 0.12] — this country has 48% predicted probability of being a partial autocracy. This is far more informative than a hard label: you can see how "on the border" each prediction is, and build uncertainty estimates for downstream analysis.</div>
          <div class="method-example" data-lang="zh">用 160 个国家的经济和政治指标分类政治政体类型（5 个类别）。Softmax 输出概率向量如 [0.05, 0.12, 0.23, 0.48, 0.12]——该国被预测为部分威权的概率为 48%。这比直接给一个标签信息量丰富得多：你能看到每次预测有多"接近边界"，并为后续分析建立不确定性估计。</div>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> More than two outcome categories; you want probability estimates for each category (not just a hard label); as the output layer of any neural network classifier; when your categories are mutually exclusive and exhaustive.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>结果类别超过两个；你想要每个类别的概率估计（而不仅是一个硬标签）；作为任何神经网络分类器的输出层；当类别互斥且穷举时。</div>
        </div>

        <!-- CNN for Images -->
        <div class="method-section">
          <h3 data-lang="en">CNN for Image Classification</h3>
          <h3 data-lang="zh">卷积神经网络（图像分类）</h3>

          <div class="method-analogy" data-lang="en">When you look at a satellite image to identify whether a building is a hospital or a military barracks, you don't analyze each pixel individually — you notice shapes, textures, patterns of windows, the layout of surrounding structures. Your brain processes the image in layers: first detecting edges and colors, then shapes like rectangles and circles, then higher-level features like "rooftop pattern" and "courtyard," and finally the category. A CNN does exactly this: layer by layer, it builds up from raw pixels to abstract categories, learning which local patterns in images predict which labels.</div>
          <div class="method-analogy" data-lang="zh">当你查看卫星图像以判断一栋建筑是医院还是军营时，你不是逐像素分析的——你会注意到形状、纹理、窗户的排列、周围建筑的布局。你的大脑分层处理图像：先检测边缘和颜色，再识别矩形和圆形等形状，然后是"屋顶模式"和"庭院"等高层特征，最后才是类别。CNN 正是如此：逐层从原始像素构建到抽象类别，学习图像中的哪些局部模式预测哪些标签。</div>

          <div class="method-desc" data-lang="en">A CNN replaces the flat "multiply-by-weights" operation of a regular neural network with <em>convolution</em>: small learned filters (e.g., 3×3 pixels) slide across the image, computing a dot product at each position. Each filter detects one type of local pattern — horizontal edges, vertical edges, diagonal textures. Deeper layers combine these simple detections into complex features. The final layers are regular fully-connected layers that map these features to class probabilities via softmax.</div>
          <div class="method-desc" data-lang="zh">CNN 用<em>卷积</em>替代了普通神经网络的"乘以权重"操作：小型学习到的滤波器（如 3×3 像素）在图像上滑动，在每个位置计算点积。每个滤波器检测一种局部模式——水平边缘、垂直边缘、斜向纹理。更深的层把这些简单检测组合成复杂特征。最后几层是普通的全连接层，通过 softmax 把这些特征映射到类别概率。</div>

          <div class="math-box">
            <div class="math-box-title"><span data-lang="en">What a convolution actually does</span><span data-lang="zh">卷积实际上在做什么</span></div>
            <div class="math-content">
              <p data-lang="en"><strong>A single filter pass:</strong> Take a 3×3 filter (9 learned numbers). Place it over a 3×3 patch of the image. Multiply each filter value by the corresponding pixel value, sum all 9 products. Slide the filter one pixel to the right, repeat. The result is an "activation map" — high values where the image matches the filter's pattern, low values elsewhere.</p>
              <p data-lang="zh"><strong>单个滤波器的操作：</strong>取一个 3×3 的滤波器（9 个学到的数值）。把它放在图像的一个 3×3 区域上。把每个滤波器值和对应的像素值相乘，把 9 个乘积加总。把滤波器向右移动一个像素，重复。结果是一个"激活图"——图像与滤波器模式匹配的地方值高，其他地方值低。</p>

              <p data-lang="en"><strong>Why this beats a plain neural network for images:</strong> A 256×256 image has 65,536 pixels. A plain neural network treating each pixel as an independent input would need millions of parameters just for the first layer — and wouldn't know that a "horizontal edge" at top-left is the same kind of thing as a "horizontal edge" at bottom-right. Convolution reuses the same filter everywhere, exploiting the fact that patterns can appear anywhere in an image. This drastically reduces parameters and improves generalization.</p>
              <p data-lang="zh"><strong>为什么它比普通神经网络更适合图像：</strong>一张 256×256 的图像有 65,536 个像素。把每个像素当作独立输入的普通神经网络，仅第一层就需要数百万个参数——而且不知道左上角的"水平边缘"和右下角的"水平边缘"是同一种东西。卷积在图像各处复用同一个滤波器，利用了"模式可以出现在图像任何位置"这一事实。这大幅减少参数量，改善泛化能力。</p>
            </div>
          </div>

          <div class="method-example" data-lang="en">Conflict research: satellite imagery of villages before and after conflict events. CNN trained to classify "damaged infrastructure" vs. "intact" achieves ~87% accuracy, enabling large-scale automated conflict damage assessment across thousands of images — a task that would take human coders months. Also used in: detecting regime censorship of social media images, classifying protest photos, estimating crop yields from aerial images.</div>
          <div class="method-example" data-lang="zh">冲突研究：冲突事件前后的村庄卫星图像。训练用于分类"受损基础设施"与"完好"的 CNN 达到约 87% 准确率，使大规模自动化冲突损毁评估成为可能——对数千张图像的任务，人工编码员需要几个月。也用于：检测政权对社交媒体图像的审查、分类抗议照片、从航空图像估算作物产量。</div>

          <div class="method-when" data-lang="en"><strong>When to use:</strong> Your data is images or has 2D spatial structure (maps, spectrograms, network adjacency matrices visualized as heatmaps). Pre-trained CNNs (ResNet, EfficientNet, ViT) are usually fine-tuned rather than trained from scratch — transfer learning makes CNNs accessible even with modest labeled datasets.</div>
          <div class="method-when" data-lang="zh"><strong>何时用：</strong>你的数据是图像，或具有 2D 空间结构（地图、频谱图、以热图形式可视化的网络邻接矩阵）。通常对预训练 CNN（ResNet、EfficientNet、ViT）进行微调，而不是从头训练——迁移学习使 CNN 即使在标注数据有限时也可使用。</div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ===== SECTION 3: UNSUPERVISED LEARNING ===== -->
      <div class="section" id="ml-unsupervised">
        <h2 data-lang="en">Module 1c · Finding Hidden Structure: Unsupervised Learning</h2>
        <h2 data-lang="zh">模块 1c · 发现隐藏结构：无监督学习</h2>

        <div class="challenge-overview">
          <div data-lang="en"><p>You have data but <strong>no labels</strong>. No "correct answers" to learn from. Goal: discover hidden structure — groupings, dimensions, patterns. Unsupervised learning is often <strong>exploratory</strong>: "What clusters exist in customer data?" "What are the underlying themes in these documents?" "Can we compress this 50-dimensional dataset to 2D for visualization?"</p></div>
          <div data-lang="zh"><p>你有数据但<strong>没有标签</strong>。没有"正确答案"可学。目标：发现隐藏结构——分组、维度、规律。无监督学习通常是<strong>探索性的</strong>："客户数据中有什么集群？""这些文档的基础主题是什么？""能把这个 50 维数据集压缩到 2D 用于可视化吗？"</p></div>
        </div>

        <!-- K-Means -->
        <div class="method-section">
          <h3 data-lang="en">K-Means Clustering</h3>
          <h3 data-lang="zh">K均值聚类</h3>
          <div data-lang="en">
            <p class="method-analogy">Sorting items in an unfamiliar supermarket with no labels. You naturally group similar items together: "all drinks here," "all snacks there." K-Means does this automatically.</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">在没有标签的陌生超市里分类商品。你自然地把相似物品聚在一起："所有饮料在这边"，"所有零食在那边"。K均值自动做这个。</p>
          </div>
          <div class="formula">J = Σ_{k=1}^K Σ_{i∈Cₖ} ||xᵢ − μₖ||²</div>
          <div data-lang="en"><p class="method-desc">Objective: minimize total within-cluster distance. μₖ is the center of cluster k.</p></div>
          <div data-lang="zh"><p class="method-desc">目标：最小化总的类内距离。μₖ 是第 k 个集群的中心。</p></div>
          <div data-lang="en"><p class="method-desc"><strong>Algorithm:</strong> (1) Initialize K centers randomly. (2) E-step: assign each point to nearest center. (3) M-step: recompute centers as cluster means. (4) Repeat until convergence. Each step decreases J, but only guaranteed to reach local optimum.</p></div>
          <div data-lang="zh"><p class="method-desc"><strong>算法：</strong>(1) 随机初始化 K 个中心。(2) E 步：把每个点分配到最近的中心。(3) M 步：重新计算中心为集群均值。(4) 重复直到收敛。每步都降低 J，但只保证到达局部最优。</p></div>
          <div data-lang="en"><p class="method-desc"><strong>Challenge:</strong> You must specify K beforehand. Use the <strong>elbow method</strong>: plot J vs. K, look for the "elbow" point where diminishing returns kick in.</p></div>
          <div data-lang="zh"><p class="method-desc"><strong>挑战：</strong>你必须提前指定 K。使用<strong>肘部法则</strong>：绘制 J vs. K，寻找"肘部"点，那是收益递减开始的地方。</p></div>
          <div data-lang="en"><p class="method-example">E-commerce customer segmentation: cluster users by spending (amount × frequency) into "high-value," "occasional," "at-risk." Then tailor promotions per segment.</p></div>
          <div data-lang="zh"><p class="method-example">电商客户分群：根据消费（金额×频率）把用户聚为"高价值"、"偶尔消费"、"风险"。然后为每段定制促销。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Spherical clusters, K is known or guessable, want fast interpretable results. Not ideal if clusters have very different sizes or shapes.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>球形集群、K 已知或可猜测、想要快速可解释的结果。如果集群大小或形状差异很大不太理想。</p></div>
        </div>

        <!-- Hierarchical Clustering -->
        <div class="method-section">
          <h3 data-lang="en">Hierarchical Clustering</h3>
          <h3 data-lang="zh">层次聚类</h3>
          <div data-lang="en">
            <p class="method-analogy">Building a family tree. Start with each person as their own "family." Merge the two most similar families, repeat until everyone is connected. You can "cut" the tree at any height for different granularities.</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">构建家谱。从每个人作为自己的"家族"开始。合并两个最相似的家族，重复直到所有人相连。你可以在任何高度"剪"树以获得不同粒度。</p>
          </div>
          <div data-lang="en"><p class="method-desc"><strong>Agglomerative approach:</strong> Start bottom-up, merge similar clusters. Produces a dendrogram — a tree showing all merges. Cut at different heights for different numbers of clusters. No need to pre-specify K!</p></div>
          <div data-lang="zh"><p class="method-desc"><strong>凝聚法：</strong>从底层开始向上，合并相似的集群。产生树状图——显示所有合并的树。在不同高度切割得到不同数量的集群。不需要预先指定 K！</p></div>
          <div data-lang="en"><p class="method-example">Gene expression analysis: cluster genes by their expression patterns across samples. Hierarchical structure reveals gene families and functional relationships.</p></div>
          <div data-lang="zh"><p class="method-example">基因表达分析：根据基因在样本中的表达模式聚类。层次结构显示基因家族和功能关系。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Want to explore multiple granularities, data size moderate (O(n²) complexity), clusters may have irregular shapes. Slower than K-Means but more interpretable.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>想探索多个粒度、数据量中等（O(n²) 复杂度）、集群可能有不规则形状。比 K均值慢但更可解释。</p></div>
        </div>

        <!-- PCA -->
        <div class="method-section">
          <h3 data-lang="en">Principal Component Analysis (PCA)</h3>
          <h3 data-lang="zh">主成分分析（PCA）</h3>
          <div data-lang="en">
            <p class="method-analogy">Photographing a building from the most informative angle. You want to capture the most variation (most information) in the fewest dimensions.</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">从最有信息量的角度拍摄建筑。你想用最少维度捕捉最多变化（最多信息）。</p>
          </div>
          <div class="formula">C = (1/n) X^T X = V Λ V^T</div>
          <div data-lang="en"><p class="method-desc">Eigendecompose the covariance matrix. Eigenvectors are principal components (directions of maximum variance). Eigenvalues are the variance along each PC.</p></div>
          <div data-lang="zh"><p class="method-desc">对协方差矩阵进行特征分解。特征向量是主成分（最大方差方向）。特征值是沿每个 PC 的方差。</p></div>
          <div class="formula">X_k = X V_k</div>
          <div data-lang="en"><p class="method-desc">Project data onto the first k principal components (keep the k eigenvectors with largest eigenvalues). This reduces 50 dimensions to 2 or 3 for visualization.</p></div>
          <div data-lang="zh"><p class="method-desc">将数据投影到前 k 个主成分（保留特征值最大的 k 个特征向量）。这把 50 维约化到 2 或 3 维用于可视化。</p></div>
          <div data-lang="en"><p class="method-example">Social science: A 50-item psychology questionnaire measures constructs like extraversion, neuroticism, openness. PCA reveals 5-6 underlying dimensions. Reduces noise and helps identify latent factors.</p></div>
          <div data-lang="zh"><p class="method-example">社会科学：50 项心理学问卷测量外向性、神经质、开放性等。PCA 揭示 5-6 个基础维度。减少噪声并帮助识别潜在因素。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Visualize high-dimensional data, denoise before downstream analysis, preprocessing for models sensitive to dimensionality. Linear only — nonlinear alternatives (t-SNE, UMAP) available.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>可视化高维数据、在下游分析前去噪、为对维度敏感的模型做预处理。仅线性——有非线性替代品（t-SNE、UMAP）。</p></div>
        </div>

        <!-- t-SNE -->
        <div class="method-section">
          <h3 data-lang="en">t-SNE (t-Distributed Stochastic Neighbor Embedding)</h3>
          <h3 data-lang="zh">t-SNE（t 分布随机邻居嵌入）</h3>
          <div data-lang="en"><p class="method-analogy">If PCA is like taking an aerial photo of a city (captures the big layout but loses street-level details), t-SNE is like a neighborhood walking tour — it faithfully preserves which houses are on the same block, even if the overall city map gets distorted.</p></div>
          <div data-lang="zh"><p class="method-analogy">如果说 PCA 像从高空俯拍城市（捕捉大布局但丢失街道细节），t-SNE 就像一次社区步行游览——它忠实保留哪些房子在同一条街上，即使整体城市地图可能变形。</p></div>
          <div data-lang="en">
            <p class="method-desc">Nonlinear dimensionality reduction purely for visualization. Unlike PCA (which preserves global structure), t-SNE preserves local neighborhoods — nearby points in high-D stay nearby in 2D.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc">非线性降维，纯粹用于可视化。与 PCA（保留全局结构）不同，t-SNE 保留局部邻域——高维中的近邻在 2D 中仍保持相近。</p>
          </div>
          <div data-lang="en"><p class="method-desc"><strong>Limitation:</strong> Not suitable for feature extraction (downstream analysis). The low-dimensional coordinates have no interpretation. Use it purely for exploring structure.</p></div>
          <div data-lang="zh"><p class="method-desc"><strong>局限：</strong>不适合特征提取（下游分析）。低维坐标没有解释。纯粹用于探索结构。</p></div>
          <div data-lang="en"><p class="method-example">MNIST digits: 784D images reduce to 2D revealing 10 clear, separated clusters (one per digit). Visually stunning and immediately shows data quality and separability.</p></div>
          <div data-lang="zh"><p class="method-example">MNIST 数字：784 维图像约化到 2D，显示 10 个清晰分离的集群（每个数字一个）。视觉上惊艳，立即展示数据质量和可分离性。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Visualization only, small-to-medium datasets (computationally expensive). Never use t-SNE coordinates as features for downstream models.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>仅可视化、小到中等数据集（计算昂贵）。永远不用 t-SNE 坐标作为下游模型的特征。</p></div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ===== SECTION 4: TRAINING MODELS ===== -->
      <div class="section" id="ml-training">
        <h2 data-lang="en">Module 2 · How Do You Train a Model?</h2>
        <h2 data-lang="zh">模块 2 · 模型怎么训练？</h2>

        <div class="challenge-overview">
          <div data-lang="en"><p>Building a model is one thing; <strong>training</strong> it is another. Training means: adjust parameters θ to minimize a loss function J(θ). Think of it as tuning a radio — small adjustments (learning rate) bring you closer to the station (optimal parameters). This section covers the optimization methods and regularization tricks that make good training possible.</p></div>
          <div data-lang="zh"><p>构建模型是一回事；<strong>训练</strong>它是另一回事。训练意味着：调整参数 θ 以最小化损失函数 J(θ)。想象调收音机——小调整（学习率）把你拉向电台（最优参数）。本节涵盖使良好训练成为可能的优化方法和正则化技巧。</p></div>
        </div>

        <!-- Gradient Descent -->
        <div class="method-section">
          <h3 data-lang="en">Gradient Descent &amp; Stochastic Gradient Descent</h3>
          <h3 data-lang="zh">梯度下降与随机梯度下降</h3>
          <div data-lang="en">
            <p class="method-analogy">Walking downhill in thick fog. You can't see the valley, only the slope beneath your feet. Each step, you move in the direction of steepest descent (negative gradient).</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">在浓雾中下山。你看不到山谷，只能感受脚下的斜坡。每一步，你朝最陡峭的下坡方向走（负梯度方向）。</p>
          </div>
          <div class="formula">θ^{t+1} = θ^t − α ∇J(θ^t)</div>
          <div data-lang="en"><p class="method-desc">Update rule: subtract learning rate α times the gradient. Large α = big steps (risk overshooting). Small α = tiny steps (slow convergence). Optimal α is problem-dependent.</p></div>
          <div data-lang="zh"><p class="method-desc">更新规则：减去学习率 α 乘以梯度。大的 α = 大步长（风险超过）。小的 α = 小步长（收敛慢）。最优的 α 取决于问题。</p></div>
          <div data-lang="en"><p class="method-desc"><strong>SGD:</strong> Instead of computing gradient on all n samples (Batch GD), use mini-batches of size 32-256. Much faster, adds noise that can help escape local minima. One epoch = seeing all data once.</p></div>
          <div data-lang="zh"><p class="method-desc"><strong>SGD：</strong>不在所有 n 个样本上计算梯度（批量 GD），而用 32-256 的小批次。快得多，添加的噪声有助于逃离局部最小值。一个 epoch = 看一遍所有数据。</p></div>
          <div class="formula">g' ≈ (1/|It|) Σ_{i∈It} ∇Jᵢ</div>
          <div data-lang="en"><p class="method-desc">Mini-batch gradient: average gradient over a small subset It. Noisy but unbiased estimate of full-batch gradient.</p></div>
          <div data-lang="zh"><p class="method-desc">小批次梯度：在一个小子集 It 上的梯度平均。有噪声但是全批次梯度的无偏估计。</p></div>
          <div data-lang="en"><p class="method-example">50,000 images, batch size 64 → 781 steps per epoch. Train for 100 epochs = 78,100 total updates. With modern GPUs, can train ResNet on ImageNet in hours.</p></div>
          <div data-lang="zh"><p class="method-example">50,000 张图片，批大小 64 → 每个 epoch 781 步。训练 100 个 epoch = 总共 78,100 次更新。用现代 GPU，在 ImageNet 上训练 ResNet 只需几小时。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Always. Gradient descent is the standard for all deep learning and most modern ML. Variants (Adam, RMSprop) improve convergence but core idea unchanged.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>总是。梯度下降是所有深度学习和大多数现代 ML 的标准。变体（Adam、RMSprop）改进收敛但核心思想不变。</p></div>
        </div>

        <!-- Loss Functions -->
        <div class="method-section">
          <h3 data-lang="en">Loss Functions</h3>
          <h3 data-lang="zh">损失函数</h3>
          <div data-lang="en">
            <p class="method-analogy">The loss function is your model's "report card." It grades predictions: how wrong are you, and how severe is the mistake?</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">损失函数是模型的"成绩单"。它给预测打分：你错得有多离谱，错误的严重程度有多高？</p>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Mean Squared Error (MSE):</strong> For regression. Penalizes large errors quadratically.</p>
            <div class="formula">MSE = (1/n) Σᵢ (yᵢ − ŷᵢ)²</div>
            <p class="method-example">Predict 100, actual 90 → error=10, loss=100. Predict 100, actual 95 → error=5, loss=25. Bigger errors are penalized much more.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>均方误差 (MSE)：</strong>用于回归。二次惩罚大误差。</p>
            <div class="formula">MSE = (1/n) Σᵢ (yᵢ − ŷᵢ)²</div>
            <p class="method-example">预测 100，实际 90 → 误差=10，损失=100。预测 100，实际 95 → 误差=5，损失=25。更大的误差受到更多惩罚。</p>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Binary Cross-Entropy (BCE):</strong> For binary classification. Derived from MLE.</p>
            <div class="formula">BCE = −(1/n) Σᵢ [yᵢ log(pᵢ) + (1−yᵢ) log(1−pᵢ)]</div>
            <p class="method-example">True=1, predict p=0.9 → loss=0.10. Predict p=0.5 → loss=0.69. Predict p=0.1 → loss=2.30. Wrong AND confident = severe penalty.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>二元交叉熵 (BCE)：</strong>用于二分类。来自最大似然。</p>
            <div class="formula">BCE = −(1/n) Σᵢ [yᵢ log(pᵢ) + (1−yᵢ) log(1−pᵢ)]</div>
            <p class="method-example">真值=1，预测 p=0.9 → 损失=0.10。预测 p=0.5 → 损失=0.69。预测 p=0.1 → 损失=2.30。错且确定 = 严重罚。</p>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Categorical Cross-Entropy (CCE):</strong> For multiclass classification.</p>
            <div class="formula">CCE = −(1/n) Σᵢ Σₖ 𝟙(yᵢ=k) log pₖ</div>
            <p class="method-desc">Only the true class term contributes to loss (indicator 𝟙 is 1 only for true class). Encourages high probability for correct class.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>分类交叉熵 (CCE)：</strong>用于多分类。</p>
            <div class="formula">CCE = −(1/n) Σᵢ Σₖ 𝟙(yᵢ=k) log pₖ</div>
            <p class="method-desc">只有真实类的项对损失有贡献（指示函数 𝟙 只在真实类时为 1）。鼓励正确类的高概率。</p>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Hinge Loss:</strong> For SVM. No penalty if prediction is correct AND confident (margin ≥ 1).</p>
            <div class="formula">Hinge = max(0, 1 − y·ŷ)</div>
            <p class="method-desc">Only misclassified or under-confident predictions incur loss.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>合页损失：</strong>用于 SVM。如果预测正确且确定（间距 ≥ 1）无惩罚。</p>
            <div class="formula">Hinge = max(0, 1 − y·ŷ)</div>
            <p class="method-desc">只有误分类或不够确定的预测产生损失。</p>
          </div>
        </div>

        <!-- Regularization -->
        <div class="method-section">
          <h3 data-lang="en">Regularization</h3>
          <h3 data-lang="zh">正则化</h3>
          <div data-lang="en">
            <p class="method-analogy">The "anti-cheating toolkit" for exams. A student who memorizes all past exams might score 100% on them but fail the actual exam (overfitting). Regularization prevents this by penalizing excessive memorization.</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">考试的"反作弊工具包"。死记硬背所有往年试卷的学生可能在它们上得 100% 但在真正考试中失败（过拟合）。正则化通过惩罚过度死记硬背来防止这种情况。</p>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Overfitting:</strong> Model learns training data perfectly (training loss low) but performs poorly on test data (test loss high). Cause: model is too flexible for the data size.</p>
            <p class="method-desc"><strong>Five key regularization techniques:</strong></p>
            <ol>
              <li><strong>L1 (Lasso):</strong> Add λ||θ||₁ to loss. Drives unimportant coefficients to exactly zero. Automatic feature selection.</li>
              <li><strong>L2 (Ridge):</strong> Add λ||θ||₂² to loss. Shrinks all coefficients toward zero but never to zero. Balances all features.</li>
              <li><strong>Dropout:</strong> During training, randomly deactivate neurons with probability p. Forces network to learn redundant representations, making it robust. Disable at test time.</li>
              <li><strong>Batch Normalization:</strong> Normalize layer inputs to zero mean, unit variance. Stabilizes training, allows higher learning rates.</li>
              <li><strong>Cross-Validation:</strong> K-fold: split data into K parts, train K times (each fold as test, others as train). Use to select hyperparameters and estimate true generalization error.</li>
            </ol>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>过拟合：</strong>模型完美学习训练数据（训练损失低）但在测试数据上表现差（测试损失高）。原因：模型对数据大小来说太灵活。</p>
            <p class="method-desc"><strong>五个关键正则化技术：</strong></p>
            <ol>
              <li><strong>L1（Lasso）：</strong>在损失上加 λ||θ||₁。把不重要系数完全压到零。自动特征选择。</li>
              <li><strong>L2（Ridge）：</strong>在损失上加 λ||θ||₂²。把所有系数压向零但永不完全为零。平衡所有特征。</li>
              <li><strong>Dropout：</strong>训练时，以概率 p 随机停用神经元。迫使网络学习冗余表示，提高鲁棒性。测试时禁用。</li>
              <li><strong>批量归一化：</strong>将层输入归一化为零均值、单位方差。稳定训练，允许更高的学习率。</li>
              <li><strong>交叉验证：</strong>K 折：把数据分成 K 部分，训练 K 次（每折作测试，其他作训练）。用于选择超参数和估计真实泛化误差。</li>
            </ol>
          </div>
          <div data-lang="en"><p class="method-example">Dataset: 100 features but only 10 truly predictive. Lasso zeros out the 90 irrelevant ones. Ridge shrinks all 100 toward zero but keeps them. Which is better depends on domain knowledge.</p></div>
          <div data-lang="zh"><p class="method-example">数据集：100 个特征但只有 10 个真正预测有效。Lasso 把 90 个无关的压到零。Ridge 把所有 100 个都压向零但保留。哪个更好取决于领域知识。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Whenever test loss > training loss (sign of overfitting). Always use cross-validation to tune hyperparameters. Modern practice: combine multiple techniques (L2 + Dropout + Batch Norm).</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>只要测试损失 > 训练损失（过拟合的征兆）。总是用交叉验证调超参数。现代实践：结合多个技术（L2 + Dropout + 批量归一化）。</p></div>
        </div>

        <p data-lang="en"><a class="sim-link" href="guides/ml-gradient-descent-convergence.html">▶ Deep Dive: Gradient Descent & Convergence</a></p>
        <p data-lang="zh"><a class="sim-link" href="guides/ml-gradient-descent-convergence.html">▶ 深入：梯度下降与收敛</a></p>
      </div>

      <hr class="section-divider">

      <!-- ===== SECTION 5: NEURAL NETWORKS ===== -->
      <div class="section" id="ml-neural">
        <h2 data-lang="en">Module 3 · How Do Neural Networks Work?</h2>
        <h2 data-lang="zh">模块 3 · 神经网络怎么工作？</h2>

        <div class="challenge-overview">
          <div data-lang="en"><p>Neural networks are brain-inspired computational models whose power comes from the <strong>Universal Approximation Theorem</strong>: given enough neurons, they can learn any pattern. The deep learning revolution happened because we finally had enough data and compute to train large networks. This module covers the architecture, training algorithm (backpropagation), and CNN for images.</p></div>
          <div data-lang="zh"><p>神经网络是受大脑启发的计算模型，其威力来自<strong>通用近似定理</strong>：给定足够多的神经元，它们可以学习任何规律。深度学习革命的发生是因为我们终于有了足够的数据和算力来训练大型网络。本模块涵盖架构、训练算法（反向传播）、以及用于图像的 CNN。</p></div>
        </div>

        <!-- Architecture -->
        <div class="method-section">
          <h3 data-lang="en">Network Architecture</h3>
          <h3 data-lang="zh">网络架构</h3>
          <div data-lang="en">
            <p class="method-analogy">Factory assembly line. Input layer receives raw materials (features), hidden layers process and transform (learn patterns), output layer packages the final product (prediction).</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">工厂生产线。输入层接收原料（特征），隐层处理和转换（学习规律），输出层包装最终产品（预测）。</p>
          </div>
          <div class="formula">z_h = W_h · x + b_h   →   a_h = h(z_h)   →   z_o = W_o · a_h + b_o</div>
          <div data-lang="en"><p class="method-desc">Forward pass: compute hidden activations a_h from input x, then output from a_h. <strong>Without activation functions, this is just a linear model</strong> (no matter how many layers). Nonlinearity comes from h (e.g., ReLU, sigmoid).</p></div>
          <div data-lang="zh"><p class="method-desc">前向传播：从输入 x 计算隐层激活 a_h，然后从 a_h 计算输出。<strong>没有激活函数，这只是一个线性模型</strong>（无论多少层）。非线性来自 h（如 ReLU、sigmoid）。</p></div>
          <div data-lang="en"><p class="method-example">MNIST: 784-dimensional input (28×28 image) → 256 hidden neurons → 10 output (one per digit). Simple architecture but effective for digit recognition.</p></div>
          <div data-lang="zh"><p class="method-example">MNIST：784 维输入（28×28 图像）→ 256 个隐层神经元 → 10 个输出（每个数字一个）。对于数字识别，简单架构但有效。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> When patterns are clearly nonlinear and sufficiently complex. Need enough data to prevent overfitting.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>当规律明显非线性且足够复杂时。需要足够数据防止过拟合。</p></div>
        </div>

        <!-- Activation Functions -->
        <div class="compare-grid">
          <div class="compare-col">
            <div class="compare-label" data-lang="en">Sigmoid</div>
            <div class="compare-label" data-lang="zh">Sigmoid</div>
            <div data-lang="en">
              <p class="formula">h(z) = 1 / (1 + e^{−z})</p>
              <p>Derivative h'(z) ≤ 0.25 — very small. After 10 layers: (0.25)^{10} ≈ 10^{−6}. Causes <strong>vanishing gradients</strong> in deep networks.</p>
            </div>
            <div data-lang="zh">
              <p class="formula">h(z) = 1 / (1 + e^{−z})</p>
              <p>导数 h'(z) ≤ 0.25 — 非常小。10 层后：(0.25)^{10} ≈ 10^{−6}。在深网络中导致<strong>梯度消失</strong>。</p>
            </div>
          </div>
          <div class="compare-col alt">
            <div class="compare-label" data-lang="en">ReLU</div>
            <div class="compare-label" data-lang="zh">ReLU</div>
            <div data-lang="en">
              <p class="formula">h(z) = max(0, z)</p>
              <p>Derivative h'(z) = 1 for z > 0, 0 for z < 0. No vanishing gradient. <strong>Default choice for modern networks</strong>. Simple, fast, works.</p>
            </div>
            <div data-lang="zh">
              <p class="formula">h(z) = max(0, z)</p>
              <p>导数 h'(z) = 1（z>0）、0（z<0）。无梯度消失。<strong>现代网络的默认选择</strong>。简单、快速、有效。</p>
            </div>
          </div>
          <div class="compare-footer" data-lang="en">Modern practice: ReLU for hidden layers, sigmoid for binary classification output, softmax for multiclass output.</div>
          <div class="compare-footer" data-lang="zh">现代实践：隐层用 ReLU，二分类输出用 sigmoid，多分类输出用 softmax。</div>
        </div>

        <!-- Backpropagation -->
        <div class="method-section">
          <h3 data-lang="en">Backpropagation</h3>
          <h3 data-lang="zh">反向传播</h3>
          <div data-lang="en">
            <p class="method-analogy">Factory defect tracing. A product is defective. Trace backward: which assembly station caused the defect? How much did each station contribute? Backprop does this for neural networks.</p>
          </div>
          <div data-lang="zh">
            <p class="method-analogy">工厂缺陷追踪。一个产品有缺陷。反向追踪：哪个组装站导致缺陷？每个站贡献多少？反向传播为神经网络做这个。</p>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Forward pass:</strong> Feed input through network, compute all z and a values, arrive at loss J.</p>
            <div class="formula">Forward: z_h = W_h·x + b_h,  a_h = h(z_h),  z_o = W_o·a_h + b_o</div>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>前向传播：</strong>把输入送入网络，计算所有 z 和 a 值，到达损失 J。</p>
            <div class="formula">前向：z_h = W_h·x + b_h,  a_h = h(z_h),  z_o = W_o·a_h + b_o</div>
          </div>
          <div data-lang="en">
            <p class="method-desc"><strong>Backward pass:</strong> Compute error signal δ (how much each layer contributed to final loss) using chain rule.</p>
            <div class="formula">δ_o = ∂J/∂z_o = (ŷ − y)</div>
            <div class="formula">δ_h = (W_o^T δ_o) ⊙ h'(z_h)</div>
            <p class="method-desc">⊙ is element-wise multiplication. δ decays layer by layer due to h'(z). If h' is small (sigmoid), δ becomes tiny — gradients vanish.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc"><strong>反向传播：</strong>用链式法则计算误差信号 δ（每层对最终损失的贡献）。</p>
            <div class="formula">δ_o = ∂J/∂z_o = (ŷ − y)</div>
            <div class="formula">δ_h = (W_o^T δ_o) ⊙ h'(z_h)</div>
            <p class="method-desc">⊙ 是逐元素乘法。δ 因 h'(z) 而逐层衰减。如果 h' 很小（sigmoid），δ 变得很小——梯度消失。</p>
          </div>
          <div class="formula">∂J/∂W_h = δ_h · x^T,  ∂J/∂W_o = δ_o · a_h^T</div>
          <div data-lang="en"><p class="method-desc">Gradients w.r.t. weights: outer product of error signal and input. These gradients feed into gradient descent.</p></div>
          <div data-lang="zh"><p class="method-desc">关于权重的梯度：误差信号和输入的外积。这些梯度送入梯度下降。</p></div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Always, for all neural networks. Backprop is the standard algorithm since the 1980s. Modern frameworks (PyTorch, TensorFlow) automate it.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>总是，对所有神经网络。反向传播是自 1980 年代的标准算法。现代框架（PyTorch、TensorFlow）自动化了它。</p></div>
          <p><a class="sim-link" href="guides/ml-backprop-math.html" data-lang="en">▶ Deep Dive: Backpropagation Mathematics</a><a class="sim-link" href="guides/ml-backprop-math.html" data-lang="zh">▶ 深入：反向传播数学</a></p>
        </div>

        <!-- CNN Deep Dive -->
        <div class="method-section">
          <h3 data-lang="en">Convolutional Neural Networks Deep Dive</h3>
          <h3 data-lang="zh">卷积神经网络深入探讨</h3>
          <div data-lang="en">
            <p class="method-desc">CNNs are specialized for spatial data (images, spectrograms). Key innovation: weight sharing — the same small filter scans the entire image, detecting the same local pattern everywhere.</p>
            <p class="method-desc"><strong>Feature hierarchy:</strong> Layer 1 detects edges (low-level). Layer 2 detects textures, shapes (mid-level). Layer 3+ detect objects, faces (semantic). This mirrors biological vision (V1 → V2 → IT).</p>
            <p class="method-desc"><strong>AlexNet milestone (2012):</strong> 3 innovations: (1) ReLU activation (fast, no gradient saturation), (2) Max Pooling (downsampling preserves important features), (3) Dropout (prevent overfitting on ImageNet). Result: top-5 error dropped from 25.6% to 15.3% — transformed computer vision.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc">CNN 专为空间数据（图像、频谱图）设计。关键创新：权重共享——相同的小过滤器扫描整个图像，在各处检测同一局部模式。</p>
            <p class="method-desc"><strong>特征层次：</strong>第 1 层检测边缘（低级）。第 2 层检测纹理、形状（中级）。第 3 层+ 检测物体、人脸（语义）。这镜像了生物视觉（V1 → V2 → IT）。</p>
            <p class="method-desc"><strong>AlexNet 里程碑 (2012)：</strong>3 个创新：(1) ReLU 激活（快速、无梯度饱和）、(2) 最大池化（下采样保留关键特征）、(3) Dropout（防止 ImageNet 过拟合）。结果：top-5 错误率从 25.6% 降至 15.3%——改变了计算机视觉。</p>
          </div>
          <div data-lang="en"><p class="method-when"><strong>When to use:</strong> Image inputs (or any 2D spatial data). CNNs are standard for computer vision. Modern variants (ResNet, EfficientNet, Vision Transformers) push performance further.</p></div>
          <div data-lang="zh"><p class="method-when"><strong>何时用：</strong>图像输入（或任何 2D 空间数据）。CNN 是计算机视觉的标准。现代变体（ResNet、EfficientNet、Vision Transformers）推进性能更远。</p></div>
        </div>
      </div>

      <hr class="section-divider">

      <!-- ===== SECTION 6: MATH FOUNDATIONS ===== -->
      <div class="section" id="ml-math">
        <h2 data-lang="en">Module 4 · What Math Do You Need?</h2>
        <h2 data-lang="zh">模块 4 · 需要什么数学？</h2>

        <div data-lang="en">
          <p>This section is intentionally light on formulas. It points to what math you need and why. Each topic has a corresponding guide for deeper dives.</p>
        </div>
        <div data-lang="zh">
          <p>本节刻意减少公式。它指出你需要什么数学以及为什么。每个主题都有相应的指南供深入学习。</p>
        </div>

        <!-- Linear Algebra -->
        <div class="method-section">
          <h3 data-lang="en">Linear Algebra</h3>
          <h3 data-lang="zh">线性代数</h3>
          <div data-lang="en"><p class="method-analogy">Linear algebra is the language ML uses to talk about data. A photo is a vector, a dataset is a matrix, and training a model is multiplying matrices — every computation under the hood speaks linear algebra.</p></div>
          <div data-lang="zh"><p class="method-analogy">线性代数是机器学习用来描述数据的语言。一张图片是一个向量，一个数据集是一个矩阵，训练模型就是做矩阵乘法——底层的每一步计算都在说线性代数。</p></div>
          <div data-lang="en">
            <p class="method-desc">Handles multi-dimensional data and transformations. Vectors represent feature sets. Matrices represent transformations. Eigendecomposition reveals principal directions of variation.</p>
            <p class="method-desc"><strong>Used throughout this page:</strong> Normal Equation (matrix inverse), PCA (eigendecomposition of covariance), neural network weight matrices (linear transformations), backprop (matrix multiplication for gradients).</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc">处理多维数据和变换。向量代表特征集。矩阵代表变换。特征分解揭示变化的主要方向。</p>
            <p class="method-desc"><strong>本页面用到：</strong>法线方程（矩阵逆）、PCA（协方差特征分解）、神经网络权重矩阵（线性变换）、反向传播（梯度的矩阵乘法）。</p>
          </div>
        </div>

        <!-- Probability & Statistics -->
        <div class="method-section">
          <h3 data-lang="en">Probability &amp; Statistics</h3>
          <h3 data-lang="zh">概率与统计</h3>
          <div data-lang="en"><p class="method-analogy">If linear algebra is the language, probability is the logic. ML models don't give yes/no answers — they say "I'm 87% sure this is a cat." Probability theory is how we formalize, measure, and optimize that kind of uncertain reasoning.</p></div>
          <div data-lang="zh"><p class="method-analogy">如果说线性代数是语言，概率就是逻辑。机器学习模型不给出"是/否"的答案——它说"我 87% 确定这是只猫"。概率论就是我们形式化、度量和优化这种不确定推理的方式。</p></div>
          <div data-lang="en">
            <p class="method-desc">Reasoning under uncertainty. Bayes' theorem, maximum likelihood estimation (MLE), probability distributions.</p>
            <p class="method-desc"><strong>Used throughout this page:</strong> MLE → Binary Cross-Entropy loss (classification), Gaussian assumption → MSE loss (regression), MAP (MLE + prior) → L2 regularization.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc">在不确定性下的推理。贝叶斯定理、最大似然估计 (MLE)、概率分布。</p>
            <p class="method-desc"><strong>本页面用到：</strong>MLE → 二元交叉熵损失（分类）、高斯假设 → MSE 损失（回归）、MAP（MLE + 先验）→ L2 正则化。</p>
          </div>
        </div>

        <!-- Optimization -->
        <div class="method-section">
          <h3 data-lang="en">Optimization Theory</h3>
          <h3 data-lang="zh">优化理论</h3>
          <div data-lang="en"><p class="method-analogy">Linear algebra stores data, probability says what "good predictions" means, and optimization is the engine that actually drives the model toward those good predictions — it's the "learning" in machine learning.</p></div>
          <div data-lang="zh"><p class="method-analogy">线性代数存储数据，概率定义"好预测"的含义，而优化是真正驱动模型走向好预测的引擎——它就是机器学习中"学习"二字的实现。</p></div>
          <div data-lang="en">
            <p class="method-desc">Finding best parameters. Gradient-based methods, convexity, constraints (KKT conditions).</p>
            <p class="method-desc"><strong>Used throughout this page:</strong> Gradient Descent (training all models), SVM dual problem (Lagrange multipliers), convergence analysis for neural networks.</p>
          </div>
          <div data-lang="zh">
            <p class="method-desc">寻找最佳参数。基于梯度的方法、凸性、约束（KKT 条件）。</p>
            <p class="method-desc"><strong>本页面用到：</strong>梯度下降（训练所有模型）、SVM 对偶问题（拉格朗日乘数）、神经网络收敛分析。</p>
          </div>
        </div>

        <p data-lang="en"><a class="sim-link" href="guides/ml-math-foundations.html">▶ Math Foundations Reference (coming soon)</a></p>
        <p data-lang="zh"><a class="sim-link" href="guides/ml-math-foundations.html">▶ 数学基础参考页（即将上线）</a></p>
      </div>

      <hr class="section-divider">

      <!-- ===== RESOURCES ===== -->
      <div class="section resources-section" id="ml-resources">
        <h2 data-lang="en">Resources</h2>
        <h2 data-lang="zh">资源</h2>

        <h3 data-lang="en">Key References</h3>
        <h3 data-lang="zh">关键参考</h3>
        <ul class="resource-list">
          <li>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The Elements of Statistical Learning</em> (2nd ed.). Springer.</li>
          <li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). <em>An Introduction to Statistical Learning</em> (2nd ed.). Springer.</li>
          <li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer.</li>
          <li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</li>
          <li>Murphy, K. P. (2022). <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press.</li>
        </ul>

        <h3 data-lang="en">Deep-Dive Guides</h3>
        <h3 data-lang="zh">深入指南</h3>
        <ul class="resource-list" data-lang="en">
          <li><a href="guides/ml-gradient-descent-convergence.html">Gradient Descent &amp; Convergence</a> — Convergence rates, learning rate selection, adaptive methods (Adam, RMSprop)</li>
          <li><a href="guides/ml-backprop-math.html">Backpropagation Mathematics</a> — Chain rule, computation graphs, vanishing gradient analysis</li>
          <li><a href="guides/ml-svm-lagrange.html">SVM &amp; Lagrange Optimization</a> — Dual problem, KKT conditions, kernel trick proof</li>
          <li><a href="guides/ml-math-foundations.html">Math Foundations Reference</a> — Linear algebra, probability, optimization essentials for ML</li>
          <li><a href="llm.html">LLM &amp; NLP →</a> — How language models build on these ML foundations</li>
        </ul>
        <ul class="resource-list" data-lang="zh">
          <li><a href="guides/ml-gradient-descent-convergence.html">梯度下降与收敛</a> — 收敛率、学习率选择、自适应方法（Adam、RMSprop）</li>
          <li><a href="guides/ml-backprop-math.html">反向传播数学</a> — 链式法则、计算图、梯度消失分析</li>
          <li><a href="guides/ml-svm-lagrange.html">SVM 与拉格朗日优化</a> — 对偶问题、KKT 条件、核技巧证明</li>
          <li><a href="guides/ml-math-foundations.html">数学基础参考</a> — ML 所需的线性代数、概率、优化要点</li>
          <li><a href="llm.html">LLM &amp; NLP →</a> — 语言模型如何建立在这些 ML 基础之上</li>
        </ul>

        <h3 data-lang="en">Software</h3>
        <h3 data-lang="zh">软件工具</h3>
        <ul class="resource-list" data-lang="en">
          <li><strong>scikit-learn</strong> — Classical ML (regression, classification, clustering, cross-validation). Best starting point for all non-deep-learning tasks.</li>
          <li><strong>PyTorch</strong> — Deep learning: autograd, GPU acceleration, research-friendly. Industry standard for neural networks.</li>
          <li><strong>pandas + NumPy</strong> — Data manipulation and numerical computing. Essential preprocessing before any ML model.</li>
        </ul>
        <ul class="resource-list" data-lang="zh">
          <li><strong>scikit-learn</strong> — 经典 ML（回归、分类、聚类、交叉验证）。所有非深度学习任务的最佳起点。</li>
          <li><strong>PyTorch</strong> — 深度学习：自动求导、GPU 加速、研究友好。神经网络的行业标准。</li>
          <li><strong>pandas + NumPy</strong> — 数据处理与数值计算。任何 ML 模型前的必要预处理层。</li>
        </ul>
      </div>

      <hr class="section-divider">

      <!-- PAGE NAV -->
      <div class="page-nav">
        <span>
          <a href="lpa.html" data-lang="en">&larr; Latent Profile Analysis</a>
          <a href="lpa.html" data-lang="zh">&larr; 潜在剖面分析</a>
        </span>
        <span>
          <a href="llm.html" data-lang="en">LLM &amp; NLP &rarr;</a>
          <a href="llm.html" data-lang="zh">LLM &amp; NLP &rarr;</a>
        </span>
      </div>

    </div>
  </div>
</div>

<script>
function setLang(lang) {
  document.body.className = lang;
  document.getElementById('btn-en').style.fontWeight = (lang === 'en') ? '600' : '400';
  document.getElementById('btn-zh').style.fontWeight = (lang === 'zh') ? '600' : '400';
  document.getElementById('btn-en').style.color = (lang === 'en') ? 'var(--red)' : 'var(--ink-faded)';
  document.getElementById('btn-zh').style.color = (lang === 'zh') ? 'var(--red)' : 'var(--ink-faded)';
  localStorage.setItem('preferred-lang', lang);
}
window.addEventListener('load', function() {
  const saved = localStorage.getItem('preferred-lang') || 'en';
  setLang(saved);
});

// ── Right-side TOC (auto-detects h2 + h3 headings) ───────
(function buildPageTOC() {
  const content = document.querySelector('.content');
  const mainEl  = document.querySelector('.main');
  if (!content || !mainEl) return;

  // Walk EN headings; pair with their ZH sibling
  const items = [];
  let autoId = 0;
  content.querySelectorAll('h2[data-lang="en"], h3[data-lang="en"]').forEach(hEn => {
    const zhSib = hEn.nextElementSibling;
    const enText = hEn.textContent.trim();
    const zhText = (zhSib && zhSib.dataset && zhSib.dataset.lang === 'zh')
                   ? zhSib.textContent.trim() : enText;
    const isH3  = hEn.tagName === 'H3';

    // Anchor: for h2 use the .section parent; for h3 use .method-section parent
    const anchorEl = isH3
      ? (hEn.closest('.method-section') || hEn.parentElement)
      : (hEn.closest('.section')        || hEn.parentElement);

    if (!anchorEl.id) anchorEl.id = 'toc-auto-' + (autoId++);
    items.push({ id: anchorEl.id, en: enText, zh: zhText, isH3 });
  });

  // Build nav element
  const nav = document.createElement('nav');
  nav.className = 'page-toc';
  const header = document.createElement('div');
  header.className = 'toc-header';
  header.textContent = 'Contents';
  nav.appendChild(header);

  const links = {};
  items.forEach(({ id, en, zh, isH3 }) => {
    const a = document.createElement('a');
    a.href = '#' + id;
    a.dataset.en = en; a.dataset.zh = zh;
    a.textContent = en;
    if (isH3) a.classList.add('toc-sub');
    links[id] = a;
    nav.appendChild(a);
  });
  mainEl.appendChild(nav);

  // Scroll-spy
  let current = null;
  const obs = new IntersectionObserver(entries => {
    entries.forEach(e => {
      if (e.isIntersecting) {
        if (current && links[current]) links[current].classList.remove('active');
        current = e.target.id;
        if (links[current]) links[current].classList.add('active');
      }
    });
  }, { rootMargin: '-8% 0px -68% 0px' });
  items.forEach(({ id }) => { const el = document.getElementById(id); if (el) obs.observe(el); });

  // Language sync
  function tocSetLang(lang) {
    header.textContent = lang === 'zh' ? '目录' : 'Contents';
    items.forEach(({ id, en, zh }) => { if (links[id]) links[id].textContent = lang === 'zh' ? zh : en; });
  }
  const _orig = window.setLang;
  window.setLang = function(lang) { _orig(lang); tocSetLang(lang); };
  tocSetLang(localStorage.getItem('preferred-lang') || 'en');
})();
</script>
</body>
</html>
